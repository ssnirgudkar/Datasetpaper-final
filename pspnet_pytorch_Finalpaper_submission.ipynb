{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pspnet-pytorch-Finalpaper_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7JY4kP0ulNbep1f1Wc00N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/Datasetpaper-final/blob/main/pspnet_pytorch_Finalpaper_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYlYx-tlnrte"
      },
      "source": [
        "#Encoder - se_resnext50_32x4d, Decoder - FPN, 100 epochs. FPN = next generation pspnet \n",
        "This is the final code that should be used\n",
        "\n",
        "## you need to create the model folder in the drive till the actual file path. model.save only creates the file not the folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-T-DGiWU3kt"
      },
      "source": [
        "https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb\n",
        "\n",
        "** our masked image is gray scale with 1 channel (height * width*channel=1). and the pixel values are equal to the class ids. Each pixle value is only 1 value and not (a*b*c). The CV2.imread, with a flag od 0 that reads it as a gray scalae does not work for our images although it works for the other dataset\n",
        "\n",
        "## image size: (256, 320) #it's height and width\n",
        "## Mask - channel - , size - extension.png, gray scale\n",
        "\n",
        "## Use GPU so that CUDA is available\n",
        "\n",
        "##Run1\n",
        "1. train - 2570, val - 740, test - 370 , Epochs - 500 \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time - 11 hrs  hrs. Testing time -  8 min\n",
        "\n",
        "##Run2\n",
        "1. train - , val - , test - , Epochs - 100\n",
        "2. NO preloaded / imagenet weights \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time - hrs. Testing time -   min\n",
        "\n",
        "##Run3\n",
        "1. train -24814 , val - 7090, test - 3544, Epochs - 76(failed on 77th)\n",
        "2. NO preloaded / imagenet weights \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time - 18 hrs. Testing time -  1hr 15 min\n",
        "\n",
        "##Run4\n",
        "1. train - 3438 , val - 982 , test - 492 , Epochs - 50\n",
        "2. NO preloaded / imagenet weights \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time - 1 hrs. Testing time -  5 min\n",
        "Model - pspnet-pytorch-finalpapersubmission_V2\n",
        "##Even with 100 epocs 346 images of the 491 have a few pixels that are not mapped and have a value of 0. need to check in the architecture\n",
        "\n",
        "##Run5\n",
        "size - actual size , 2020 only \n",
        "1. train - 3438 , val - 982 , test - 492 , Epochs - 50\n",
        "2. NO preloaded / imagenet weights \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time - 1 hrs. Testing time -  5 min\n",
        "Model - pspnet-pytorch-finalpapersubmission_V2\n",
        "##Even with 100 epocs 346 images of the 491 have a few pixels that are not mapped and have a value of 0. need to check in the architecture\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11nEKAj4mXcL"
      },
      "source": [
        "#To Do - \n",
        "1. Right now dice loss and iau for accuracy. check if we need to select any other metrics \n",
        "2. We are not plotting the losses / accuracy. need to check what to use to get the plots. should that be consistent across all the models? in that case should we use tensorboard?\n",
        "3. collapse all classes into 1 complete image \n",
        "4. get the file name automatically based on the run number, instead of having to manually add it while saving the image on drive\n",
        "5. Ensure that the sort order is the same between unet and pspnet so that we know that its the same image \n",
        "6. HUman objects are really bad in pspnet. many ob are also missed if they are small. check if any parameter needs to be changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWlqTikUn49F"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBdE7JFgimEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb98655f-b344-4c65-8f1c-d560f4c3786d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWmjSDMi34CQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1b322e-cce6-4567-8f6b-55c202ad9dbe"
      },
      "source": [
        "# Install required libs\n",
        "!pip install -U segmentation-models-pytorch albumentations --user"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.2.1-py3-none-any.whl (88 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 40 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 61 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 88 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 44.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 30 kB 38.6 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 40 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 61 kB 17.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 71 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 81 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 92 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 15.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.11.1+cu111)\n",
            "Collecting efficientnet-pytorch==0.6.3\n",
            "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 15.9 MB/s \n",
            "\u001b[?25hCollecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.10.0+cu111)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 75.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.0.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=c30f32fe1c125c916e3fc82f33dbe8ad4ef2820d81cd8ac167ac4af3434a917b\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=9016bc234555f1d93bf4e4a6f49ba6ec759405aaf50eacb476ee6169992f8fa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: opencv-python-headless, munch, timm, qudida, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch, albumentations\n",
            "Successfully installed albumentations-1.1.0 efficientnet-pytorch-0.6.3 munch-2.5.0 opencv-python-headless-4.5.4.60 pretrainedmodels-0.7.4 qudida-0.0.4 segmentation-models-pytorch-0.2.1 timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bshuhFbf4HRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3ea71d-3e66-44c9-bd8c-3dd9e242b2fb"
      },
      "source": [
        "!pip uninstall -y segmentation-models-pytorch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: segmentation-models-pytorch 0.2.1\n",
            "Uninstalling segmentation-models-pytorch-0.2.1:\n",
            "  Successfully uninstalled segmentation-models-pytorch-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz0adR3tUZVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb4be253-3047-450e-ee56-4665cc1fc22d"
      },
      "source": [
        "!git clone https://github.com/Cadene/pretrained-models.pytorch.git\n",
        "!git clone https://github.com/qubvel/segmentation_models.pytorch\n",
        "!git clone https://github.com/alexgkendall/SegNet-Tutorial"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pretrained-models.pytorch'...\n",
            "remote: Enumerating objects: 803, done.\u001b[K\n",
            "remote: Total 803 (delta 0), reused 0 (delta 0), pack-reused 803\u001b[K\n",
            "Receiving objects: 100% (803/803), 522.66 KiB | 3.29 MiB/s, done.\n",
            "Resolving deltas: 100% (491/491), done.\n",
            "Cloning into 'segmentation_models.pytorch'...\n",
            "remote: Enumerating objects: 1526, done.\u001b[K\n",
            "remote: Counting objects: 100% (249/249), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 1526 (delta 149), reused 142 (delta 86), pack-reused 1277\u001b[K\n",
            "Receiving objects: 100% (1526/1526), 4.90 MiB | 6.04 MiB/s, done.\n",
            "Resolving deltas: 100% (832/832), done.\n",
            "Cloning into 'SegNet-Tutorial'...\n",
            "remote: Enumerating objects: 2785, done.\u001b[K\n",
            "remote: Total 2785 (delta 0), reused 0 (delta 0), pack-reused 2785\u001b[K\n",
            "Receiving objects: 100% (2785/2785), 340.84 MiB | 13.87 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7thoJfVhVlga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68bc00b5-5b57-42ea-8fd2-e15a609a55b0"
      },
      "source": [
        "#!pip install -U segmentation-models-pytorch albumentations --user\n",
        "#!pip uninstall -y segmentation-models-pytorch\n",
        "#!pip install segmentation_models_pytorch\n",
        "\n",
        "!pip install git+https://github.com/IvyGongoogle/pretrained-models.pytorch\n",
        "!pip install git+https://github.com/lukemelas/EfficientNet-PyTorch\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/IvyGongoogle/pretrained-models.pytorch\n",
            "  Cloning https://github.com/IvyGongoogle/pretrained-models.pytorch to /tmp/pip-req-build-lq77ig88\n",
            "  Running command git clone -q https://github.com/IvyGongoogle/pretrained-models.pytorch /tmp/pip-req-build-lq77ig88\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: munch in /root/.local/lib/python3.7/site-packages (from pretrainedmodels==0.7.0) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.0) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels==0.7.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels==0.7.0) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels==0.7.0) (7.1.2)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.0-py3-none-any.whl size=60553 sha256=9fc8f0f67f75f1d41fe01df772df087b385e11f59160c67dee0b21b38d9cc363\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wkr5nwe7/wheels/a8/51/08/bb41925a23e2225b4ca505b4e1709ce700613282945986a4b0\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: pretrainedmodels\n",
            "  Attempting uninstall: pretrainedmodels\n",
            "    Found existing installation: pretrainedmodels 0.7.4\n",
            "    Uninstalling pretrainedmodels-0.7.4:\n",
            "      Successfully uninstalled pretrainedmodels-0.7.4\n",
            "Successfully installed pretrainedmodels-0.7.0\n",
            "Collecting git+https://github.com/lukemelas/EfficientNet-PyTorch\n",
            "  Cloning https://github.com/lukemelas/EfficientNet-PyTorch to /tmp/pip-req-build-o0046r_2\n",
            "  Running command git clone -q https://github.com/lukemelas/EfficientNet-PyTorch /tmp/pip-req-build-o0046r_2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.7.1) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.7.1) (3.10.0.2)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=20622 sha256=57ffa50cbd1bf6431a277f622ae9fc1522ed2b02c94a092765c1ba201fb8f6a4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e352fwhw/wheels/f2/33/a5/85d7a0f3b00e37fc1cbe80d6edb12baeee3f7eddc9c881938c\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "  Attempting uninstall: efficientnet-pytorch\n",
            "    Found existing installation: efficientnet-pytorch 0.6.3\n",
            "    Uninstalling efficientnet-pytorch-0.6.3:\n",
            "      Successfully uninstalled efficientnet-pytorch-0.6.3\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u3NegrvTlOl"
      },
      "source": [
        "#!pip install efficientnet-pytorch\n",
        "#!pip install pretrainedmodels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3_Ebd77C2Hb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6befa4-b5ca-483e-863c-aebf377eadc3"
      },
      "source": [
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
            "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-y4b5qrha\n",
            "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-y4b5qrha\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.2.1) (0.11.1+cu111)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Using cached pretrainedmodels-0.7.4-py3-none-any.whl\n",
            "Collecting efficientnet-pytorch==0.6.3\n",
            "  Using cached efficientnet_pytorch-0.6.3-py3-none-any.whl\n",
            "Requirement already satisfied: timm==0.4.12 in /root/.local/lib/python3.7/site-packages (from segmentation-models-pytorch==0.2.1) (0.4.12)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.1) (1.10.0+cu111)\n",
            "Requirement already satisfied: munch in /root/.local/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.1) (2.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.1) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.1) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.1) (1.15.0)\n",
            "Building wheels for collected packages: segmentation-models-pytorch\n",
            "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.2.1-py3-none-any.whl size=88649 sha256=5c668bff0106523cb194fa9ba3ddbab46a729f4f06d0828d63e8cba28bd1b935\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6po98bnj/wheels/fa/c5/a8/1e8af6cb04a0974db8a4a156ebd2fdd1d99ad2558d3fce49d4\n",
            "Successfully built segmentation-models-pytorch\n",
            "Installing collected packages: pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "  Attempting uninstall: pretrainedmodels\n",
            "    Found existing installation: pretrainedmodels 0.7.0\n",
            "    Uninstalling pretrainedmodels-0.7.0:\n",
            "      Successfully uninstalled pretrainedmodels-0.7.0\n",
            "  Attempting uninstall: efficientnet-pytorch\n",
            "    Found existing installation: efficientnet-pytorch 0.7.1\n",
            "    Uninstalling efficientnet-pytorch-0.7.1:\n",
            "      Successfully uninstalled efficientnet-pytorch-0.7.1\n",
            "Successfully installed efficientnet-pytorch-0.6.3 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9xamsKiE9c6"
      },
      "source": [
        "# Restart Runtime now from the MENU !!! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkIUWW0c6IJN"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCpIpta03RyH"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIQFK_CNoJa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e06f7a-1560-4ba9-a0fe-2f3140375c0f"
      },
      "source": [
        "torch.cuda.is_available()\n",
        "!torch. __version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: torch.: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/train.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/trainannot.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/val.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/valannot.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "PUg6t9rmb5yz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "APP_FOLDER = '/content/IRDatasetFinal/train'\n",
        "totalFiles = 0\n",
        "totalDir = 0\n",
        "\n",
        "for base, dirs, files in os.walk(APP_FOLDER):\n",
        "    print('Searching in : ',base)\n",
        "    for directories in dirs:\n",
        "        totalDir += 1\n",
        "    for Files in files:\n",
        "        totalFiles += 1\n",
        "   \n",
        "\n",
        "print('Total number of files',totalFiles)\n",
        "print('Total Number of directories',totalDir)\n",
        "print('Total:',(totalDir + totalFiles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tTWrwVLewvQ",
        "outputId": "967b9dbb-6227-4745-924e-fdfce5a3b01b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching in :  /content/IRDatasetFinal/train\n",
            "Total number of files 26081\n",
            "Total Number of directories 0\n",
            "Total: 26081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c8jJFC435wa"
      },
      "source": [
        "#DATA_DIR = '/content/drive/MyDrive/TheIRDatasetMini'\n",
        "#DATA_DIR = '/content/drive/MyDrive/TheIRDatasetMini_backup'\n",
        "#DATA_DIR = '/content/drive/MyDrive/psp-1.0'\n",
        "DATA_DIR = '/content/IRDatasetFinal'\n",
        "# load repo with data if it is not exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print('Loading data...')\n",
        "    os.system('git clone https://github.com/alexgkendall/SegNet-Tutorial ./data')\n",
        "    print('Done!')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyxerGLW4FLz"
      },
      "source": [
        "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
        "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
        "\n",
        "#x_test_dir = os.path.join(DATA_DIR, 'test')\n",
        "#y_test_dir = os.path.join(DATA_DIR, 'testannot')\n",
        "\n",
        "NUM_CLASSES = 7 \n",
        "CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "\n",
        "#image_width = 320\n",
        "#image_height = 256\n",
        "  \n",
        "image_width = 640\n",
        "image_height = 512\n",
        "\n",
        "train_batch_size = 10 \n",
        "val_batch_size = 5 \n",
        "\n",
        "#learning rate \n",
        "lr=0.001 \n",
        "\n",
        "#epoch +1 as range is used \n",
        "epoch = 1\n",
        " \n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHvu2KtF4gzr"
      },
      "source": [
        "# helper function for data visualization\n",
        "### TO DO - CHANGE THE COLOR MAP IN MATPLOTLIB FOR CONSISTENCY\n",
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzxOM82S0M7X"
      },
      "source": [
        "'''\n",
        "#inference\n",
        "import PIL\n",
        "#def visualize2(input_image, input_mask, predicted_mask):\n",
        "def visualize2(predicted_mask,sequence,imagename):\n",
        "    #NUM_CLASSES = 7\n",
        "    \"\"\"Quick utility to display a model's prediction. we need color masked images, so displaying both color and black and white\"\"\"\n",
        "                   \n",
        "    #0 - sky(dark.blue). \n",
        "    #1 - water(light.blue).   \n",
        "    #2 - bridge(yellow).   \n",
        "    #3 - obstacle(purple).  \n",
        "    #4- living ob(green).  \n",
        "    #5- backgnd (orange). \n",
        "    #6 - self(pink)\n",
        "    #7 - null values. white\n",
        "    # Colors are same as segments.ai scheme\n",
        "    \n",
        "    label_colours = [(0,113,188), (216,82,24), (236,176,31), (125, 46, 141), (118, 171, 47), (161, 19, 46), (255,0,0), (255,255,255)]  \n",
        "    \n",
        "    #print(\"mask shape = \", np.shape(predicted_mask))\n",
        "    #print(\"Unique pixel values = \", np.unique(predicted_mask))\n",
        "    #print(\"Type of mask = \", type(predicted_mask))\n",
        "    \n",
        "\n",
        "    img1 = PIL.Image.new('RGB', (image_width, image_height))\n",
        "    pixels = img1.load()\n",
        "    #print(type(pixels))\n",
        "    #print(pixels[0,0])\n",
        "    #converting incoming predicted mask from float to an int \n",
        "    int_predicted_mask = predicted_mask.astype(int)\n",
        "    #for j_, j in enumerate(predicted_mask[:, :]):\n",
        "    for j_, j in enumerate(int_predicted_mask[:, :]):  \n",
        "        #print (j_, j)\n",
        "        for k_, k in enumerate(j):\n",
        "              #print(k_, k)\n",
        "              if k < NUM_CLASSES:\n",
        "                 pixels[k_,j_] = label_colours[k]\n",
        "    output = np.array(img1)\n",
        "    \n",
        "    #print(\"this is the colored inferred image\")\n",
        "    display(img1)\n",
        "\n",
        "    #save inferred image \n",
        "    #img1.save('/content/drive/MyDrive/IRdatasetmini-inferences/pspnet/picture/' + str(sequence) + '--run1--' + imagename)\n",
        "    img1.save('/content/drive/MyDrive/IRDatasetFinal-Inferences/pspnet/picture/' + str(sequence) + '--run2020-1--' + imagename)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex_P5Fvi4mF4"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "import random"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLwpyW_P4vtk"
      },
      "source": [
        "class Dataset(BaseDataset):\n",
        "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
        "    \n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_values (list): values of classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline \n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing \n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    #0 - sky 1 - water 2 - bridge 3 - obstacle 4- living ob  5- backgnd 6 - self  \n",
        "               \n",
        "    \n",
        "    #CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "    #NUM_CLASSES = 7              \n",
        "    \n",
        "    def __init__(\n",
        "            self, \n",
        "            images_dir, \n",
        "            masks_dir, \n",
        "            classes=None, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        #removing the sort and replacing with random so that images get trained in a shuffled way\n",
        "        self.images_fps = sorted([os.path.join(images_dir, image_id) for image_id in self.ids])\n",
        "        self.masks_fps = sorted([os.path.join(masks_dir, image_id) for image_id in self.ids])\n",
        "\n",
        "        indices_train = random.sample(range(len(self.images_fps)), len(self.images_fps))\n",
        "        self.images_fps = list(map(self.images_fps.__getitem__, indices_train))\n",
        "        self.masks_fps = list(map(self.masks_fps.__getitem__, indices_train))\n",
        "        \n",
        "        #mask1 = cv2.imread(self.masks_fps[1], cv2.IMREAD_UNCHANGED)\n",
        "        #print(\"true value of image\", np.unique(mask1))\n",
        "        # convert str names to class values on masks\n",
        "        #self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "        self.class_values = [CLASSES.index(cls.lower()) for cls in classes]\n",
        "\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "        print(\"shuffled images are\", self.images_fps[1:5])\n",
        "        print(\"shuffled masks are\", self.masks_fps[1:5])\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        image = cv2.imread(self.images_fps[i])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        #HAD TO CHANGE FLAG 0 TO IM_READ_UNCHANGED BELOW as it was not reading the pixel values otherwise \n",
        "        # mask = cv2.imread(self.masks_fps[i], 0)\n",
        "        mask = cv2.imread(self.masks_fps[i], cv2.IMREAD_UNCHANGED)\n",
        "        #print (\"masked image values\", mask)\n",
        "        #print (\"masked shape values\", mask.shape)\n",
        "        #print(\"masked unique values\", np.unique(mask))\n",
        "        #print(\"pixel values of mask\", mask)\n",
        "        \n",
        "        # extract certain classes from mask (e.g. cars)\n",
        "        masks = [(mask == v) for v in self.class_values]\n",
        "        #print (\"masks extraction\", masks)\n",
        "        mask = np.stack(masks, axis=-1).astype('float')\n",
        "        #print(\"stacks of classes for mask\", masks)\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            #print('masked array first', np.unique(mask))\n",
        "            \n",
        "        return image, mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7UyZe2T5FWv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "478eb113-ee86-4a31-af7a-7c298c564ec1"
      },
      "source": [
        "'''\n",
        "# Lets look at data we have. it can only put 1 class at a time \n",
        "\n",
        "#CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "dataset = Dataset(x_train_dir, y_train_dir, classes= [\"background\"])\n",
        "\n",
        "image, mask = dataset[2569] # get some sample. \n",
        "\n",
        "\n",
        "'''\n",
        "#just picking 1 image from the train dataset. Also selecting only class 'obstacle'. \n",
        "#the stacks of classes change when i change the class to sky. so the visualize is only showing the array for that specific class. \n",
        "# it's boolean - yes or No. Sp it will say if every pixel belongs to that class or not. it's able to print the ground truth properly\n",
        "#which means dataset is created properly. \n",
        "'''\n",
        "#print ('masked array' , np.unique(mask))\n",
        "visualize(\n",
        "    image=image, \n",
        "    cars_mask=mask.squeeze(),\n",
        "   \n",
        ")\n",
        "'''"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#print ('masked array' , np.unique(mask))\\nvisualize(\\n    image=image, \\n    cars_mask=mask.squeeze(),\\n   \\n)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHskJX8u6foc"
      },
      "source": [
        "import albumentations as albu"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OSF_x2YHu9p"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG-uG-R06kNm"
      },
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "\n",
        "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
        "\n",
        "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
        "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
        "\n",
        "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
        "        albu.IAAPerspective(p=0.5),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.CLAHE(p=1),\n",
        "                albu.RandomBrightness(p=1),\n",
        "                albu.RandomGamma(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.IAASharpen(p=1),\n",
        "                albu.Blur(blur_limit=3, p=1),\n",
        "                albu.MotionBlur(blur_limit=3, p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.RandomContrast(p=1),\n",
        "                albu.HueSaturationValue(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
        "    test_transform = [\n",
        "        albu.PadIfNeeded(384, 480)\n",
        "    ]\n",
        "    return albu.Compose(test_transform)\n",
        "\n",
        "\n",
        "'''\n",
        "OpenCV img = cv2.imread(path) loads an image with HWC-layout (height, width, channels), \n",
        "while Pytorch requires CHW-layout. So we have to do np.transpose(image,(2,0,1))\n",
        "for HWC->CHW transformation.\n",
        "'''\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construct preprocessing transform\n",
        "    \n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUqMy0pc6qAj"
      },
      "source": [
        "#### Visualize resulted augmented images and masks - thows error after 1st picture. \n",
        "#THIS IS NOT NECESSARY. plus our images have some augmentation\n",
        "'''\n",
        "augmented_dataset = Dataset(\n",
        "    x_train_dir, \n",
        "    y_train_dir, \n",
        "    augmentation=get_training_augmentation(), \n",
        "    classes=['obstacle'],\n",
        ")\n",
        "\n",
        "# same image with different random transforms\n",
        "#for i in range(3):  (Had to comment this out as i only have 1 image. will remove during actual training)\n",
        "for i in range(3):\n",
        "   image, mask = augmented_dataset[1]\n",
        "   visualize(image=image, mask=mask.squeeze(-1))\n",
        "   '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2rVoCdu_u5r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "085f3ee2-7e38-47d1-9696-b6c4ae5a87d4"
      },
      "source": [
        "ENCODER = 'se_resnext50_32x4d'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "#CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "ACTIVATION = 'softmax2d' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# create segmentation model with pretrained encoder\n",
        "model = smp.FPN(\n",
        "    encoder_name=ENCODER, \n",
        "    #encoder_weights=ENCODER_WEIGHTS, Do not want to use existing encoder weights. \n",
        "    encoder_weights=None, \n",
        "    classes=len(CLASSES), \n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
        "'''if i use existing encoder weights, then it doesn't display all classes in the prediction for some reason. \n",
        "    have not found out exactly why. but perhaps std classes as sky / water are being taken up from the weights. \n",
        "    obstacle , backngd are not standard and therefore may not be plotted. So i will train the model from scratch\n",
        "''' \n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"if i use existing encoder weights, then it doesn't display all classes in the prediction for some reason. \\n    have not found out exactly why. but perhaps std classes as sky / water are being taken up from the weights. \\n    obstacle , backngd are not standard and therefore may not be plotted. So i will train the model from scratch\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwDZ6rvsyowG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b339c94d-4df1-43ee-aa9d-a284256931cd"
      },
      "source": [
        "train_dataset = Dataset(\n",
        "    x_train_dir, \n",
        "    y_train_dir, \n",
        "    #augmentation=get_training_augmentation(), try w/o augmentation \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES, \n",
        ")\n",
        "\n",
        "valid_dataset = Dataset(\n",
        "    x_valid_dir, \n",
        "    y_valid_dir, \n",
        "    #augmentation=get_validation_augmentation(), try w/o augmentation\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, train_batch_size, shuffle=True, num_workers=4)\n",
        "#train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=12) - used for running iwth 1 image\n",
        "valid_loader = DataLoader(valid_dataset, val_batch_size, shuffle=False, num_workers=1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shuffled images are ['/content/IRDatasetFinal/train/a1635458864_013558_47.png', '/content/IRDatasetFinal/train/a1602782536_218305_47.png', '/content/IRDatasetFinal/train/a1622943255_231943_417_2.png', '/content/IRDatasetFinal/train/a1603211684_096642_42.png']\n",
            "shuffled masks are ['/content/IRDatasetFinal/trainannot/a1635458864_013558_47.png', '/content/IRDatasetFinal/trainannot/a1602782536_218305_47.png', '/content/IRDatasetFinal/trainannot/a1622943255_231943_417_2.png', '/content/IRDatasetFinal/trainannot/a1603211684_096642_42.png']\n",
            "shuffled images are ['/content/IRDatasetFinal/val/a1602782542_023886_47.png', '/content/IRDatasetFinal/val/a1603388796_213713.png', '/content/IRDatasetFinal/val/a1571760143_328819_45.png', '/content/IRDatasetFinal/val/a1603209327_972786_415_2.png']\n",
            "shuffled masks are ['/content/IRDatasetFinal/valannot/a1602782542_023886_47.png', '/content/IRDatasetFinal/valannot/a1603388796_213713.png', '/content/IRDatasetFinal/valannot/a1571760143_328819_45.png', '/content/IRDatasetFinal/valannot/a1603209327_972786_415_2.png']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzS1HykysUz"
      },
      "source": [
        "\n",
        "loss = smp.utils.losses.DiceLoss()\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.Adam([ \n",
        "    dict(params=model.parameters(), lr=0.0001),\n",
        "    #dict(params=model.parameters(), lr=0.001),  #changed learning rate to keep it consistent \n",
        "])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPhMq-P4yvZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82f1a08-7dfa-40fb-ce7c-ef699bffcae9"
      },
      "source": [
        "#THIS IS FOR TRAINING 1ST TIME \n",
        "\n",
        "#create train and valid epochs \n",
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, #use for 1st training. \n",
        "    #best_model, #use this for retraining\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    #best_model,\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "#train the model \n",
        "max_score = 0\n",
        "\n",
        "for i in range(0, epoch):\n",
        "    \n",
        "    print('\\nEpoch: {}'.format(i))\n",
        "    train_logs = train_epoch.run(train_loader)\n",
        "    valid_logs = valid_epoch.run(valid_loader)\n",
        "    \n",
        "    # do something (save model, change lr, etc.)\n",
        "    if max_score < valid_logs['iou_score']:\n",
        "        max_score = valid_logs['iou_score']\n",
        "        torch.save(model, '/content/drive/MyDrive/Models/pspnet/pspnet-7classes-finalpapersubmission_2019&2020_shuffled/best_model.pth')\n",
        "        print('Model saved!')\n",
        "        \n",
        "    if i == 25:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-5\n",
        "        print('Decrease decoder learning rate to 1e-5!')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "train:   0%|          | 0/2609 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 2609/2609 [41:40<00:00,  1.04it/s, dice_loss - 0.05954, iou_score - 0.8918]\n",
            "valid: 100%|██████████| 1491/1491 [06:11<00:00,  4.01it/s, dice_loss - 0.04744, iou_score - 0.9117]\n",
            "Model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvUHOusgy3-2"
      },
      "source": [
        "### THIS IS FOR TRAINING 2ND TIME ON. \n",
        "\n",
        "#load the model\n",
        "best_model = torch.load('/content/drive/MyDrive/Models/pspnet/pspnet-7classes-finalpapersubmission_2019&2020_shuffled/best_model.pth')\n",
        "\n",
        "## below line is if we want to run using cpu\n",
        "#best_model = torch.load('/content/drive/MyDrive/Models/pspnet-pytorch/best_model.pth', map_location=torch.device('cpu')) \n",
        "\n",
        "#create the train and valid epochs\n",
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    #model, #use for 1st training. \n",
        "    best_model, #use this for retraining\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    #model, \n",
        "    best_model,\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "#train the model  \n",
        "max_score = 0\n",
        "\n",
        "for i in range(0, epoch):\n",
        "    \n",
        "    print('\\nEpoch: {}'.format(i))\n",
        "    train_logs = train_epoch.run(train_loader)\n",
        "    valid_logs = valid_epoch.run(valid_loader)\n",
        "    \n",
        "    # do something (save model, change lr, etc.)\n",
        "    if max_score < valid_logs['iou_score']:\n",
        "        max_score = valid_logs['iou_score']\n",
        "        torch.save(model, '/content/drive/MyDrive/Models/pspnet/pspnet-7classes-finalpapersubmission_2019&2020_shuffled/best_model.pth')\n",
        "        print('Model saved!')\n",
        "        \n",
        "    if i == 25:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-5\n",
        "        print('Decrease decoder learning rate to 1e-5!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXdduyOcy7pI"
      },
      "source": [
        "'''\n",
        "#inference\n",
        "# create test dataset\n",
        "test_dataset = Dataset(\n",
        "    x_test_dir, \n",
        "    y_test_dir, \n",
        "    #x_train_dir, - check with trained images to see if that gives any prediction\n",
        "    #y_train_dir,\n",
        "    #augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        "    \n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False, num_workers=1)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0sxWoPLy-VN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8dcf69-fa91-4f4b-9fbd-f8ea50a7d5de"
      },
      "source": [
        "'''\n",
        "#inference\n",
        "## evaluate model on test set. THIS IS WHERE YOU ARE TESTING THE TEST DATA SET ON THE MODEL\n",
        "test_epoch = smp.utils.train.ValidEpoch(\n",
        "    model=best_model,\n",
        "    loss=loss,\n",
        "    metrics=metrics,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "logs = test_epoch.run(test_dataloader)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid: 100%|██████████| 491/491 [00:14<00:00, 34.20it/s, dice_loss - 0.01979, iou_score - 0.9634]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfeP4lx8zA88"
      },
      "source": [
        "'''\n",
        "#inference \n",
        "# test dataset without transformations for image visualization\n",
        "test_dataset_vis = Dataset(\n",
        "    x_test_dir, y_test_dir, \n",
        "    #x_train_dir, y_train_dir, - checking with train dataset \n",
        "    classes=CLASSES,\n",
        "    \n",
        ")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgQgkpxiptDY"
      },
      "source": [
        "'''\n",
        "#inference\n",
        "from torchvision.utils import save_image\n",
        "import torch\n",
        "import torchvision\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJBflBNqzDRo"
      },
      "source": [
        "'''\n",
        "## THIS CODE PREDICTS ONE CLASS AT A TIME. NOT NEEDED NOW DUE TO FUSION. KEEPING IT TO DEBUG AS NECESSARY\n",
        "#CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "import numpy as np \n",
        "DEVICE = 'cuda'\n",
        "for i in range(1):\n",
        "    #the below line will visualize for any random sample. we need specific. so i have removed it. keep changing value of n. \n",
        "    # i think value of i should loop thru no of classes.but we cna check it later\n",
        "    #n = np.random.choice(len(test_dataset))\n",
        "    n = 2\n",
        "    image_name = os.listdir(x_test_dir)[n]\n",
        "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
        "    image, gt_mask = test_dataset[n]\n",
        "        \n",
        "    #image_vis = test_dataset_vis[10][0].astype('uint8')\n",
        "    #image, gt_mask = test_dataset[10]\n",
        "    '''\n",
        "    print('gt mask before squeeze - 0', np.unique(gt_mask[0]))\n",
        "    print('gt mask before squeeze - 1', np.unique(gt_mask[1]))\n",
        "    print('gt mask before squeeze - 2', np.unique(gt_mask[2]))\n",
        "    print('gt mask before squeeze - 3', np.unique(gt_mask[3]))\n",
        "    print('gt mask before squeeze - 4', np.unique(gt_mask[4]))\n",
        "    print('gt mask before squeeze - 5', np.unique(gt_mask[5]))\n",
        "    print('gt mask before squeeze - 6', np.unique(gt_mask[6]))\n",
        "    '''\n",
        "    if (np.unique(gt_mask[3]).any() == 1) or (np.unique(gt_mask[4]).all() == 0):  \n",
        "       print (\"there are living ob and obstacles\")\n",
        "       print(\"Name of the image being predicted\",os.listdir(x_test_dir)[n])\n",
        "       gt_mask = gt_mask.squeeze()\n",
        "       x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "       #x_tensor = torch.from_numpy(image).unsqueeze(0)\n",
        "       pr_mask = best_model.predict(x_tensor)\n",
        "       pr_mask1 = (pr_mask.squeeze().cpu().numpy().round())\n",
        "       print(\"type of pr_mask\",type(pr_mask))\n",
        "       #print(pr_mask[0])\n",
        "       #print(\"x shape = {0}\".format(x_tensor.shape))\n",
        "       #print (\"shape of pr mask\", pr_mask.shape)\n",
        "       #print (\"shape of gt mask\", gt_mask.shape)\n",
        "       #print (\"shape of pr mask1\", pr_mask1.shape)\n",
        "\n",
        "       #print(\"unique values of pr mask\", np.unique(pr_mask[0]))\n",
        "       #print(\"unique values of gt mask\", np.unique(gt_mask))\n",
        "       #print(\"unique values of pr mask\", np.unique(pr_mask1[0]))\n",
        "       print(\"unique values of pr mask1\", np.unique(pr_mask1[2]))\n",
        "\n",
        "       #want to save the array as an image#\n",
        "       img1 = pr_mask.squeeze()\n",
        "       #print (\"shape of img1 after squeeze\", img1.shape)\n",
        "       #print (\"type of img1 after squeeze\", type(img1))\n",
        "       # we are only saving 1 class at a time. since we want obstacles, i have put 3 in the array below#\n",
        "       #save_image(img1[3], '/content/drive/MyDrive/TheIRDataset-Inferences/pspnet/Prediction-obstacles/0--run1--1570555911.468876_1_2.png') \n",
        "       #instead of hardcoding the name, dynamically getting it from the value of n\n",
        "       #save_image(img1[3], '/content/drive/MyDrive/IRdatasetmini-inferences/pspnet/Prediction-obstacles/0--run1--' + image_name)\n",
        "    \n",
        "       #CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "       #              0.      1.       2.        3             4                 5           6\n",
        "       visualize(\n",
        "          image=image_vis, \n",
        "          ground_truth_mask=gt_mask[2], \n",
        "          predicted_mask=pr_mask1[2]\n",
        "               )\n",
        "       \n",
        "       visualize2(\n",
        "          #image=image_vis, \n",
        "          #ground_truth_mask=gt_mask[3], \n",
        "          predicted_mask=pr_mask1[2]\n",
        "               )\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu0LcU_dladE"
      },
      "source": [
        "'''\n",
        "mask_image = cv2.imread(\"/content/img1.png\", cv2.IMREAD_UNCHANGED)\n",
        "print(np.unique(mask_image))\n",
        "print(mask_image)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXaRFYzpoZeN"
      },
      "source": [
        "The predicted mask of dimensions (7,256,320) is collapsed into (256,320) 2D array. The (7,256,320) 3D array is a binary mask for each type of class. 7 layers correspond to 7 classes. E.g. in 0th layer, if sky is present (class id=0) then that array entry will be 1 everything else will be 0. So when we convert this 3D array to 2D array, 0th slice is multiplied by 1, 1st slice by 2, .. so on and so forth. This preserves the information of 0th class ID. Then all the 2D slices are summed up to obtain (256,320) mask image. Then, 1 is subtracted from this 2D array so that the class IDs match with ground truth.\n",
        "Now, theoretically speaking, there should not be any 0 values in predicted mask because we are multiplying sky mask (class id=0) with 1 so where ever sky is present, it will produce 1 and rest of the entries will be occupied by other classes. However, there are some spurious entries with value 0 in the prediction mask. There are anomaly or noise. I need to decide what should be done with it.\n",
        "1. Check if PSPNet is happy with ground truth having 0 as valid class id. Many a times, there is hidden assumption that the class id will not be 0. WaSR had such assumption which was explicitly stated. If PSPNet has such assumption then I will need to regenerate/preprocess all the ground truth masks such that sky is 1, water is 2 etc. and then we should try PSPNet again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4zr_MnrhHW9"
      },
      "source": [
        "'''\n",
        "#inference \n",
        "# THIS COMBINES ALL CLASSES IN 1. FOR CHECKING EACH IMAGE SEPARATELY, CHANGE VALUE OF N. \n",
        "#FOR CHECKING ALL IMAGES TOGETHER, YOU WILL HAVE TO REMOVE N. AND USE I EVERYWHERE\n",
        "#CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "import numpy as np \n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "rawimagefilenamelist = sorted(os.listdir(x_test_dir))\n",
        "for i in range(50,100):\n",
        "    #the below line will visualize for any random sample. we need specific. so i have removed it. keep changing value of n. \n",
        "    # i think value of i should loop thru no of classes.but we cna check it later\n",
        "    #n = np.random.choice(len(test_dataset))\n",
        "    #print(\"the iteration that is running is\", i)\n",
        "    image_name = rawimagefilenamelist[i]\n",
        "    image_vis = test_dataset_vis[i][0].astype('uint8')\n",
        "    image, gt_mask = test_dataset[i]\n",
        "        \n",
        "    #image_vis = test_dataset_vis[10][0].astype('uint8')\n",
        "    #image, gt_mask = test_dataset[10]\n",
        "    \n",
        "    #print('gt mask before squeeze', np.unique(gt_mask))\n",
        "    # only throw images if the ground truth has both obstacle and living obstacle \n",
        "    if (np.unique(gt_mask[3]).any() == 1) and (np.unique(gt_mask[4]).any() == 1):\n",
        "\n",
        "    # only throw images if the ground truth has  obstacle but no living obstacle \n",
        "    #if (np.unique(gt_mask[3]).any() == 1) and (np.unique(gt_mask[4]).all() == 0) - checking if atleast an ob can be found:  \n",
        "    #if (np.unique(gt_mask[3]).any() == 1) and (np.unique(gt_mask[4]).all() == 0) - checking if atleast an ob can be found:  \n",
        "    #if (np.unique(gt_mask[6]).any() == 1 -- no self in test dataset ):  \n",
        "       print(\"There are living obstacles and obstacles in the image\",os.listdir(x_test_dir)[i])\n",
        "       print(\"The position of the file is\", i)\n",
        "       gt_mask = gt_mask.squeeze()\n",
        "       image_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "       #x_tensor = torch.from_numpy(image).unsqueeze(0)\n",
        "       pr_mask = best_model.predict(image_tensor)\n",
        "       pr_mask1 = (pr_mask.squeeze().cpu().numpy().round())\n",
        "       #print(\"type of pr_mask\", type(pr_mask))\n",
        "       #print(\"shape of pr_mask\", pr_mask.shape)\n",
        "       #print(\"type of pr_mask1\", type(pr_mask1))\n",
        "       #print(\"shape of pr_mask1\", pr_mask1.shape)\n",
        "    \n",
        "       #print(\"unique values of pr mask\", np.unique(pr_mask))\n",
        "       #print(\"unique values of gt mask\", np.unique(gt_mask))\n",
        "       #print(\"unique values of pr mask\", np.unique(pr_mask1))\n",
        "       #print(\"unique values of pr mask1\", np.unique(pr_mask1))\n",
        "\n",
        "       #want to save the array as an image#\n",
        "       pr_mask1_sqz = pr_mask1.squeeze()\n",
        "       #print (\"shape of pr_mask1_sqz after squeeze\", pr_mask1_sqz.shape)\n",
        "       #print (\"type of pr_mask1_sqz after squeeze\", type(pr_mask1_sqz))\n",
        "       #print(\"pr_mask1_sqz\", np.unique(pr_mask1_sqz[2, :, :]))\n",
        "       collapsed_pr_mask1_sqz = 1 * pr_mask1_sqz[0, :, :] + 2 * pr_mask1_sqz[1, :, :] + 3 * pr_mask1_sqz[2, :, :] + 4 * pr_mask1_sqz[3, :, :] + 5 * pr_mask1_sqz[4, :, :] + 6 * pr_mask1_sqz[5, :, :] + 7 * pr_mask1_sqz[6, :, :]    \n",
        "       #print (\"pr_mask1_sqz[0, 80, :] = \", pr_mask1_sqz[0, 109, :])\n",
        "       #print (\"pr_mask1_sqz[1, 80, :] = \", pr_mask1_sqz[1, 109, :])\n",
        "       #print (\"pr_mask1_sqz[2, 80, :] = \", pr_mask1_sqz[2, 109, :])\n",
        "       #print (\"pr_mask1_sqz[3, 80, :] = \", pr_mask1_sqz[3, 109, :])\n",
        "       #print (\"pr_mask1_sqz[4, 80, :] = \", pr_mask1_sqz[4, 109, :])\n",
        "       #print (\"pr_mask1_sqz[5, 80, :] = \", pr_mask1_sqz[5, 109, :])\n",
        "       #print (\"pr_mask1_sqz[6, 80, :] = \", pr_mask1_sqz[6, 109, :])\n",
        "       #print (\"Shape of collapsed_pr_mask1_sqz = {0}\".format(collapsed_pr_mask1_sqz.shape))\n",
        "       #print (\"collapsed_pr_mask1_sqz = {0}\".format(collapsed_pr_mask1_sqz))\n",
        "       #print (\"collapsed_pr_mask1_sqz[118] = {0}\".format(collapsed_pr_mask1_sqz[118, :]))\n",
        "       #print (\"Unique values in collapsed_pr_mask1_sqz = \", np.unique(collapsed_pr_mask1_sqz))\n",
        "       # Account for labeling correction: In earlier step, pr_mask_sqz[0, :, :] has to be multiplied by 1 to retain on-pixels. So \n",
        "       # subsequent classes also get (class no. + 1) as multiplier. So now subtract 1 to get back original ids in predicted\n",
        "       result = np.where(collapsed_pr_mask1_sqz==0)\n",
        "       print(\"indices where collapsed_pr_mask1_sqz has 0 value = \", result)\n",
        "       #print (\"values in collapsed_pr_mask1_sqz = \", collapsed_pr_mask1_sqz[25, :])\n",
        "       #print (\"Unique values in collapsed_pr_mask1_sqz = \", np.unique(collapsed_pr_mask1_sqz))\n",
        "       # mask image\n",
        "       collapsed_pr_mask1_sqz = collapsed_pr_mask1_sqz - 1 # why is this needed? \n",
        "       #print (\"Unique values in collapsed_pr_mask1_sqz after subtraction = \", np.unique(collapsed_pr_mask1_sqz))\n",
        "       #since pspnet is giving some pixels value of 0, which means none of the slices are being inferred in that pixel. \n",
        "       #changing that to sky so that there is no -1 in the array for visualization\n",
        "       collapsed_pr_mask1_sqz = np.where(collapsed_pr_mask1_sqz == -1, 0, collapsed_pr_mask1_sqz)\n",
        "       #print (\"Unique values in collapsed_pr_mask1_sqz after changing -1 to 0 = \", np.unique(collapsed_pr_mask1_sqz))\n",
        "       img_collapse = Image.fromarray(collapsed_pr_mask1_sqz) \n",
        "       img_collapse = img_collapse.convert(\"L\")\n",
        "\n",
        "       #Saving the category ids in an image for programatic IoU check \n",
        "       img_collapse.save('/content/drive/MyDrive/IRDatasetFinal-Inferences/pspnet/program/' + str(i) + '--run2020-1--' + image_name)\n",
        "       \n",
        "       #CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "       #              0.      1.       2.        3             4                 5           6\n",
        "       visualize(\n",
        "           image=image_vis,\n",
        "           ground_truth_mask=gt_mask[3], \n",
        "           #predicted_mask=collapsed_pr_mask1_sqz\n",
        "               )\n",
        "     \n",
        "       visualize(\n",
        "           #image=image_vis,\n",
        "           ground_truth_mask=gt_mask[4], \n",
        "           #predicted_mask=collapsed_pr_mask1_sqz\n",
        "               )\n",
        "     \n",
        "       visualize2(\n",
        "           #image=image_vis, \n",
        "           #ground_truth_mask=gt_mask[3], \n",
        "           predicted_mask=collapsed_pr_mask1_sqz,\n",
        "           sequence = i,\n",
        "           imagename = image_name \n",
        "                )\n",
        "       \n",
        " '''      \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n692E7if8KT4"
      },
      "source": [
        "'''\n",
        "# out of 490, there are 340 images where there are empty pixels. Below code is to check if there are any 0s.\n",
        "import numpy as np \n",
        "import PIL\n",
        "from PIL import Image\n",
        "DEVICE = 'cuda'\n",
        "count = 0\n",
        "for i in range(0,490):\n",
        "    image_name = os.listdir(x_test_dir)[i]\n",
        "    image_vis = test_dataset_vis[i][0].astype('uint8')\n",
        "    image, gt_mask = test_dataset[i]\n",
        "    #print(\"The position of the file is\", i)\n",
        "    gt_mask = gt_mask.squeeze()\n",
        "    image_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    pr_mask = best_model.predict(image_tensor)\n",
        "    pr_mask1 = (pr_mask.squeeze().cpu().numpy().round())\n",
        "    pr_mask1_sqz = pr_mask1.squeeze()\n",
        "    collapsed_pr_mask1_sqz = 1 * pr_mask1_sqz[0, :, :] + 2 * pr_mask1_sqz[1, :, :] + 3 * pr_mask1_sqz[2, :, :] + 4 * pr_mask1_sqz[3, :, :] + 5 * pr_mask1_sqz[4, :, :] + 6 * pr_mask1_sqz[5, :, :] + 7 * pr_mask1_sqz[6, :, :]    \n",
        "    result = np.where(collapsed_pr_mask1_sqz==0)\n",
        "    \n",
        "    if (len(result[0]) > 0): \n",
        "        #print(\"indices where collapsed_pr_mask1_sqz has 0 value = \", result)\n",
        "        count = count+1 \n",
        "print(\"total count of images where there are 0s\" , count)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}