{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pspnet-pytorch-Finalpaper_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5q7pJj3TFR6T1l5RCILbj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/Datasetpaper-final/blob/main/pspnet_pytorch_Finalpaper_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYlYx-tlnrte"
      },
      "source": [
        "#Encoder - se_resnext50_32x4d, Decoder - FPN, 100 epochs. FPN = next generation pspnet \n",
        "This is the final code that should be used\n",
        "\n",
        "## you need to create the model folder in the drive till the actual file path. model.save only creates the file not the folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-T-DGiWU3kt"
      },
      "source": [
        "https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb\n",
        "\n",
        "** our masked image is gray scale with 1 channel (height * width*channel=1). and the pixel values are equal to the class ids. Each pixle value is only 1 value and not (a*b*c). The CV2.imread, with a flag od 0 that reads it as a gray scalae does not work for our images although it works for the other dataset\n",
        "\n",
        "## image size: (256, 320) #it's height and width\n",
        "## Mask - channel - , size - extension.png, gray scale\n",
        "\n",
        "## Use GPU so that CUDA is available\n",
        "\n",
        "##Run1\n",
        "1. train - 2570, val - 740, test - 370 , Epochs - 500 \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time - 11 hrs  hrs. Testing time -  8 min\n",
        "\n",
        "##Run2\n",
        "1. train - , val - , test - , Epochs - 100\n",
        "2. NO preloaded / imagenet weights \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time - hrs. Testing time -   min\n",
        "\n",
        "##Run3\n",
        "1. train -24814 , val - 7090, test - 3544, Epochs - 76(failed on 77th)\n",
        "2. NO preloaded / imagenet weights \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time - 18 hrs. Testing time -  1hr 15 min\n",
        "\n",
        "##Run4\n",
        "1. train - 3438 , val - 982 , test - 492 , Epochs - 50\n",
        "2. NO preloaded / imagenet weights \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time - 1 hrs. Testing time -  5 min\n",
        "Model - pspnet-pytorch-finalpapersubmission_V2\n",
        "##Even with 100 epocs 346 images of the 491 have a few pixels that are not mapped and have a value of 0. need to check in the architecture\n",
        "\n",
        "##Run5\n",
        "size - actual size , 2020 only \n",
        "1. train - 3438 , val - 982 , test - 492 , Epochs - 50\n",
        "2. NO preloaded / imagenet weights \n",
        "2. batchsize - 10 for train, 5 for validation. \n",
        "3. train running with 4 worker threads, validation with 1.\n",
        "4. i am not using augmentation, and no existing encoder weights.\n",
        "5. Training time - 1 hrs. Testing time -  5 min\n",
        "Model - pspnet-pytorch-finalpapersubmission_V2\n",
        "##Even with 100 epocs 346 images of the 491 have a few pixels that are not mapped and have a value of 0. need to check in the architecture\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11nEKAj4mXcL"
      },
      "source": [
        "#To Do - \n",
        "1. Right now dice loss and iau for accuracy. check if we need to select any other metrics \n",
        "2. We are not plotting the losses / accuracy. need to check what to use to get the plots. should that be consistent across all the models? in that case should we use tensorboard?\n",
        "3. collapse all classes into 1 complete image \n",
        "4. get the file name automatically based on the run number, instead of having to manually add it while saving the image on drive\n",
        "5. Ensure that the sort order is the same between unet and pspnet so that we know that its the same image \n",
        "6. HUman objects are really bad in pspnet. many ob are also missed if they are small. check if any parameter needs to be changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWlqTikUn49F"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBdE7JFgimEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2305b091-8617-4520-c6c8-c3038f45cc1e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWmjSDMi34CQ"
      },
      "source": [
        "# Install required libs\n",
        "#!pip install -U segmentation-models-pytorch==0.2.0 albumentations --user"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U  albumentations --user"
      ],
      "metadata": {
        "id": "pjp_rM3wEJ62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bshuhFbf4HRM"
      },
      "source": [
        "#!pip uninstall -y segmentation-models-pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz0adR3tUZVl"
      },
      "source": [
        "#!git clone https://github.com/Cadene/pretrained-models.pytorch.git\n",
        "#!git clone https://github.com/qubvel/segmentation_models.pytorch\n",
        "#!git clone https://github.com/alexgkendall/SegNet-Tutorial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7thoJfVhVlga"
      },
      "source": [
        "#!pip install -U segmentation-models-pytorch albumentations --user\n",
        "#!pip uninstall -y segmentation-models-pytorch\n",
        "#!pip install segmentation_models_pytorch\n",
        "\n",
        "#!pip install git+https://github.com/IvyGongoogle/pretrained-models.pytorch\n",
        "#!pip install git+https://github.com/lukemelas/EfficientNet-PyTorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u3NegrvTlOl"
      },
      "source": [
        "#!pip install efficientnet-pytorch\n",
        "#!pip install pretrainedmodels "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3_Ebd77C2Hb"
      },
      "source": [
        "#!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "\n",
        "#!pip install git+https://github.com/qubvel/segmentation_models.pytorch@4f94380815f831605f4641b7193df2eccd5652a3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch@a288d337821716ad67125127b5dd96a1cd833391 #pick the older version of git commit"
      ],
      "metadata": {
        "id": "JSN3nS4nD7WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9xamsKiE9c6"
      },
      "source": [
        "# Restart Runtime now from the MENU !!! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkIUWW0c6IJN"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import segmentation_models_pytorch as smp"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCpIpta03RyH"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIQFK_CNoJa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635ac297-0517-45a7-b445-bd6660719543"
      },
      "source": [
        "torch.cuda.is_available()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/train.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/trainannot.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/val.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/valannot.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "PUg6t9rmb5yz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''only for testing humans\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/test.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/testannot.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "'''"
      ],
      "metadata": {
        "id": "4xc5_c7jkyFZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''only for testing humans\n",
        "!cp \"/content/IRDatasetFinal/test/a1571167609_333506.png\" \"/content/drive/MyDrive/psp-image/train\"\n",
        "!cp /content/IRDatasetFinal/testannot/a1571167609_333506.png /content/drive/MyDrive/psp-image/trainannot\n",
        "\n",
        "!cp \"/content/IRDatasetFinal/test/a1571161484_881385.png\" \"/content/drive/MyDrive/psp-image/train\"\n",
        "!cp /content/IRDatasetFinal/testannot/a1571161484_881385.png /content/drive/MyDrive/psp-image/trainannot\n",
        "\n",
        "\n",
        "!cp /content/IRDatasetFinal/train/a1570555799_168847_417_2.png /content/drive/MyDrive/psp-image/val\n",
        "!cp /content/IRDatasetFinal/trainannot/a1570555799_168847_417_2.png /content/drive/MyDrive/psp-image/valannot\n",
        "'''"
      ],
      "metadata": {
        "id": "IxDbjNHSjJmT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''only for testing humans\n",
        "!cp /content/drive/MyDrive/psp-image/train/*.png /content/drive/MyDrive/psp-image/test\n",
        "!cp /content/drive/MyDrive/psp-image/trainannot/*.png /content/drive/MyDrive/psp-image/testannot\n",
        "'''"
      ],
      "metadata": {
        "id": "S9B-XWRQt127"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "APP_FOLDER = '/content/IRDatasetFinal/val'\n",
        "totalFiles = 0\n",
        "totalDir = 0\n",
        "\n",
        "for base, dirs, files in os.walk(APP_FOLDER):\n",
        "    print('Searching in : ',base)\n",
        "    for directories in dirs:\n",
        "        totalDir += 1\n",
        "    for Files in files:\n",
        "        totalFiles += 1\n",
        "   \n",
        "\n",
        "print('Total number of files',totalFiles)\n",
        "print('Total Number of directories',totalDir)\n",
        "print('Total:',(totalDir + totalFiles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tTWrwVLewvQ",
        "outputId": "9d9f190e-5d65-4a03-d870-426ac0886ebf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching in :  /content/IRDatasetFinal/val\n",
            "Total number of files 7910\n",
            "Total Number of directories 0\n",
            "Total: 7910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c8jJFC435wa"
      },
      "source": [
        "#DATA_DIR = '/content/drive/MyDrive/TheIRDatasetMini'\n",
        "#DATA_DIR = '/content/drive/MyDrive/TheIRDatasetMini_backup'\n",
        "#DATA_DIR = '/content/drive/MyDrive/psp-1.0'\n",
        "DATA_DIR = '/content/IRDatasetFinal'\n",
        "#DATA_DIR = '/content/drive/MyDrive/psp-image' - for testing humans \n",
        "# load repo with data if it is not exists\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print('Loading data...')\n",
        "    os.system('git clone https://github.com/alexgkendall/SegNet-Tutorial ./data')\n",
        "    print('Done!')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyxerGLW4FLz"
      },
      "source": [
        "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
        "y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
        "y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n",
        "\n",
        "#x_test_dir = os.path.join(DATA_DIR, 'test')\n",
        "#y_test_dir = os.path.join(DATA_DIR, 'testannot')\n",
        "\n",
        "NUM_CLASSES = 7 \n",
        "CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "\n",
        "#image_width = 320\n",
        "#image_height = 256\n",
        "  \n",
        "image_width = 640\n",
        "image_height = 512\n",
        "\n",
        "train_batch_size = 10 \n",
        "#train_batch_size = 1\n",
        "val_batch_size = 5 \n",
        "#val_batch_size = 1 \n",
        "\n",
        "#learning rate \n",
        "lr=0.001 \n",
        "\n",
        "#epoch +1 as range is used \n",
        "epoch = 50\n",
        " \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHvu2KtF4gzr"
      },
      "source": [
        "# helper function for data visualization\n",
        "### TO DO - CHANGE THE COLOR MAP IN MATPLOTLIB FOR CONSISTENCY\n",
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzxOM82S0M7X"
      },
      "source": [
        "'''\n",
        "#inference\n",
        "import PIL\n",
        "#def visualize2(input_image, input_mask, predicted_mask):\n",
        "def visualize2(predicted_mask,sequence,imagename):\n",
        "    #NUM_CLASSES = 7\n",
        "    \"\"\"Quick utility to display a model's prediction. we need color masked images, so displaying both color and black and white\"\"\"\n",
        "                   \n",
        "    #0 - sky(dark.blue). \n",
        "    #1 - water(light.blue).   \n",
        "    #2 - bridge(yellow).   \n",
        "    #3 - obstacle(purple).  \n",
        "    #4- living ob(green).  \n",
        "    #5- backgnd (orange). \n",
        "    #6 - self(pink)\n",
        "    #7 - null values. white\n",
        "    # Colors are same as segments.ai scheme\n",
        "    \n",
        "    label_colours = [(0,113,188), (216,82,24), (236,176,31), (125, 46, 141), (118, 171, 47), (161, 19, 46), (255,0,0), (255,255,255)]  \n",
        "    \n",
        "    #print(\"mask shape = \", np.shape(predicted_mask))\n",
        "    #print(\"Unique pixel values = \", np.unique(predicted_mask))\n",
        "    #print(\"Type of mask = \", type(predicted_mask))\n",
        "    \n",
        "\n",
        "    img1 = PIL.Image.new('RGB', (image_width, image_height))\n",
        "    pixels = img1.load()\n",
        "    #print(type(pixels))\n",
        "    #print(pixels[0,0])\n",
        "    #converting incoming predicted mask from float to an int \n",
        "    int_predicted_mask = predicted_mask.astype(int)\n",
        "    #for j_, j in enumerate(predicted_mask[:, :]):\n",
        "    for j_, j in enumerate(int_predicted_mask[:, :]):  \n",
        "        #print (j_, j)\n",
        "        for k_, k in enumerate(j):\n",
        "              #print(k_, k)\n",
        "              if k < NUM_CLASSES:\n",
        "                 pixels[k_,j_] = label_colours[k]\n",
        "    output = np.array(img1)\n",
        "    \n",
        "    #print(\"this is the colored inferred image\")\n",
        "    display(img1)\n",
        "\n",
        "    #save inferred image \n",
        "    #img1.save('/content/drive/MyDrive/IRdatasetmini-inferences/pspnet/picture/' + str(sequence) + '--run1--' + imagename)\n",
        "    img1.save('/content/drive/MyDrive/IRDatasetFinal-Inferences/pspnet/picture/' + str(sequence) + '--run2020-1--' + imagename)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex_P5Fvi4mF4"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLwpyW_P4vtk"
      },
      "source": [
        "class Dataset(BaseDataset):\n",
        "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
        "    \n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_values (list): values of classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline \n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing \n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    #0 - sky 1 - water 2 - bridge 3 - obstacle 4- living ob  5- backgnd 6 - self  \n",
        "               \n",
        "    \n",
        "    #CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "    #NUM_CLASSES = 7              \n",
        "    \n",
        "    def __init__(\n",
        "            self, \n",
        "            images_dir, \n",
        "            masks_dir, \n",
        "            classes=None, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        #removing the sort and replacing with random so that images get trained in a shuffled way\n",
        "        self.images_fps = sorted([os.path.join(images_dir, image_id) for image_id in self.ids])\n",
        "        self.masks_fps = sorted([os.path.join(masks_dir, image_id) for image_id in self.ids])\n",
        "\n",
        "        indices_train = random.sample(range(len(self.images_fps)), len(self.images_fps))\n",
        "        self.images_fps = list(map(self.images_fps.__getitem__, indices_train))\n",
        "        self.masks_fps = list(map(self.masks_fps.__getitem__, indices_train))\n",
        "        \n",
        "        mask1 = cv2.imread(self.masks_fps[0], cv2.IMREAD_UNCHANGED)\n",
        "        #print(\"true unique value of mask\", np.unique(mask1))\n",
        "        # convert str names to class values on masks\n",
        "        #self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "        self.class_values = [CLASSES.index(cls.lower()) for cls in classes]\n",
        "\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "        #print(\"shuffled images are\", self.images_fps[1:5])\n",
        "        #print(\"shuffled masks are\", self.masks_fps[1:5])\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        image = cv2.imread(self.images_fps[i])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        #HAD TO CHANGE FLAG 0 TO IM_READ_UNCHANGED BELOW as it was not reading the pixel values otherwise \n",
        "        # mask = cv2.imread(self.masks_fps[i], 0)\n",
        "        mask = cv2.imread(self.masks_fps[i], cv2.IMREAD_UNCHANGED)\n",
        "        #print (\"masked image values\", mask)\n",
        "        #print (\"masked shape\", mask.shape)\n",
        "        #print(\"masked unique values\", np.unique(mask))\n",
        "        #print(\"pixel values of mask\", mask)\n",
        "        \n",
        "        # extract certain classes from mask (e.g. cars)\n",
        "        masks = [(mask == v) for v in self.class_values]\n",
        "        #print (\"masks extraction\", masks)\n",
        "        mask = np.stack(masks, axis=-1).astype('float')\n",
        "        #print(\"stacks of classes for mask\", masks)\n",
        "        #print (\"shape of mask stack\",mask.shape )\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            #print('file name is', self.masks_fps[i])\n",
        "            #print('unique values in mask', np.unique(mask))\n",
        "            maskOfLivingObs = mask[:, :, 4] # 4 is class id of living obs\n",
        "            #print('Unique values in 4th layer of stacked mask={0}'.format(np.unique(maskOfLivingObs)))\n",
        "            #print('Shape of maskOfLivingObs={0}'.format(np.shape(maskOfLivingObs)))\n",
        "            #print(\"Mask of mid-area={0}\".format(maskOfLivingObs))\n",
        "            \n",
        "        return image, mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ids)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7UyZe2T5FWv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "478eb113-ee86-4a31-af7a-7c298c564ec1"
      },
      "source": [
        "'''\n",
        "# Lets look at data we have. it can only put 1 class at a time \n",
        "\n",
        "#CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "dataset = Dataset(x_train_dir, y_train_dir, classes= [\"background\"])\n",
        "\n",
        "image, mask = dataset[2569] # get some sample. \n",
        "\n",
        "\n",
        "'''\n",
        "#just picking 1 image from the train dataset. Also selecting only class 'obstacle'. \n",
        "#the stacks of classes change when i change the class to sky. so the visualize is only showing the array for that specific class. \n",
        "# it's boolean - yes or No. Sp it will say if every pixel belongs to that class or not. it's able to print the ground truth properly\n",
        "#which means dataset is created properly. \n",
        "'''\n",
        "#print ('masked array' , np.unique(mask))\n",
        "visualize(\n",
        "    image=image, \n",
        "    cars_mask=mask.squeeze(),\n",
        "   \n",
        ")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#print ('masked array' , np.unique(mask))\\nvisualize(\\n    image=image, \\n    cars_mask=mask.squeeze(),\\n   \\n)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHskJX8u6foc"
      },
      "source": [
        "import albumentations as albu"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OSF_x2YHu9p"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG-uG-R06kNm"
      },
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "\n",
        "        albu.HorizontalFlip(p=0.5),\n",
        "\n",
        "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
        "\n",
        "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
        "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
        "\n",
        "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
        "        albu.IAAPerspective(p=0.5),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.CLAHE(p=1),\n",
        "                albu.RandomBrightness(p=1),\n",
        "                albu.RandomGamma(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.IAASharpen(p=1),\n",
        "                albu.Blur(blur_limit=3, p=1),\n",
        "                albu.MotionBlur(blur_limit=3, p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "\n",
        "        albu.OneOf(\n",
        "            [\n",
        "                albu.RandomContrast(p=1),\n",
        "                albu.HueSaturationValue(p=1),\n",
        "            ],\n",
        "            p=0.9,\n",
        "        ),\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
        "    test_transform = [\n",
        "        albu.PadIfNeeded(384, 480)\n",
        "    ]\n",
        "    return albu.Compose(test_transform)\n",
        "\n",
        "\n",
        "'''\n",
        "OpenCV img = cv2.imread(path) loads an image with HWC-layout (height, width, channels), \n",
        "while Pytorch requires CHW-layout. So we have to do np.transpose(image,(2,0,1))\n",
        "for HWC->CHW transformation.\n",
        "'''\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construct preprocessing transform\n",
        "    \n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUqMy0pc6qAj"
      },
      "source": [
        "#### Visualize resulted augmented images and masks - thows error after 1st picture. \n",
        "#THIS IS NOT NECESSARY. plus our images have some augmentation\n",
        "'''\n",
        "augmented_dataset = Dataset(\n",
        "    x_train_dir, \n",
        "    y_train_dir, \n",
        "    augmentation=get_training_augmentation(), \n",
        "    classes=['obstacle'],\n",
        ")\n",
        "\n",
        "# same image with different random transforms\n",
        "#for i in range(3):  (Had to comment this out as i only have 1 image. will remove during actual training)\n",
        "for i in range(3):\n",
        "   image, mask = augmented_dataset[1]\n",
        "   visualize(image=image, mask=mask.squeeze(-1))\n",
        "   '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2rVoCdu_u5r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "89885225-14e0-464b-feee-b5471aa4725e"
      },
      "source": [
        "ENCODER = 'se_resnext50_32x4d'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "#CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "ACTIVATION = 'softmax2d' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# create segmentation model with pretrained encoder\n",
        "model = smp.FPN(\n",
        "    encoder_name=ENCODER, \n",
        "    #encoder_weights=ENCODER_WEIGHTS, Do not want to use existing encoder weights. \n",
        "    encoder_weights=None, \n",
        "    classes=len(CLASSES), \n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
        "'''if i use existing encoder weights, then it doesn't display all classes in the prediction for some reason. \n",
        "    have not found out exactly why. but perhaps std classes as sky / water are being taken up from the weights. \n",
        "    obstacle , backngd are not standard and therefore may not be plotted. So i will train the model from scratch\n",
        "''' \n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"if i use existing encoder weights, then it doesn't display all classes in the prediction for some reason. \\n    have not found out exactly why. but perhaps std classes as sky / water are being taken up from the weights. \\n    obstacle , backngd are not standard and therefore may not be plotted. So i will train the model from scratch\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwDZ6rvsyowG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c39b102-532b-4be3-acf9-9bd5c81b3e5e"
      },
      "source": [
        "train_dataset = Dataset(\n",
        "    x_train_dir, \n",
        "    y_train_dir, \n",
        "    #augmentation=get_training_augmentation(), try w/o augmentation \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES, \n",
        ")\n",
        "\n",
        "valid_dataset = Dataset(\n",
        "    x_valid_dir, \n",
        "    y_valid_dir, \n",
        "    #augmentation=get_validation_augmentation(), try w/o augmentation\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, train_batch_size, shuffle=True, num_workers=4)\n",
        "#train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=12) - used for running iwth 1 image\n",
        "valid_loader = DataLoader(valid_dataset, val_batch_size, shuffle=False, num_workers=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzS1HykysUz"
      },
      "source": [
        "\n",
        "loss = smp.utils.losses.DiceLoss()\n",
        "metrics = [\n",
        "    #smp.utils.metrics.IoU(threshold=0.5),\n",
        "    smp.utils.metrics.IoU(threshold=0.3),\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.Adam([ \n",
        "    dict(params=model.parameters(), lr=0.0001),\n",
        "    #dict(params=model.parameters(), lr=0.001),  #changed learning rate to keep it consistent \n",
        "])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPhMq-P4yvZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1a025e-8d56-455c-d1b7-5fd9c42754f7"
      },
      "source": [
        "#THIS IS FOR TRAINING 1ST TIME \n",
        "\n",
        "#create train and valid epochs \n",
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, #use for 1st training. \n",
        "    #best_model, #use this for retraining\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    #best_model,\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "#train the model \n",
        "max_score = 0\n",
        "\n",
        "for i in range(0, epoch):\n",
        "    \n",
        "    print('\\nEpoch: {}'.format(i))\n",
        "    train_logs = train_epoch.run(train_loader)\n",
        "    valid_logs = valid_epoch.run(valid_loader)\n",
        "    \n",
        "    # do something (save model, change lr, etc.)\n",
        "    if max_score < valid_logs['iou_score']:\n",
        "        max_score = valid_logs['iou_score']\n",
        "        torch.save(model, '/content/drive/MyDrive/Models/pspnet/pspnet-many-epochs/best_model.pth')\n",
        "        print('Model saved!')\n",
        "        \n",
        "    if i == 25:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-5\n",
        "        print('Decrease decoder learning rate to 1e-5!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "train:   0%|          | 0/2768 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 2768/2768 [44:16<00:00,  1.04it/s, dice_loss - 0.08306, iou_score - 0.8553]\n",
            "valid: 100%|██████████| 1582/1582 [06:27<00:00,  4.08it/s, dice_loss - 0.06777, iou_score - 0.875]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 1\n",
            "train: 100%|██████████| 2768/2768 [44:23<00:00,  1.04it/s, dice_loss - 0.05889, iou_score - 0.8892]\n",
            "valid: 100%|██████████| 1582/1582 [06:29<00:00,  4.06it/s, dice_loss - 0.06337, iou_score - 0.8824]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 2\n",
            "train: 100%|██████████| 2768/2768 [44:26<00:00,  1.04it/s, dice_loss - 0.05298, iou_score - 0.8997]\n",
            "valid: 100%|██████████| 1582/1582 [06:27<00:00,  4.09it/s, dice_loss - 0.05915, iou_score - 0.8899]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 3\n",
            "train: 100%|██████████| 2768/2768 [44:28<00:00,  1.04it/s, dice_loss - 0.05131, iou_score - 0.9027]\n",
            "valid: 100%|██████████| 1582/1582 [06:28<00:00,  4.07it/s, dice_loss - 0.05737, iou_score - 0.8931]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 4\n",
            "train: 100%|██████████| 2768/2768 [44:28<00:00,  1.04it/s, dice_loss - 0.04982, iou_score - 0.9055]\n",
            "valid: 100%|██████████| 1582/1582 [06:32<00:00,  4.03it/s, dice_loss - 0.05624, iou_score - 0.8951]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 5\n",
            "train: 100%|██████████| 2768/2768 [44:30<00:00,  1.04it/s, dice_loss - 0.04882, iou_score - 0.9073]\n",
            "valid: 100%|██████████| 1582/1582 [06:28<00:00,  4.08it/s, dice_loss - 0.05614, iou_score - 0.8954]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 6\n",
            "train: 100%|██████████| 2768/2768 [44:31<00:00,  1.04it/s, dice_loss - 0.04757, iou_score - 0.9096]\n",
            "valid: 100%|██████████| 1582/1582 [06:33<00:00,  4.02it/s, dice_loss - 0.05535, iou_score - 0.8967]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 7\n",
            "train: 100%|██████████| 2768/2768 [44:30<00:00,  1.04it/s, dice_loss - 0.04721, iou_score - 0.9102]\n",
            "valid: 100%|██████████| 1582/1582 [06:31<00:00,  4.04it/s, dice_loss - 0.05501, iou_score - 0.8973]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 8\n",
            "train: 100%|██████████| 2768/2768 [44:31<00:00,  1.04it/s, dice_loss - 0.04637, iou_score - 0.9117]\n",
            "valid: 100%|██████████| 1582/1582 [06:26<00:00,  4.09it/s, dice_loss - 0.05487, iou_score - 0.8976]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 9\n",
            "train: 100%|██████████| 2768/2768 [44:31<00:00,  1.04it/s, dice_loss - 0.04572, iou_score - 0.9129]\n",
            "valid: 100%|██████████| 1582/1582 [06:34<00:00,  4.01it/s, dice_loss - 0.06821, iou_score - 0.8741]\n",
            "\n",
            "Epoch: 10\n",
            "train: 100%|██████████| 2768/2768 [44:29<00:00,  1.04it/s, dice_loss - 0.04589, iou_score - 0.9127]\n",
            "valid: 100%|██████████| 1582/1582 [06:35<00:00,  4.00it/s, dice_loss - 0.05407, iou_score - 0.8991]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 11\n",
            "train: 100%|██████████| 2768/2768 [44:32<00:00,  1.04it/s, dice_loss - 0.0448, iou_score - 0.9146]\n",
            "valid: 100%|██████████| 1582/1582 [06:34<00:00,  4.01it/s, dice_loss - 0.05444, iou_score - 0.8984]\n",
            "\n",
            "Epoch: 12\n",
            "train: 100%|██████████| 2768/2768 [44:32<00:00,  1.04it/s, dice_loss - 0.04448, iou_score - 0.9153]\n",
            "valid: 100%|██████████| 1582/1582 [06:44<00:00,  3.91it/s, dice_loss - 0.05641, iou_score - 0.895]\n",
            "\n",
            "Epoch: 13\n",
            "train: 100%|██████████| 2768/2768 [44:35<00:00,  1.03it/s, dice_loss - 0.04444, iou_score - 0.9153]\n",
            "valid: 100%|██████████| 1582/1582 [06:39<00:00,  3.96it/s, dice_loss - 0.05376, iou_score - 0.8997]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 14\n",
            "train: 100%|██████████| 2768/2768 [44:35<00:00,  1.03it/s, dice_loss - 0.04376, iou_score - 0.9166]\n",
            "valid: 100%|██████████| 1582/1582 [06:37<00:00,  3.98it/s, dice_loss - 0.05344, iou_score - 0.9002]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 15\n",
            "train: 100%|██████████| 2768/2768 [44:34<00:00,  1.03it/s, dice_loss - 0.04349, iou_score - 0.9171]\n",
            "valid: 100%|██████████| 1582/1582 [06:32<00:00,  4.03it/s, dice_loss - 0.05376, iou_score - 0.8997]\n",
            "\n",
            "Epoch: 16\n",
            "train: 100%|██████████| 2768/2768 [44:34<00:00,  1.04it/s, dice_loss - 0.04362, iou_score - 0.9168]\n",
            "valid: 100%|██████████| 1582/1582 [06:35<00:00,  4.00it/s, dice_loss - 0.05348, iou_score - 0.9002]\n",
            "\n",
            "Epoch: 17\n",
            "train: 100%|██████████| 2768/2768 [44:35<00:00,  1.03it/s, dice_loss - 0.04289, iou_score - 0.9182]\n",
            "valid: 100%|██████████| 1582/1582 [06:43<00:00,  3.92it/s, dice_loss - 0.05358, iou_score - 0.9001]\n",
            "\n",
            "Epoch: 18\n",
            "train: 100%|██████████| 2768/2768 [44:34<00:00,  1.04it/s, dice_loss - 0.0428, iou_score - 0.9184]\n",
            "valid: 100%|██████████| 1582/1582 [06:32<00:00,  4.03it/s, dice_loss - 0.05335, iou_score - 0.9004]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 19\n",
            "train: 100%|██████████| 2768/2768 [44:36<00:00,  1.03it/s, dice_loss - 0.04308, iou_score - 0.9179]\n",
            "valid: 100%|██████████| 1582/1582 [06:38<00:00,  3.97it/s, dice_loss - 0.05325, iou_score - 0.9006]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 20\n",
            "train: 100%|██████████| 2768/2768 [44:37<00:00,  1.03it/s, dice_loss - 0.04269, iou_score - 0.9186]\n",
            "valid: 100%|██████████| 1582/1582 [06:37<00:00,  3.98it/s, dice_loss - 0.05296, iou_score - 0.9011]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 21\n",
            "train: 100%|██████████| 2768/2768 [44:32<00:00,  1.04it/s, dice_loss - 0.04232, iou_score - 0.9193]\n",
            "valid: 100%|██████████| 1582/1582 [06:31<00:00,  4.05it/s, dice_loss - 0.05314, iou_score - 0.9009]\n",
            "\n",
            "Epoch: 22\n",
            "train: 100%|██████████| 2768/2768 [44:33<00:00,  1.04it/s, dice_loss - 0.04241, iou_score - 0.9191]\n",
            "valid: 100%|██████████| 1582/1582 [06:29<00:00,  4.06it/s, dice_loss - 0.05326, iou_score - 0.9006]\n",
            "\n",
            "Epoch: 23\n",
            "train: 100%|██████████| 2768/2768 [44:32<00:00,  1.04it/s, dice_loss - 0.04212, iou_score - 0.9197]\n",
            "valid: 100%|██████████| 1582/1582 [06:27<00:00,  4.08it/s, dice_loss - 0.05319, iou_score - 0.9008]\n",
            "\n",
            "Epoch: 24\n",
            "train: 100%|██████████| 2768/2768 [44:32<00:00,  1.04it/s, dice_loss - 0.04186, iou_score - 0.9201]\n",
            "valid: 100%|██████████| 1582/1582 [06:36<00:00,  3.99it/s, dice_loss - 0.05316, iou_score - 0.9008]\n",
            "\n",
            "Epoch: 25\n",
            "train: 100%|██████████| 2768/2768 [44:33<00:00,  1.04it/s, dice_loss - 0.04174, iou_score - 0.9204]\n",
            "valid: 100%|██████████| 1582/1582 [06:33<00:00,  4.03it/s, dice_loss - 0.05291, iou_score - 0.9012]\n",
            "Model saved!\n",
            "Decrease decoder learning rate to 1e-5!\n",
            "\n",
            "Epoch: 26\n",
            "train: 100%|██████████| 2768/2768 [44:32<00:00,  1.04it/s, dice_loss - 0.0412, iou_score - 0.9214]\n",
            "valid: 100%|██████████| 1582/1582 [06:32<00:00,  4.04it/s, dice_loss - 0.05265, iou_score - 0.9017]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 27\n",
            "train: 100%|██████████| 2768/2768 [44:32<00:00,  1.04it/s, dice_loss - 0.04103, iou_score - 0.9217]\n",
            "valid: 100%|██████████| 1582/1582 [06:27<00:00,  4.08it/s, dice_loss - 0.0526, iou_score - 0.9018]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 28\n",
            "train:   6%|▋         | 176/2768 [02:52<41:50,  1.03it/s, dice_loss - 0.04319, iou_score - 0.9178]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvUHOusgy3-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d8ed45-3da5-4486-d016-45eb37537ea4"
      },
      "source": [
        "### THIS IS FOR TRAINING 2ND TIME ON. \n",
        "\n",
        "#load the model\n",
        "best_model = torch.load('/content/drive/MyDrive/Models/pspnet/pspnet-old-version/best_model.pth')\n",
        "#best_model = torch.load('/content/drive/MyDrive/Models/pspnet/pspnet-7classes-finalpapersubmission_2019&2020_shuffled/best_model.pth')\n",
        "\n",
        "## below line is if we want to run using cpu\n",
        "#best_model = torch.load('/content/drive/MyDrive/Models/pspnet-pytorch/best_model.pth', map_location=torch.device('cpu')) \n",
        "\n",
        "#create the train and valid epochs\n",
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    #model, #use for 1st training. \n",
        "    best_model, #use this for retraining\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    #model, \n",
        "    best_model,\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "#train the model  \n",
        "max_score = 0\n",
        "\n",
        "for i in range(0, epoch):\n",
        "    \n",
        "    print('\\nEpoch: {}'.format(i))\n",
        "    train_logs = train_epoch.run(train_loader)\n",
        "    valid_logs = valid_epoch.run(valid_loader)\n",
        "    \n",
        "    # do something (save model, change lr, etc.)\n",
        "    if max_score < valid_logs['iou_score']:\n",
        "        max_score = valid_logs['iou_score']\n",
        "        torch.save(model, '/content/drive/MyDrive/Models/pspnet/pspnet-old-version/best_model.pth')\n",
        "        print('Model saved!')\n",
        "        \n",
        "    if i == 25:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-5\n",
        "        print('Decrease decoder learning rate to 1e-5!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "train:   0%|          | 0/2768 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 2768/2768 [43:45<00:00,  1.05it/s, dice_loss - 0.02363, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:58<00:00,  3.78it/s, dice_loss - 0.03517, iou_score - 0.9336]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 1\n",
            "train: 100%|██████████| 2768/2768 [43:51<00:00,  1.05it/s, dice_loss - 0.02363, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:32<00:00,  4.03it/s, dice_loss - 0.03515, iou_score - 0.9337]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 2\n",
            "train: 100%|██████████| 2768/2768 [43:55<00:00,  1.05it/s, dice_loss - 0.02363, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:42<00:00,  3.93it/s, dice_loss - 0.03517, iou_score - 0.9336]\n",
            "\n",
            "Epoch: 3\n",
            "train: 100%|██████████| 2768/2768 [43:56<00:00,  1.05it/s, dice_loss - 0.02363, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:36<00:00,  3.99it/s, dice_loss - 0.03516, iou_score - 0.9337]\n",
            "\n",
            "Epoch: 4\n",
            "train: 100%|██████████| 2768/2768 [43:57<00:00,  1.05it/s, dice_loss - 0.02363, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:38<00:00,  3.97it/s, dice_loss - 0.03516, iou_score - 0.9337]\n",
            "\n",
            "Epoch: 5\n",
            "train: 100%|██████████| 2768/2768 [43:58<00:00,  1.05it/s, dice_loss - 0.02363, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:34<00:00,  4.01it/s, dice_loss - 0.03515, iou_score - 0.9337]\n",
            "\n",
            "Epoch: 6\n",
            "train: 100%|██████████| 2768/2768 [43:58<00:00,  1.05it/s, dice_loss - 0.02363, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:36<00:00,  3.99it/s, dice_loss - 0.03518, iou_score - 0.9336]\n",
            "\n",
            "Epoch: 7\n",
            "train: 100%|██████████| 2768/2768 [43:58<00:00,  1.05it/s, dice_loss - 0.02364, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:39<00:00,  3.96it/s, dice_loss - 0.03516, iou_score - 0.9337]\n",
            "\n",
            "Epoch: 8\n",
            "train: 100%|██████████| 2768/2768 [43:57<00:00,  1.05it/s, dice_loss - 0.02363, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:29<00:00,  4.06it/s, dice_loss - 0.03516, iou_score - 0.9337]\n",
            "\n",
            "Epoch: 9\n",
            "train: 100%|██████████| 2768/2768 [43:55<00:00,  1.05it/s, dice_loss - 0.02363, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:28<00:00,  4.07it/s, dice_loss - 0.0352, iou_score - 0.9336]\n",
            "\n",
            "Epoch: 10\n",
            "train: 100%|██████████| 2768/2768 [43:56<00:00,  1.05it/s, dice_loss - 0.02363, iou_score - 0.9543]\n",
            "valid: 100%|██████████| 1582/1582 [06:34<00:00,  4.01it/s, dice_loss - 0.03517, iou_score - 0.9336]\n",
            "\n",
            "Epoch: 11\n",
            "train:   1%|▏         | 37/2768 [00:38<43:37,  1.04it/s, dice_loss - 0.02205, iou_score - 0.9573]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXdduyOcy7pI"
      },
      "source": [
        "'''\n",
        "#inference\n",
        "# create test dataset\n",
        "test_dataset = Dataset(\n",
        "    x_test_dir, \n",
        "    y_test_dir, \n",
        "    #x_train_dir, - check with trained images to see if that gives any prediction\n",
        "    #y_train_dir,\n",
        "    #augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    classes=CLASSES,\n",
        "    \n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False, num_workers=1)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0sxWoPLy-VN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8dcf69-fa91-4f4b-9fbd-f8ea50a7d5de"
      },
      "source": [
        "'''\n",
        "#inference\n",
        "## evaluate model on test set. THIS IS WHERE YOU ARE TESTING THE TEST DATA SET ON THE MODEL\n",
        "test_epoch = smp.utils.train.ValidEpoch(\n",
        "    model=best_model,\n",
        "    loss=loss,\n",
        "    metrics=metrics,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "logs = test_epoch.run(test_dataloader)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid: 100%|██████████| 491/491 [00:14<00:00, 34.20it/s, dice_loss - 0.01979, iou_score - 0.9634]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfeP4lx8zA88"
      },
      "source": [
        "'''\n",
        "#inference \n",
        "# test dataset without transformations for image visualization\n",
        "test_dataset_vis = Dataset(\n",
        "    x_test_dir, y_test_dir, \n",
        "    #x_train_dir, y_train_dir, - checking with train dataset \n",
        "    classes=CLASSES,\n",
        "    \n",
        ")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgQgkpxiptDY"
      },
      "source": [
        "'''\n",
        "#inference\n",
        "from torchvision.utils import save_image\n",
        "import torch\n",
        "import torchvision\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJBflBNqzDRo"
      },
      "source": [
        "'''\n",
        "## THIS CODE PREDICTS ONE CLASS AT A TIME. NOT NEEDED NOW DUE TO FUSION. KEEPING IT TO DEBUG AS NECESSARY\n",
        "#CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "import numpy as np \n",
        "DEVICE = 'cuda'\n",
        "for i in range(1):\n",
        "    #the below line will visualize for any random sample. we need specific. so i have removed it. keep changing value of n. \n",
        "    # i think value of i should loop thru no of classes.but we cna check it later\n",
        "    #n = np.random.choice(len(test_dataset))\n",
        "    n = 2\n",
        "    image_name = os.listdir(x_test_dir)[n]\n",
        "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
        "    image, gt_mask = test_dataset[n]\n",
        "        \n",
        "    #image_vis = test_dataset_vis[10][0].astype('uint8')\n",
        "    #image, gt_mask = test_dataset[10]\n",
        "    '''\n",
        "    print('gt mask before squeeze - 0', np.unique(gt_mask[0]))\n",
        "    print('gt mask before squeeze - 1', np.unique(gt_mask[1]))\n",
        "    print('gt mask before squeeze - 2', np.unique(gt_mask[2]))\n",
        "    print('gt mask before squeeze - 3', np.unique(gt_mask[3]))\n",
        "    print('gt mask before squeeze - 4', np.unique(gt_mask[4]))\n",
        "    print('gt mask before squeeze - 5', np.unique(gt_mask[5]))\n",
        "    print('gt mask before squeeze - 6', np.unique(gt_mask[6]))\n",
        "    '''\n",
        "    if (np.unique(gt_mask[3]).any() == 1) or (np.unique(gt_mask[4]).all() == 0):  \n",
        "       print (\"there are living ob and obstacles\")\n",
        "       print(\"Name of the image being predicted\",os.listdir(x_test_dir)[n])\n",
        "       gt_mask = gt_mask.squeeze()\n",
        "       x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "       #x_tensor = torch.from_numpy(image).unsqueeze(0)\n",
        "       pr_mask = best_model.predict(x_tensor)\n",
        "       pr_mask1 = (pr_mask.squeeze().cpu().numpy().round())\n",
        "       print(\"type of pr_mask\",type(pr_mask))\n",
        "       #print(pr_mask[0])\n",
        "       #print(\"x shape = {0}\".format(x_tensor.shape))\n",
        "       #print (\"shape of pr mask\", pr_mask.shape)\n",
        "       #print (\"shape of gt mask\", gt_mask.shape)\n",
        "       #print (\"shape of pr mask1\", pr_mask1.shape)\n",
        "\n",
        "       #print(\"unique values of pr mask\", np.unique(pr_mask[0]))\n",
        "       #print(\"unique values of gt mask\", np.unique(gt_mask))\n",
        "       #print(\"unique values of pr mask\", np.unique(pr_mask1[0]))\n",
        "       print(\"unique values of pr mask1\", np.unique(pr_mask1[2]))\n",
        "\n",
        "       #want to save the array as an image#\n",
        "       img1 = pr_mask.squeeze()\n",
        "       #print (\"shape of img1 after squeeze\", img1.shape)\n",
        "       #print (\"type of img1 after squeeze\", type(img1))\n",
        "       # we are only saving 1 class at a time. since we want obstacles, i have put 3 in the array below#\n",
        "       #save_image(img1[3], '/content/drive/MyDrive/TheIRDataset-Inferences/pspnet/Prediction-obstacles/0--run1--1570555911.468876_1_2.png') \n",
        "       #instead of hardcoding the name, dynamically getting it from the value of n\n",
        "       #save_image(img1[3], '/content/drive/MyDrive/IRdatasetmini-inferences/pspnet/Prediction-obstacles/0--run1--' + image_name)\n",
        "    \n",
        "       #CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "       #              0.      1.       2.        3             4                 5           6\n",
        "       visualize(\n",
        "          image=image_vis, \n",
        "          ground_truth_mask=gt_mask[2], \n",
        "          predicted_mask=pr_mask1[2]\n",
        "               )\n",
        "       \n",
        "       visualize2(\n",
        "          #image=image_vis, \n",
        "          #ground_truth_mask=gt_mask[3], \n",
        "          predicted_mask=pr_mask1[2]\n",
        "               )\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu0LcU_dladE"
      },
      "source": [
        "'''\n",
        "mask_image = cv2.imread(\"/content/img1.png\", cv2.IMREAD_UNCHANGED)\n",
        "print(np.unique(mask_image))\n",
        "print(mask_image)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXaRFYzpoZeN"
      },
      "source": [
        "The predicted mask of dimensions (7,256,320) is collapsed into (256,320) 2D array. The (7,256,320) 3D array is a binary mask for each type of class. 7 layers correspond to 7 classes. E.g. in 0th layer, if sky is present (class id=0) then that array entry will be 1 everything else will be 0. So when we convert this 3D array to 2D array, 0th slice is multiplied by 1, 1st slice by 2, .. so on and so forth. This preserves the information of 0th class ID. Then all the 2D slices are summed up to obtain (256,320) mask image. Then, 1 is subtracted from this 2D array so that the class IDs match with ground truth.\n",
        "Now, theoretically speaking, there should not be any 0 values in predicted mask because we are multiplying sky mask (class id=0) with 1 so where ever sky is present, it will produce 1 and rest of the entries will be occupied by other classes. However, there are some spurious entries with value 0 in the prediction mask. There are anomaly or noise. I need to decide what should be done with it.\n",
        "1. Check if PSPNet is happy with ground truth having 0 as valid class id. Many a times, there is hidden assumption that the class id will not be 0. WaSR had such assumption which was explicitly stated. If PSPNet has such assumption then I will need to regenerate/preprocess all the ground truth masks such that sky is 1, water is 2 etc. and then we should try PSPNet again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4zr_MnrhHW9"
      },
      "source": [
        "'''\n",
        "#inference \n",
        "# THIS COMBINES ALL CLASSES IN 1. FOR CHECKING EACH IMAGE SEPARATELY, CHANGE VALUE OF N. \n",
        "#FOR CHECKING ALL IMAGES TOGETHER, YOU WILL HAVE TO REMOVE N. AND USE I EVERYWHERE\n",
        "#CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "import numpy as np \n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "DEVICE = 'cuda'\n",
        "rawimagefilenamelist = sorted(os.listdir(x_test_dir))\n",
        "for i in range(50,100):\n",
        "    #the below line will visualize for any random sample. we need specific. so i have removed it. keep changing value of n. \n",
        "    # i think value of i should loop thru no of classes.but we cna check it later\n",
        "    #n = np.random.choice(len(test_dataset))\n",
        "    #print(\"the iteration that is running is\", i)\n",
        "    image_name = rawimagefilenamelist[i]\n",
        "    image_vis = test_dataset_vis[i][0].astype('uint8')\n",
        "    image, gt_mask = test_dataset[i]\n",
        "        \n",
        "    #image_vis = test_dataset_vis[10][0].astype('uint8')\n",
        "    #image, gt_mask = test_dataset[10]\n",
        "    \n",
        "    #print('gt mask before squeeze', np.unique(gt_mask))\n",
        "    # only throw images if the ground truth has both obstacle and living obstacle \n",
        "    if (np.unique(gt_mask[3]).any() == 1) and (np.unique(gt_mask[4]).any() == 1):\n",
        "\n",
        "    # only throw images if the ground truth has  obstacle but no living obstacle \n",
        "    #if (np.unique(gt_mask[3]).any() == 1) and (np.unique(gt_mask[4]).all() == 0) - checking if atleast an ob can be found:  \n",
        "    #if (np.unique(gt_mask[3]).any() == 1) and (np.unique(gt_mask[4]).all() == 0) - checking if atleast an ob can be found:  \n",
        "    #if (np.unique(gt_mask[6]).any() == 1 -- no self in test dataset ):  \n",
        "       print(\"There are living obstacles and obstacles in the image\",os.listdir(x_test_dir)[i])\n",
        "       print(\"The position of the file is\", i)\n",
        "       gt_mask = gt_mask.squeeze()\n",
        "       image_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "       #x_tensor = torch.from_numpy(image).unsqueeze(0)\n",
        "       pr_mask = best_model.predict(image_tensor)\n",
        "       pr_mask1 = (pr_mask.squeeze().cpu().numpy().round())\n",
        "       #print(\"type of pr_mask\", type(pr_mask))\n",
        "       #print(\"shape of pr_mask\", pr_mask.shape)\n",
        "       #print(\"type of pr_mask1\", type(pr_mask1))\n",
        "       #print(\"shape of pr_mask1\", pr_mask1.shape)\n",
        "    \n",
        "       #print(\"unique values of pr mask\", np.unique(pr_mask))\n",
        "       #print(\"unique values of gt mask\", np.unique(gt_mask))\n",
        "       #print(\"unique values of pr mask\", np.unique(pr_mask1))\n",
        "       #print(\"unique values of pr mask1\", np.unique(pr_mask1))\n",
        "\n",
        "       #want to save the array as an image#\n",
        "       pr_mask1_sqz = pr_mask1.squeeze()\n",
        "       #print (\"shape of pr_mask1_sqz after squeeze\", pr_mask1_sqz.shape)\n",
        "       #print (\"type of pr_mask1_sqz after squeeze\", type(pr_mask1_sqz))\n",
        "       #print(\"pr_mask1_sqz\", np.unique(pr_mask1_sqz[2, :, :]))\n",
        "       collapsed_pr_mask1_sqz = 1 * pr_mask1_sqz[0, :, :] + 2 * pr_mask1_sqz[1, :, :] + 3 * pr_mask1_sqz[2, :, :] + 4 * pr_mask1_sqz[3, :, :] + 5 * pr_mask1_sqz[4, :, :] + 6 * pr_mask1_sqz[5, :, :] + 7 * pr_mask1_sqz[6, :, :]    \n",
        "       #print (\"pr_mask1_sqz[0, 80, :] = \", pr_mask1_sqz[0, 109, :])\n",
        "       #print (\"pr_mask1_sqz[1, 80, :] = \", pr_mask1_sqz[1, 109, :])\n",
        "       #print (\"pr_mask1_sqz[2, 80, :] = \", pr_mask1_sqz[2, 109, :])\n",
        "       #print (\"pr_mask1_sqz[3, 80, :] = \", pr_mask1_sqz[3, 109, :])\n",
        "       #print (\"pr_mask1_sqz[4, 80, :] = \", pr_mask1_sqz[4, 109, :])\n",
        "       #print (\"pr_mask1_sqz[5, 80, :] = \", pr_mask1_sqz[5, 109, :])\n",
        "       #print (\"pr_mask1_sqz[6, 80, :] = \", pr_mask1_sqz[6, 109, :])\n",
        "       #print (\"Shape of collapsed_pr_mask1_sqz = {0}\".format(collapsed_pr_mask1_sqz.shape))\n",
        "       #print (\"collapsed_pr_mask1_sqz = {0}\".format(collapsed_pr_mask1_sqz))\n",
        "       #print (\"collapsed_pr_mask1_sqz[118] = {0}\".format(collapsed_pr_mask1_sqz[118, :]))\n",
        "       #print (\"Unique values in collapsed_pr_mask1_sqz = \", np.unique(collapsed_pr_mask1_sqz))\n",
        "       # Account for labeling correction: In earlier step, pr_mask_sqz[0, :, :] has to be multiplied by 1 to retain on-pixels. So \n",
        "       # subsequent classes also get (class no. + 1) as multiplier. So now subtract 1 to get back original ids in predicted\n",
        "       result = np.where(collapsed_pr_mask1_sqz==0)\n",
        "       print(\"indices where collapsed_pr_mask1_sqz has 0 value = \", result)\n",
        "       #print (\"values in collapsed_pr_mask1_sqz = \", collapsed_pr_mask1_sqz[25, :])\n",
        "       #print (\"Unique values in collapsed_pr_mask1_sqz = \", np.unique(collapsed_pr_mask1_sqz))\n",
        "       # mask image\n",
        "       collapsed_pr_mask1_sqz = collapsed_pr_mask1_sqz - 1 # why is this needed? \n",
        "       #print (\"Unique values in collapsed_pr_mask1_sqz after subtraction = \", np.unique(collapsed_pr_mask1_sqz))\n",
        "       #since pspnet is giving some pixels value of 0, which means none of the slices are being inferred in that pixel. \n",
        "       #changing that to sky so that there is no -1 in the array for visualization\n",
        "       collapsed_pr_mask1_sqz = np.where(collapsed_pr_mask1_sqz == -1, 0, collapsed_pr_mask1_sqz)\n",
        "       #print (\"Unique values in collapsed_pr_mask1_sqz after changing -1 to 0 = \", np.unique(collapsed_pr_mask1_sqz))\n",
        "       img_collapse = Image.fromarray(collapsed_pr_mask1_sqz) \n",
        "       img_collapse = img_collapse.convert(\"L\")\n",
        "\n",
        "       #Saving the category ids in an image for programatic IoU check \n",
        "       img_collapse.save('/content/drive/MyDrive/IRDatasetFinal-Inferences/pspnet/program/' + str(i) + '--run2020-1--' + image_name)\n",
        "       \n",
        "       #CLASSES = ['sky', 'water', 'bridge', 'obstacle', 'living obstacle', 'background', 'self']\n",
        "       #              0.      1.       2.        3             4                 5           6\n",
        "       visualize(\n",
        "           image=image_vis,\n",
        "           ground_truth_mask=gt_mask[3], \n",
        "           #predicted_mask=collapsed_pr_mask1_sqz\n",
        "               )\n",
        "     \n",
        "       visualize(\n",
        "           #image=image_vis,\n",
        "           ground_truth_mask=gt_mask[4], \n",
        "           #predicted_mask=collapsed_pr_mask1_sqz\n",
        "               )\n",
        "     \n",
        "       visualize2(\n",
        "           #image=image_vis, \n",
        "           #ground_truth_mask=gt_mask[3], \n",
        "           predicted_mask=collapsed_pr_mask1_sqz,\n",
        "           sequence = i,\n",
        "           imagename = image_name \n",
        "                )\n",
        "       \n",
        " '''      \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n692E7if8KT4"
      },
      "source": [
        "'''\n",
        "# out of 490, there are 340 images where there are empty pixels. Below code is to check if there are any 0s.\n",
        "import numpy as np \n",
        "import PIL\n",
        "from PIL import Image\n",
        "DEVICE = 'cuda'\n",
        "count = 0\n",
        "for i in range(0,490):\n",
        "    image_name = os.listdir(x_test_dir)[i]\n",
        "    image_vis = test_dataset_vis[i][0].astype('uint8')\n",
        "    image, gt_mask = test_dataset[i]\n",
        "    #print(\"The position of the file is\", i)\n",
        "    gt_mask = gt_mask.squeeze()\n",
        "    image_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    pr_mask = best_model.predict(image_tensor)\n",
        "    pr_mask1 = (pr_mask.squeeze().cpu().numpy().round())\n",
        "    pr_mask1_sqz = pr_mask1.squeeze()\n",
        "    collapsed_pr_mask1_sqz = 1 * pr_mask1_sqz[0, :, :] + 2 * pr_mask1_sqz[1, :, :] + 3 * pr_mask1_sqz[2, :, :] + 4 * pr_mask1_sqz[3, :, :] + 5 * pr_mask1_sqz[4, :, :] + 6 * pr_mask1_sqz[5, :, :] + 7 * pr_mask1_sqz[6, :, :]    \n",
        "    result = np.where(collapsed_pr_mask1_sqz==0)\n",
        "    \n",
        "    if (len(result[0]) > 0): \n",
        "        #print(\"indices where collapsed_pr_mask1_sqz has 0 value = \", result)\n",
        "        count = count+1 \n",
        "print(\"total count of images where there are 0s\" , count)\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}