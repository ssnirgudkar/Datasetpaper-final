{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IR_7classes_Unet_Finalpaper_submission",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/Datasetpaper-final/blob/main/IR_7classes_Unet_Finalpaper_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nmjKJ8iztVY"
      },
      "source": [
        "Use this as the final code. - DOES NOT USE ANY PREEXISTING WEIGHTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o_-p5pVmn6r"
      },
      "source": [
        "1. no of classes if mentioned 3, then the labels **can only** have values of 0,1 and 2\n",
        "2. You can check the actual error using CPU, and not GPU\n",
        "3. softmax is good for image segmentation. it's a probablistic algorithm, so the losses also haev to be accordingly. all of the below are with softmax activation. optimizers are changed in the middle.  \n",
        "4. relu / leakyrelu, adam will work \n",
        "5. **Option 1-** leakyrelu - epoc 50, 62 images of which validation images 3. batch size 3 , kernel_initializer = 'he_normal'- , leakyrelu, sparse_categorical_crossentropy, optimizer - rmsprop: val_loss went from 87 to 0.2. But it was not continually reducing and was fluctutating.  Output segmentation was good. obstacles were showing up.\n",
        "6. **Option 2- **RELU, epoc 50, total images - 62, validation 3 images, sparse_categorical_crossentropy, kernel_initializer = 'he_normal', optimizer = rmsprop: Val_loss started from 127 and went to 0.08. was better converging than option 1, but still not always decreasing. Image output better than option 1\n",
        "7. Option 3: **RELU, epoc 50, total images - 62, validation 3 images, sparse_categorical_crossentropy, kernel_initializer = 'he_normal', optimizer = adam, defalt learn rate(lr): Val_loss started from 45 and went to 0.0528. The val loss continually decreased and converged. Image output better than option 1 and 2. Observations - reflection in water is shown as an obstacle. Bits of water are shown as cloud\n",
        "8. Option 4: **RELU, epoc 50, total images - 62, validation 3 images, categorical_crossentropy, kernel_initializer = 'he_normal', optimizer - adam, lr = 0.01: Val_loss started from 476 billion  and went to NAN in the 6th apoch. No segmentation output available. it was all black. Tried to change the loss to sparse_categorical_crossentropy with the same lr - 0.01, but the op was worse than that with default lr = 0.001  \n",
        "Kept the loss to sparse_categorical_crossentropy , but changed the lr to 0.005. That works comparable to lr = 0.001(default), but a touch lower. Some images are at par with default lr, some are not.   \n",
        "9. adam, lr = 0.001(default), softmax, kernel_initializer = 'uniform'. val_loss starts from 2.4 to 0.06. Some images with this are better than those using he_normal. \n",
        "**Recommendation - adam, sparse_categorical_crossentropy, relu, lr=. default = 0.001, kernal initilizer = 'he_normal' OR kernal initilizer = 'uniform'**\n",
        "\n",
        "Training results - \n",
        "1. Ran with 291 IR images (including night _ pilot IR. Mirror images of those + labeled) = 882 images in this directory. The checkpoint file is IR_1000images_uniform_adam_softmax_100_10.h5. Indictes, it was run for 1000 images, uniform kernel initializer, adam optimizer, softmax algo, 100 epochs with a batch size of 10. No of classes are 4 (0 thru 3). This was run with TPU. Per Epoc (86 batches with a batch size of 10 - estimate time was 10 min). Val_loss for epoc 1 was 5.0147 although average loss was 0.3 for each batch. Then ran with GPU to see which one to go with. GPU checking the unique label value took lot of time for 800 images. However, the training time - 1 epoch 1st batch with 86 batches with a batch size of 10 - estimate was only 2 min. Val loss was 7 although loss was 0.1. So GPU was much faster for training than TPU.Also TPU libraries to be imported change, plus TPU gives verbose explanbation of error as against GPU that just says NAN\n",
        "\n",
        "Results - IMP - numbering, and number of images shoild be exactly same between image and masks. The algo sorts it and uses it one to one. else we will see very bad losses and bad inference. \n",
        "Epoch 25/25\n",
        "837/837  - 188s 224ms/step - loss: 0.0197 - val_loss: 0.0206\n",
        "\n",
        "# Now i save model from run1, and use that saved model to restart training in run2. so each time i run, the model is getting retrained. i need to be careful that if i am running a test (w/o good data), i shld not save the model, else it will corrupt the earlier learning. \n",
        "\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "754aS1YJgOVn"
      },
      "source": [
        "# Image segmentation with a U-Net-like architecture\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2019/03/20<br>\n",
        "**Last modified:** 2020/04/20<br>\n",
        "**Description:** Image segmentation model trained from scratch on the Oxford Pets dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jUdvZcqUxIb"
      },
      "source": [
        "##Run1 - No preloaded weights\n",
        "train - 2570, val - 740, test - 370 , Epochs - 500 \n",
        "Training time - 5 hrs\n",
        "##Run 2 - No preloaded weights\n",
        "train - , val - , test - , Epochs - 100\n",
        "Training time -  failed due to NAN \n",
        "##Run 3 - No preloaded weights - 24K + images\n",
        "train - 24814, val - 7090, test - 3544, Epochs - 76\n",
        "Training time -  14 hrs, Testing time - failed with OOM after 20 min with GPU , after 23 min with TPU. Now running with TPU - High Ram. this completed in 20 min\n",
        "##Run 4 - No preloaded weights - resize + mirror  \n",
        "train - 3438  , val - 982  , test - 492 , Epochs - 50\n",
        "Training time - 1 hr 22 min  hrs, Testing time - 5 min\n",
        "Model - Unet-7classes-finalpapersubmission_V1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQcQdRZQgOVo"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XufPVSlGtx0e",
        "outputId": "35d1f898-6d56-412b-fef3-20d41a87bf93"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"keras version is\", tf.keras.__version__)\n",
        "print (\"tf version is\", tf.__version__) \n",
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keras version is 2.7.0\n",
            "tf version is 2.7.0\n",
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTVPl0nyi9Ms",
        "outputId": "a90851d1-44cc-4ecb-d077-d28eb4f0cf23"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "#!cd \"drive/My Drive/PhD/IRLabeledDataset\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQgZp6JWgOVp"
      },
      "source": [
        "## Prepare paths of input images and target segmentation masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKBranBjhihg"
      },
      "source": [
        "'''this is a test code to check syntax \n",
        "flag = 0 \n",
        "ideal_label_array = [0,1,2,3]\n",
        "target_label_array = [0,1,2,3,5]\n",
        "if(set(target_label_array).issubset(set(ideal_label_array))):\n",
        "  flag = 0\n",
        "else:    \n",
        "  flag = 1 \n",
        "  print(\"Error in label\", target_label_array ) \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qGXBC0LgOVp"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "input_dir = \"/content/drive/MyDrive/TheIRDatasetMini_backup/train\" # directory containing 2570 IR input images\n",
        "target_dir = \"/content/drive/MyDrive/TheIRDatasetMini_backup/trainannot\" # directory containing 2570 segmented IR images\n",
        "\n",
        "input_dir_val = \"/content/drive/MyDrive/TheIRDatasetMini_backup/val\" # directory containing 2570 IR input images\n",
        "target_dir_val = \"/content/drive/MyDrive/TheIRDatasetMini_backup/valannot\" # directory containing 2570 segmented IR images\n",
        "\n",
        "input_dir_test = \"/content/drive/MyDrive/TheIRDatasetMini_backup/test\" # directory containing 2570 IR input images\n",
        "target_dir_test = \"/content/drive/MyDrive/TheIRDatasetMini_backup/testannot\" # directory containing 2570 segmented IR images\n",
        "\n",
        "img_size = (256, 320) #it's height and width. it's a 1/2 sized image from the original image from segment.ai\n",
        "#img_size = (512, 640) #it's height and width. This is the original image from segment.ai\n",
        "img_size_width_ht = (320,256)\n",
        "\n",
        "num_classes = 7\n",
        "batch_size = 10\n",
        "test_batch_size = 2\n",
        "epochs = 100 \n",
        "\n",
        "# check for distinct label values in all masked files \n",
        "def getFullyQualifiedImagePaths(image_dir):\n",
        "  return sorted([ os.path.join(image_dir, fname)\n",
        "                  for fname in os.listdir(image_dir)\n",
        "                  if fname.endswith(\".png\")\n",
        "                ])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "XtbQNg-uvU2O",
        "outputId": "955fe3a5-3c69-4800-9d0e-794c0b901d5a"
      },
      "source": [
        "#prints how many files are in the train batch\n",
        "input_img_paths = getFullyQualifiedImagePaths(input_dir)\n",
        "target_img_paths = getFullyQualifiedImagePaths(target_dir)\n",
        "\n",
        "print(\"Number of train images:\", len(input_img_paths))\n",
        "print(\"Number of train masks:\", len(target_img_paths))\n",
        "\n",
        "#prints how many files are in the validation batch\n",
        "input_val_paths = getFullyQualifiedImagePaths(input_dir_val)\n",
        "target_val_paths = getFullyQualifiedImagePaths(target_dir_val)\n",
        "\n",
        "print(\"Number of validation images:\", len(input_val_paths))\n",
        "print(\"Number of validation masks:\", len(target_val_paths))\n",
        "\n",
        "#prints how many files are in the test batch\n",
        "input_test_paths = getFullyQualifiedImagePaths(input_dir_test)\n",
        "target_test_paths = getFullyQualifiedImagePaths(target_dir_test)\n",
        "\n",
        "#prints how many files are in the batch\n",
        "print(\"Number of test images:\", len(input_test_paths))\n",
        "print(\"Number of test masks:\", len(target_test_paths))\n",
        "\n",
        "\n",
        "\n",
        "#prints the name of 10 sets of input and labeled file\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)\n",
        "\n",
        "#print((cv2.imread('/content/drive/MyDrive/TheIRDatasetMini/trainannot/j02972132_1_41_2.png',cv2.IMREAD_UNCHANGED)))\n",
        "\n",
        "\n",
        "'''\n",
        "#COMMENTING THIS CODE AS THE CHECK HAS ALREADY BEEN DONE DURING PREPROCESSING\n",
        "#below code checks if the labels are in the 1 through 3 range (4 classes - sky,water,object,background). \n",
        "#Because if not, then model training gives a NAN error.\n",
        "#it displays the label file that has an error, and the values of the label\n",
        "ideal_label_array = [0,1,2,3,4,5,6] # depends on the number of classes\n",
        "flag = 0 \n",
        "\n",
        "for target_path in (target_img_paths):\n",
        "    target_label_array = (np.unique(cv2.imread(target_path,cv2.IMREAD_UNCHANGED)))\n",
        "    if(set(target_label_array).issubset(set(ideal_label_array))):\n",
        "       flag +=1\n",
        "       #print(\"good label\", flag)\n",
        "    else:    \n",
        "       flag +=1\n",
        "       print(\"Error in label\", flag,target_path,target_label_array )  \n",
        "'''\n",
        "'''\n",
        "for target_path in (target_val_paths):\n",
        "    target_label_array = (np.unique(cv2.imread(target_path,cv2.IMREAD_UNCHANGED)))\n",
        "    if(set(target_label_array).issubset(set(ideal_label_array))):\n",
        "       flag +=1\n",
        "       #print(\"good label\", flag)\n",
        "    else:    \n",
        "       flag +=1\n",
        "       print(\"Error in label\", flag,target_path,target_label_array )  \n",
        "      \n",
        "\n",
        "'''\n",
        "'''\n",
        "#COMMENTING THIS CODE AS THE CHECK HAS ALREADY BEEN DONE DURING PREPROCESSING\n",
        "#below code checks which testing files have living obstacles and obstacles. this way we can use them for prediction\n",
        "ideal_label_array = [3,4] # depends on the number of classes\n",
        "flag = 0 \n",
        "for index, target_path in enumerate(target_test_paths):\n",
        "    target_label_array = (np.unique(cv2.imread(target_path,cv2.IMREAD_UNCHANGED)))\n",
        "    #if(set(target_label_array).issubset(set(ideal_label_array))):\n",
        "    if len(np.intersect1d(target_label_array, ideal_label_array)) == 2:  \n",
        "       print(\"file has obstacle & living ob\", index,target_path,target_label_array )  \n",
        "       #print(np.intersect1d(target_label_array, ideal_label_array))\n",
        "       #print (target_label_array)  \n",
        "'''      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train images: 3440\n",
            "Number of train masks: 3440\n",
            "Number of validation images: 983\n",
            "Number of validation masks: 983\n",
            "Number of test images: 491\n",
            "Number of test masks: 491\n",
            "/content/drive/MyDrive/TheIRDatasetMini/train/a1570555799_168847.png | /content/drive/MyDrive/TheIRDatasetMini/trainannot/a1570555799_168847.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini/train/a1570555799_168847_2.png | /content/drive/MyDrive/TheIRDatasetMini/trainannot/a1570555799_168847_2.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini/train/a1570555846_769019.png | /content/drive/MyDrive/TheIRDatasetMini/trainannot/a1570555846_769019.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini/train/a1570555846_769019_2.png | /content/drive/MyDrive/TheIRDatasetMini/trainannot/a1570555846_769019_2.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini/train/a1570555911_468876.png | /content/drive/MyDrive/TheIRDatasetMini/trainannot/a1570555911_468876.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini/train/a1570555911_468876_2.png | /content/drive/MyDrive/TheIRDatasetMini/trainannot/a1570555911_468876_2.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini/train/a1570555914_968766.png | /content/drive/MyDrive/TheIRDatasetMini/trainannot/a1570555914_968766.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini/train/a1570555914_968766_2.png | /content/drive/MyDrive/TheIRDatasetMini/trainannot/a1570555914_968766_2.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini/train/a1570555918_435529.png | /content/drive/MyDrive/TheIRDatasetMini/trainannot/a1570555918_435529.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini/train/a1570555918_435529_2.png | /content/drive/MyDrive/TheIRDatasetMini/trainannot/a1570555918_435529_2.png\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#COMMENTING THIS CODE AS THE CHECK HAS ALREADY BEEN DONE DURING PREPROCESSING\\n#below code checks which testing files have living obstacles and obstacles. this way we can use them for prediction\\nideal_label_array = [3,4] # depends on the number of classes\\nflag = 0 \\nfor index, target_path in enumerate(target_test_paths):\\n    target_label_array = (np.unique(cv2.imread(target_path,cv2.IMREAD_UNCHANGED)))\\n    #if(set(target_label_array).issubset(set(ideal_label_array))):\\n    if len(np.intersect1d(target_label_array, ideal_label_array)) == 2:  \\n       print(\"file has obstacle & living ob\", index,target_path,target_label_array )  \\n       #print(np.intersect1d(target_label_array, ideal_label_array))\\n       #print (target_label_array)  \\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5SOaM9JvIao",
        "outputId": "ecebe496-0d88-4b16-c82d-5657314a0581"
      },
      "source": [
        "# Inference \n",
        "#prints how many files are in the test batch\n",
        "input_test_paths = getFullyQualifiedImagePaths(input_dir_test)\n",
        "target_test_paths = getFullyQualifiedImagePaths(target_dir_test)\n",
        "\n",
        "#prints how many files are in the batch\n",
        "print(\"Number of test images:\", len(input_test_paths))\n",
        "print(\"Number of test masks:\", len(target_test_paths))\n",
        "\n",
        "#pattern of the test images and masks \n",
        "for input_path, target_path in zip(input_test_paths[:5], target_test_paths[:5]):\n",
        "    print(input_path, \"|\", target_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test images: 491\n",
            "Number of test masks: 491\n",
            "/content/drive/MyDrive/TheIRDatasetMini_backup/test/a1570555799_168847_1_2.png | /content/drive/MyDrive/TheIRDatasetMini_backup/testannot/a1570555799_168847_1_2.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini_backup/test/a1570556017_735642_1.png | /content/drive/MyDrive/TheIRDatasetMini_backup/testannot/a1570556017_735642_1.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini_backup/test/a1570556017_735642_1_2.png | /content/drive/MyDrive/TheIRDatasetMini_backup/testannot/a1570556017_735642_1_2.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini_backup/test/a1570556172_102191_1_2.png | /content/drive/MyDrive/TheIRDatasetMini_backup/testannot/a1570556172_102191_1_2.png\n",
            "/content/drive/MyDrive/TheIRDatasetMini_backup/test/a1570556181_568857_1_2.png | /content/drive/MyDrive/TheIRDatasetMini_backup/testannot/a1570556181_568857_1_2.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKfn04GOFbN3"
      },
      "source": [
        "'''\n",
        "## This is a test to see how intersect runs in python \n",
        "arr1 = [1,2]\n",
        "arr2 = [2,5,1]\n",
        "if len(np.intersect1d(arr1, arr2)) == 2: \n",
        "     flag +=1\n",
        "     print(\"intersect\" ) \n",
        "     print(np.intersect1d(arr1, arr2))\n",
        "else:\n",
        "     print(\"no intersect\" ) \n",
        "     print(np.intersect1d(arr1, arr2))\n",
        " \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZxuEZuOgOVq"
      },
      "source": [
        "## What does one input image and corresponding segmentation mask look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qTygTjxLB_M"
      },
      "source": [
        "'''\n",
        "import cv2\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import PIL\n",
        "\n",
        "img = cv2.imread('/content/drive/MyDrive/IRImages_7classes_unet/1571161174.581624_1_2.png')\n",
        "print(img) \n",
        "img1 = cv2.imread('/content/drive/MyDrive/IRImages_7classes_masks_unet/1571161174.581624_1_2.png', cv2.IMREAD_UNCHANGED)\n",
        "print(img1)\n",
        "print(img.shape) # height , width, color\n",
        "print(img1.shape)\n",
        "array1 = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
        "print(array1) \n",
        "array2 = tensorflow.keras.preprocessing.image.img_to_array(img1)\n",
        "print(array2) \n",
        "print(np.unique(array1))\n",
        "print(np.unique(array2))\n",
        "print(tensorflow.keras.backend.epsilon())\n",
        "print(np.unique(cv2.imread('/content/drive/MyDrive/IRImages_7classes_unet/1571161174.581624_1_2.png')))\n",
        "print(np.unique(cv2.imread('/content/drive/MyDrive/IRImages_7classes_masks_unet/1571161174.581624_1_2.png',cv2.IMREAD_UNCHANGED)))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ucoLp1UgOVq"
      },
      "source": [
        "'''\n",
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "\n",
        "# Display input image #9\n",
        "display(Image(filename=input_img_paths[9]))\n",
        "\n",
        "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
        "img = PIL.ImageOps.autocontrast(load_img(target_img_paths[9]))\n",
        "display(img)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n26N4oURgOVq"
      },
      "source": [
        "## Prepare `Sequence` class to load & vectorize batches of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvauKhqOgOVr"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "\n",
        "class OxfordPets(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            x[j] = img\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
        "            #in case we want to see if masked image have the right values\n",
        "            #print(tensorflow.keras.preprocessing.image.img_to_array(img))\n",
        "            y[j] = np.expand_dims(img, 2)\n",
        "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2: \n",
        "            #if classes are 3, keras expects masked values to be 0,1,2 only. Cannot take 1,3,5. It checks numerically not just the number of masked values\n",
        "            #y[j] += 1 \n",
        "        return x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O844f7oDgOVr"
      },
      "source": [
        "## Prepare U-Net Xception-style model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-pfuOspgOVr",
        "outputId": "339970ef-4ae0-43c1-c2fc-4a63e3e09e84"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2 ,padding=\"same\",kernel_initializer = 'uniform')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"ReLU\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\", kernel_initializer = 'uniform')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\", kernel_initializer = 'uniform')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\", )(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\",  kernel_initializer = 'uniform')(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3,padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"ReLU\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3,padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1,padding=\"same\", kernel_initializer = 'uniform')(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\",kernel_initializer = 'uniform')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Free up RAM in case the model definition cells were run multiple times\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Build model\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 256, 320, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 128, 160, 32  896         ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128, 160, 32  128        ['conv2d[0][0]']                 \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 128, 160, 32  0           ['batch_normalization[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 128, 160, 32  0           ['activation[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " separable_conv2d (SeparableCon  (None, 128, 160, 64  2400       ['activation_1[0][0]']           \n",
            " v2D)                           )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 128, 160, 64  256        ['separable_conv2d[0][0]']       \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 128, 160, 64  0           ['batch_normalization_1[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " separable_conv2d_1 (SeparableC  (None, 128, 160, 64  4736       ['activation_2[0][0]']           \n",
            " onv2D)                         )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 128, 160, 64  256        ['separable_conv2d_1[0][0]']     \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 64, 80, 64)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 64, 80, 64)   2112        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 64, 80, 64)   0           ['max_pooling2d[0][0]',          \n",
            "                                                                  'conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 64, 80, 64)   0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " separable_conv2d_2 (SeparableC  (None, 64, 80, 128)  8896       ['activation_3[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 64, 80, 128)  512        ['separable_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 64, 80, 128)  0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " separable_conv2d_3 (SeparableC  (None, 64, 80, 128)  17664      ['activation_4[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 64, 80, 128)  512        ['separable_conv2d_3[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 32, 40, 128)  0          ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 40, 128)  8320        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 32, 40, 128)  0           ['max_pooling2d_1[0][0]',        \n",
            "                                                                  'conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 40, 128)  0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " separable_conv2d_4 (SeparableC  (None, 32, 40, 256)  34176      ['activation_5[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 40, 256)  1024       ['separable_conv2d_4[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 40, 256)  0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " separable_conv2d_5 (SeparableC  (None, 32, 40, 256)  68096      ['activation_6[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 40, 256)  1024       ['separable_conv2d_5[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 16, 20, 256)  0          ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 16, 20, 256)  33024       ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 16, 20, 256)  0           ['max_pooling2d_2[0][0]',        \n",
            "                                                                  'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 16, 20, 256)  0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTransp  (None, 16, 20, 256)  590080     ['activation_7[0][0]']           \n",
            " ose)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 16, 20, 256)  1024       ['conv2d_transpose[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 16, 20, 256)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2DTran  (None, 16, 20, 256)  590080     ['activation_8[0][0]']           \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 16, 20, 256)  1024       ['conv2d_transpose_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSampling2D)  (None, 32, 40, 256)  0          ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2D)   (None, 32, 40, 256)  0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 40, 256)  65792       ['up_sampling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 32, 40, 256)  0           ['up_sampling2d[0][0]',          \n",
            "                                                                  'conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 32, 40, 256)  0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2DTran  (None, 32, 40, 128)  295040     ['activation_9[0][0]']           \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 32, 40, 128)  512        ['conv2d_transpose_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 32, 40, 128)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2DTran  (None, 32, 40, 128)  147584     ['activation_10[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 32, 40, 128)  512        ['conv2d_transpose_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " up_sampling2d_3 (UpSampling2D)  (None, 64, 80, 256)  0          ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSampling2D)  (None, 64, 80, 128)  0          ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 64, 80, 128)  32896       ['up_sampling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 64, 80, 128)  0           ['up_sampling2d_2[0][0]',        \n",
            "                                                                  'conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 64, 80, 128)  0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2DTran  (None, 64, 80, 64)  73792       ['activation_11[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 64, 80, 64)  256         ['conv2d_transpose_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 64, 80, 64)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2DTran  (None, 64, 80, 64)  36928       ['activation_12[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 64, 80, 64)  256         ['conv2d_transpose_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " up_sampling2d_5 (UpSampling2D)  (None, 128, 160, 12  0          ['add_4[0][0]']                  \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " up_sampling2d_4 (UpSampling2D)  (None, 128, 160, 64  0          ['batch_normalization_12[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 128, 160, 64  8256        ['up_sampling2d_5[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 128, 160, 64  0           ['up_sampling2d_4[0][0]',        \n",
            "                                )                                 'conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 128, 160, 64  0           ['add_5[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_6 (Conv2DTran  (None, 128, 160, 32  18464      ['activation_13[0][0]']          \n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 128, 160, 32  128        ['conv2d_transpose_6[0][0]']     \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 128, 160, 32  0           ['batch_normalization_13[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_7 (Conv2DTran  (None, 128, 160, 32  9248       ['activation_14[0][0]']          \n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 128, 160, 32  128        ['conv2d_transpose_7[0][0]']     \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " up_sampling2d_7 (UpSampling2D)  (None, 256, 320, 64  0          ['add_5[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " up_sampling2d_6 (UpSampling2D)  (None, 256, 320, 32  0          ['batch_normalization_14[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 256, 320, 32  2080        ['up_sampling2d_7[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 256, 320, 32  0           ['up_sampling2d_6[0][0]',        \n",
            "                                )                                 'conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 256, 320, 7)  2023        ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,060,135\n",
            "Trainable params: 2,056,359\n",
            "Non-trainable params: 3,776\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D88fKSmGgOVs"
      },
      "source": [
        "## Set aside a validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eukoj81gOVs"
      },
      "source": [
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set. There are 8000 images in the input. \n",
        "# val_samples = 500\n",
        "# random.Random(1337).shuffle(input_img_paths)\n",
        "# random.Random(1337).shuffle(target_img_paths)\n",
        "# train_input_img_paths = input_img_paths[:-val_samples]\n",
        "# train_target_img_paths = target_img_paths[:-val_samples]\n",
        "# val_input_img_paths = input_img_paths[-val_samples:]\n",
        "# val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# In new scheme, we are dividing the whole dataset into training, validation and testing sets. So we do not need\n",
        "# shuffling and creating validation set out of training data set.\n",
        "\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "\n",
        "train_gen = OxfordPets(batch_size, img_size, input_img_paths, target_img_paths)\n",
        "\n",
        "val_gen = OxfordPets(batch_size, img_size, input_val_paths, target_val_paths)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nJLhugZvqTP"
      },
      "source": [
        "# Inference \n",
        "import random\n",
        "# Instantiate data Sequences for each split\n",
        "\n",
        "test_gen = OxfordPets(test_batch_size, img_size, input_test_paths, target_test_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9yf05kfgOVs"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Mgsn2PgOVt"
      },
      "source": [
        "# Configure the model for training.\n",
        "# We use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data is integers.\n",
        "\n",
        "import tensorflow as tf\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)\n",
        "\n",
        "#model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "callbacks = [\n",
        "    #commenting checkpoint because i am going to save the entire model and start with earlier model\n",
        "    #keras.callbacks.ModelCheckpoint(\"IR_1000images_he_normal_adam_softmax_relu_300_10_1.h5\", save_best_only=True),\n",
        "    #tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
        "    #tf.keras.callbacks.TensorBoard(log_dir='/content/drive/MyDrive/Logs/Unet-7classes-datasetpaper')\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='/content/drive/MyDrive/Logs/Unet-7classes-finalpapersubmission')\n",
        "]\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch. This will be the first time code. Thereafter, you first load the saved model and then train again\n",
        "#epochs = 1\n",
        "model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)\n",
        "\n",
        "# Save the model after training \n",
        "model.save(\"/content/drive/MyDrive/Models/Unet-7classes-finalpapersubmission\")\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oKMJ1nFFneP"
      },
      "source": [
        "## Train the Model - Run 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTQKN5E-Fkuj"
      },
      "source": [
        "# after the 1st run, we can just run from this step on (excluding the files etc)\n",
        "# ADD metrics / losses. I have NOT done that. \n",
        "# now load the model for the next run \n",
        "import tensorflow as tf\n",
        "reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Models/Unet-7classes-finalpapersubmission\")\n",
        "\n",
        "# now re-train on the saved model - this will be from run2\n",
        "#epochs = 100\n",
        "callbacks = [\n",
        "     tf.keras.callbacks.TensorBoard(log_dir='/content/drive/MyDrive/Logs/Unet-7classes-finalpapersubmission')\n",
        "]\n",
        "reconstructed_model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)\n",
        "\n",
        "# now save the model back ### REMEMBER : ONLY save the model if you know your run is with good data. \n",
        "# ELSE it will spoil all the prior learning. Will save the new model in a different folder so that it doesn't spoil the \n",
        "# original model. will run in batches of Epochs. \n",
        "reconstructed_model.save(\"/content/drive/MyDrive/Models/Unet-7classes-finalpapersubmission\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9gYCGN4gOVt"
      },
      "source": [
        "## Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u11wWRX4gOVt"
      },
      "source": [
        "# Inference \n",
        "# Generate predictions for all images in the TEST set\n",
        "\n",
        "from IPython.display import Image, display\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "#from PIL import Image\n",
        "\n",
        "#val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
        "#val_input_img_paths_test = ['/content/drive/MyDrive/poorlighting_brightlight_resized/1603208980.690948_resized.png']\n",
        "#val_input_img_paths_test = ['/content/drive/MyDrive/IRImages_7classes_unet/1571161440.714693_1_2.png']\n",
        "#val_target_img_paths_test = ['/content/drive/MyDrive/IRImages_7classes_masks_unet/1571161440.714693_1_2.png']\n",
        "\n",
        "\n",
        "#print(type(val_input_img_paths_test))\n",
        "\n",
        "#test_gen = OxfordPets(1, img_size, test_input_img_paths_test, test_target_img_paths_test)\n",
        "# we will use model for the 1st run. and then reconstructed_model from run 2\n",
        "#val_preds = model.predict(val_gen)\n",
        "\n",
        "#predict using the saved model - run 2 on \n",
        "reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Models/Unet-7classes-finalpapersubmission\")\n",
        "test_preds = reconstructed_model.predict(test_gen)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtQj6oy-jkw0"
      },
      "source": [
        "# Inference \n",
        "## Now visualize predictions for a specific image. Change the value of i\n",
        "\n",
        "def display_mask(i):\n",
        "    \"\"\"Quick utility to display a model's prediction. we need color masked images, so displaying both color and black and white\"\"\"\n",
        "    \n",
        "    #0 - sky(dark.blue). \n",
        "    #1 - water(light.blue).   \n",
        "    #2 - bridge(yellow).   \n",
        "    #3 - obstacle(purple).  \n",
        "    #4- living ob(green).  \n",
        "    #5- backgnd (orange). \n",
        "    #6 - self(pink)\n",
        "    # Colors are same as segments.ai scheme\n",
        "    label_colours = [(0,113,188), (216,82,24), (236,176,31), (125, 46, 141), (118, 171, 47), (161, 19, 46), (255,0,0)]  \n",
        "    \n",
        "    \n",
        "    mask = np.argmax(test_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    #print(\"mask shape = \", np.shape(mask))\n",
        "    #print(\"mask length = \", len(mask[i, 0]))\n",
        "    #print(\"mask length = \", len(mask[i]))\n",
        "    #print(\"Unique pixel values = \", np.unique(mask))\n",
        "    #print(\"Type of mask = \", type(mask))\n",
        "    \n",
        "   \n",
        "    #img1 = PIL.Image.new('RGB', (640, 512))\n",
        "    img1 = PIL.Image.new('RGB', img_size_width_ht)\n",
        "    #print (\"image size\", img_size_width_ht)\n",
        "    pixels = img1.load()\n",
        "    #print(type(pixels))\n",
        "    #print(pixels[0,0])\n",
        "    for j_, j in enumerate(mask[:, :, 0]):\n",
        "        #print (j_, j)\n",
        "        for k_, k in enumerate(j):\n",
        "             #print(k_, k)\n",
        "             if k < num_classes:\n",
        "                pixels[k_,j_] = label_colours[k]\n",
        "    output = np.array(img1)\n",
        "\n",
        "    outputpicturePath = '/content/drive/MyDrive/IRdatasetmini-inferences/unet/picture' + '/' + str(i) + '--run1--' + imagefilepathtokens[-1]     \n",
        "    \n",
        "    print(\"this is the colored inferred image\")\n",
        "    display(img1)\n",
        "\n",
        "    ##Saving the category ids in an image for programatic IoU check \n",
        "    img1.save(outputpicturePath)\n",
        "\n",
        "    '''\n",
        "    print(\"this is the gray inferred image\")\n",
        "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
        "    display(img)\n",
        "    '''\n",
        "    return mask\n",
        "   \n",
        "\n",
        "\n",
        "# Display results for validation image #40. it uses val_input_images. so will only pick from the ones that we have kept for validation.\n",
        "#i = 190\n",
        "for i in range(50,100): \n",
        "      # check if ground-truth target mask as any object - class id - 3\n",
        "      image_gt = tf.keras.preprocessing.image.load_img(target_test_paths[i])\n",
        "      input_arr = tf.keras.preprocessing.image.img_to_array(image_gt)\n",
        "      gt_mask_array = np.unique(input_arr) \n",
        "      object_living_array = [3,4]\n",
        "      #print(gt_mask_array)\n",
        "      #intersect_array_object = np.intersect1d(gt_mask_array, object_array) \n",
        "      #intersect_array_living = np.intersect1d(gt_mask_array, living_array) \n",
        "      if((set(object_living_array) & set(gt_mask_array))== set(object_living_array)):\n",
        "          #print (\"intersect output 2\", gt_mask_array )\n",
        "          #print(gt_mask_array)\n",
        "          #display the raw input image \n",
        "          print(\"this is the raw image\")\n",
        "          display(Image(filename=input_test_paths[i]))\n",
        "          \n",
        "          #display the ground truth masked image \n",
        "          img = PIL.ImageOps.autocontrast(load_img(target_test_paths[i]))\n",
        "          print(\"this is the ground truth mask\")\n",
        "          display(img)\n",
        "          \n",
        "          #set file names for storing the prediction \n",
        "          imagefilepathtokens = input_test_paths[i].split('/')\n",
        "          #print(\"tokens = \", imagefilepathtokens)\n",
        "\n",
        "          outputFilePath = '/content/drive/MyDrive/IRdatasetmini-inferences/unet/program' + '/' + str(i) + '--run1--' + imagefilepathtokens[-1] \n",
        "          #print(\"output file name = \", outputFilePath)\n",
        "\n",
        "          #invoke the prediction function \n",
        "          output_mask = display_mask(i)  # Note that the model only sees inputs at 150x150.\n",
        "          output_mask = np.squeeze(output_mask, axis=2)\n",
        "          #result = np.where(output_mask==0)\n",
        "          #print(\"indices where output_mask has 0 value = \", result)\n",
        "          output_Im = PIL.Image.fromarray(output_mask.astype(np.uint8))\n",
        "\n",
        "          ##Saving the category ids in an image for programatic IoU check \n",
        "          output_Im.save(outputFilePath)      \n",
        "      \n",
        "\n",
        "      \n",
        "     \n",
        "\n",
        "       \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA6-u9PsdYxU"
      },
      "source": [
        " '''\n",
        " # This check is to see if unet gives pixel values of 0 - means leaves pixels unassigned. \n",
        " # we did not see a single image even when run with 1 epoc where the pixel had a class id of 0 \n",
        " # valid class ids were 1,2,3,4,5,6,7\n",
        " count = 0 \n",
        " for i in range(0,492): \n",
        "    mask = np.argmax(test_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    #output_mask = np.squeeze(output_mask, axis=2)\n",
        "    result = np.where(mask==0)\n",
        "    if (len(result[0]) > 0): \n",
        "        #print(\"indices where mask has 0 value = \", result)\n",
        "        count = count+1 \n",
        " print(\"total count of images where there are 0s\" , count)\n",
        "    \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY8aN3EA3bCw"
      },
      "source": [
        "'''\n",
        "#visualize the architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=1, activation='relu'))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "plot_model(model, to_file='/content/IR_1000images_he_normal_adam_softmax_relu_100_10-h5-modelplot.png', show_shapes=True, show_layer_names=True)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWkR-8Tu4IVb"
      },
      "source": [
        "'''\n",
        "# to evaluate the weight tensors\n",
        "from keras import backend as K\n",
        "\n",
        "for w in model.trainable_weights:\n",
        "    print(K.eval(w))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkxOGK0G5EYh"
      },
      "source": [
        "'''\n",
        "# visualize the model in tensorboard - we will use the log files created in checkpoint  \n",
        "#!kill 587\n",
        "\n",
        "%load_ext tensorboard\n",
        "import datetime, os\n",
        "\n",
        "%tensorboard --logdir logs \n",
        "'''\n",
        " \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGYZiR6yrGav"
      },
      "source": [
        "##Save the .pb file in .h5 format "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHaI7U0cpaqL"
      },
      "source": [
        "'''\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Models/Unet-7classes-finalpapersubmission\")\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Lo6VuGp1E3"
      },
      "source": [
        "tf.keras.models.save_model(model,\"/content/drive/MyDrive/Models/h5-Unet-7classes-finalpapersubmission.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfDabGp9atzh"
      },
      "source": [
        "#### CODE TO PREDICT ANY IMAGE THAT WILL NOT HAVE A GROUNDTRUTH. INPUT IS A SINGLE IMAGE\n",
        "\n",
        "test_input_dir1 = \"/content/drive/MyDrive/RandomImagesForInference\"  \n",
        "test_target_input_dir1 = \"/content/drive/MyDrive/TheIRDatasetMini/testannot\"  \n",
        "test_input_img_paths1 = getFullyQualifiedImagePaths(test_input_dir1)\n",
        "test_target_img_paths1 = getFullyQualifiedImagePaths(test_target_input_dir1)\n",
        "\n",
        "test_gen1 = OxfordPets(batch_size, img_size, test_input_img_paths1, test_target_img_paths1)\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import Image, display\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "\n",
        "reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Models/Unet-7classes-finalpapersubmission\")\n",
        "test_preds1 = reconstructed_model.predict(test_gen1)\n",
        "\n",
        "## Now visualize predictions for a specific image. Change the value of i\n",
        "\n",
        "def display_mask(i):\n",
        "    \"\"\"Quick utility to display a model's prediction. we need color masked images, so displaying both color and black and white\"\"\"\n",
        "    #label_colours = [(59,193,246), (222,168,51), (161,78,69), (177, 168, 52), (238, 159, 231), (240, 244, 74), (160, 253, 134)]  \n",
        "    #label_colours = [(246,193,59), (51,168,222), (69,78,161), (52, 168, 177), (231, 159, 238), (74, 244, 240), (134, 253, 160)]  \n",
        "                    \n",
        "    #0 - sky(dark.blue). \n",
        "    #1 - water(light.blue).   \n",
        "    #2 - bridge(yellow).   \n",
        "    #3 - obstacle(purple).  \n",
        "    #4- living ob(green).  \n",
        "    #5- backgnd (orange). \n",
        "    #6 - self(pink)\n",
        "    \n",
        "    label_colours = [(0,113,188), (76,189,237), (236,176,31), (125, 46, 141), (118, 171, 47), (216, 82, 24), (161, 19, 46)]  \n",
        "    \n",
        "    \n",
        "    mask = np.argmax(test_preds1[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    print(\"mask shape = \", np.shape(mask))\n",
        "    #print(\"mask length = \", len(mask[i, 0]))\n",
        "    #print(\"mask length = \", len(mask[i]))\n",
        "    print(\"Unique pixel values = \", np.unique(mask))\n",
        "    print(\"Type of mask = \", type(mask))\n",
        "    \n",
        "\n",
        "    img_tst = PIL.Image.new('RGB', (320, 256))\n",
        "    pixels = img_tst.load()\n",
        "    print(type(pixels))\n",
        "    print(pixels[0,0])\n",
        "    for j_, j in enumerate(mask[:, :, 0]):\n",
        "        #print (j_, j)\n",
        "        for k_, k in enumerate(j):\n",
        "              #print(k_, k)\n",
        "              if k < num_classes:\n",
        "                  pixels[k_,j_] = label_colours[k]\n",
        "    output = np.array(img_tst)\n",
        "    \n",
        "    print(\"this is the colored inferred image\")\n",
        "    display(img_tst)\n",
        "  \n",
        "\n",
        "    print(\"this is the gray inferred image\")\n",
        "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
        "    display(img)\n",
        "\n",
        "    return mask\n",
        "   \n",
        "\n",
        "\n",
        "# Display results for validation image #40. it uses val_input_images. so will only pick from the ones that we have kept for validation.\n",
        "i = 0\n",
        "\n",
        "imagefilepathtokens = test_input_img_paths[i].split('/')\n",
        "print(\"tokens = \", imagefilepathtokens)\n",
        "\n",
        "#outputFilePath = '/content/drive/MyDrive/TheIRDataset-Inferences/UNET/Prediction' + '/' + str(i) + '--run1--' + imagefilepathtokens[-1] \n",
        "#print(\"output file name = \", outputFilePath)\n",
        "# Display input image\n",
        "print(\"this is the raw image, name = \", test_input_img_paths1[i])\n",
        "display(Image(filename=test_input_img_paths1[i]))\n",
        "\n",
        "# Display ground-truth target mask\n",
        "#print(\"this is the labeled image\")\n",
        "#img = PIL.ImageOps.autocontrast(load_img(test_target_img_paths[i]))\n",
        "#display(img)\n",
        "\n",
        "# Display mask predicted by our model\n",
        "output_mask = display_mask(i)  # Note that the model only sees inputs at 150x150.\n",
        "output_mask = np.squeeze(output_mask, axis=2)\n",
        "output_Im = PIL.Image.fromarray(output_mask.astype(np.uint8))\n",
        "#output_Im.save(outputFilePath)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}