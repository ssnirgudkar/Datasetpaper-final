{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplab2-Finalpaper_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7MP4aSk7C0McXXzJajg/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/Datasetpaper-final/blob/main/deeplab2_Finalpaper_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW3lSbsBxFzp"
      },
      "source": [
        "## Use this version as the most final\n",
        "\n",
        "creates model folder automatically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GKo9sXGKeri"
      },
      "source": [
        "to do - \n",
        "1. Batch size should be 10 (identical with UNet, PSPNet) - This batch size gives OOM. 7 is the max we can go with regular RAM\n",
        "2. Image size 512*512 : Is this really needed? We are not using imagenet weights anyway. - Updted to use the actual \n",
        "3. Shuffle the training and validation images. Use the code which was shared earlier. I think this will be beneficial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoAs-oCSzGI6"
      },
      "source": [
        "## we have to keep batchsize = 2 or more. It does not run with batch size 1. Image will be resized to 512*512\n",
        "## mask datatype is uint8. segments.ai semantic masks are uint8 and instance masks are uint16. We need to be careful about which type of masks are getting processed else it reads masks as 0. \n",
        "##the resize was changed to nearest. but we can check if thats needed. bilinear is default. So we can try to keep it as bilinera and that shld also work. With this run, objects were not coming up well. objects and background was getting mixed\n",
        "\n",
        "Run2 11/11 - Uncommented the 127.5-1 code. so now both image and masks are float 32. also changed the nearest back to bilnear. changed the batch size from 10 to 4. running with 50 epochs to see how it works.\n",
        "\n",
        "Train images - \n",
        "Val images - \n",
        "Test images \n",
        "Epochs - 500 \n",
        "Train time -\n",
        "\n",
        "##Run3 - \n",
        "Train images - 3438\n",
        "Val images - 982\n",
        "Test images - 492\n",
        "Epochs - 50 \n",
        "Train time - 3 hrs 49 min\n",
        "Test time - \n",
        "Any preloaded weights - NO. Imagenet weights are not used\n",
        "The masks are uint8. the code had it as uint16. Hence we were getting pixel values of 257. fixed that\n",
        "Model - deeplab-finalpapersubmission_V1\n",
        "\n",
        "##Run4 - \n",
        "Train images - 26081\n",
        "Val images - 7452\n",
        "Test images - 3726\n",
        "Epochs - 25(will break into 4 parts as this is a big dataset.  \n",
        "Train time - 10 hrs 40 min\n",
        "Test time - \n",
        "Any preloaded weights - NO. Imagenet weights are not used\n",
        "Model - deeplab-finalpapersubmission (the next ones will be submission2,3 and 4)\n",
        "infernce folder - IRDatasetFinal-Inferences\n",
        "loss: 0.0623 - accuracy: 0.9790 \n",
        "\n",
        "##Run5 - 2020 & 2021\n",
        "loss: 0.0361 - accuracy: 0.9873 - val_loss: 3.3841 - val_accuracy: 0.7209\n",
        "epochs - 50 \n",
        "ran with 2020 and 2021 dataset only \n",
        "Model - deeplab folder - deeplab-finalpapersubmission-2020only (there's another with 2020-only. need to check)\n",
        "training time - 9 hrs, 27 min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaETn9Z21S9y"
      },
      "source": [
        "# https://keras.io/examples/vision/deeplabv3_plus/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl9XF-Fty5RZ",
        "outputId": "1d917d90-9a03-44db-f3d8-3c731db068ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/train.zip\", \"r\")\n",
        "#zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetAfter2019/train.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/trainannot.zip\", \"r\")\n",
        "#zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetAfter2019/trainannot.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/val.zip\", \"r\")\n",
        "#zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetAfter2019/val.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/valannot.zip\", \"r\")\n",
        "#zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetAfter2019/valannot.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "'''\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetAfter2019/test.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetAfter2019/testannot.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "'''"
      ],
      "metadata": {
        "id": "ClQ6pwk4y6Cv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "89d4e2d2-9c33-48dd-c71f-a0ce64f18f7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nzip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetAfter2019/test.zip\", \"r\")\\nzip_ref.extractall(\"/content/IRDatasetFinal\")\\nzip_ref.close()\\n\\nzip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetAfter2019/testannot.zip\", \"r\")\\nzip_ref.extractall(\"/content/IRDatasetFinal\")\\nzip_ref.close()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "APP_FOLDER = '/content/IRDatasetFinal/train'\n",
        "totalFiles = 0\n",
        "totalDir = 0\n",
        "\n",
        "for base, dirs, files in os.walk(APP_FOLDER):\n",
        "    print('Searching in : ',base)\n",
        "    for directories in dirs:\n",
        "        totalDir += 1\n",
        "    for Files in files:\n",
        "        totalFiles += 1\n",
        "   \n",
        "\n",
        "print('Total number of files',totalFiles)\n",
        "print('Total Number of directories',totalDir)\n",
        "print('Total:',(totalDir + totalFiles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LrEwKPVy545",
        "outputId": "a71c5642-029a-462b-86c7-ec2824f6b3bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching in :  /content/IRDatasetFinal/train\n",
            "Total number of files 27678\n",
            "Total Number of directories 0\n",
            "Total: 27678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "675_gYSZ1X9M"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83c9SFcwp5dM"
      },
      "source": [
        "Downloading the data We will use the Crowd Instance-level Human Parsing Dataset for training our model. The Crowd Instance-level Human Parsing (CIHP) dataset has 38,280 diverse human images. Each image in CIHP is labeled with pixel-wise annotations for 20 categories, as well as instance-level identification. This dataset can be used for the \"human part segmentation\" task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4GnbNOS1gOo"
      },
      "source": [
        "#!gdown https://drive.google.com/uc?id=1B9A9UCJYMwTL4oBEo4RZfbMZMaZhKJaz\n",
        "#!unzip -q instance-level-human-parsing.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGYmazAh-FGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49cd14d6-8d14-4601-920a-450cb66f6c23"
      },
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "# to copy files from google drive to colab memory\n",
        "#%cp -av /content/drive/MyDrive/TheIRDatasetMini/ TheIRDatasetMini\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D60VHKzY2QWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9631af38-0430-4e8e-b0e1-41af7167f1a1"
      },
      "source": [
        "IMAGE_SIZE_WIDTH = 640\n",
        "IMAGE_SIZE_HEIGHT = 512\n",
        "BATCH_SIZE = 7\n",
        "NUM_CLASSES = 7\n",
        "#DATA_DIR = '/content/drive/MyDrive/TheIRDatasetMini_backup'\n",
        "DATA_DIR = '/content/IRDatasetFinal'\n",
        "#DATA_DIR = '/content/drive/MyDrive/TheIRDataset'\n",
        "#DATA_DIR = '/content/drive/MyDrive/IR -test'\n",
        "\n",
        "#VAL_DATA_DIR = \"./instance-level_human_parsing/instance-level_human_parsing/Training\"\n",
        "#NUM_TRAIN_IMAGES = 80\n",
        "#NUM_VAL_IMAGES = 10\n",
        "\n",
        "'''\n",
        "train_images = sorted(glob(os.path.join(DATA_DIR, \"train/*\")))[:NUM_TRAIN_IMAGES]\n",
        "train_masks = sorted(glob(os.path.join(DATA_DIR, \"trainannot/*\")))[:NUM_TRAIN_IMAGES]\n",
        "val_images = sorted(glob(os.path.join(DATA_DIR, \"train/*\")))[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "val_masks = sorted(glob(os.path.join(DATA_DIR, \"trainannot/*\")))[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "'''\n",
        "train_images = sorted(glob(os.path.join(DATA_DIR, \"train/*\"))) \n",
        "train_masks = sorted(glob(os.path.join(DATA_DIR, \"trainannot/*\"))) \n",
        "indices_train = random.sample(range(len(train_images)), len(train_images))\n",
        "train_images = list(map(train_images.__getitem__, indices_train))\n",
        "train_masks = list(map(train_masks.__getitem__, indices_train))\n",
        "print(\"type of train_images = \", type(train_images))\n",
        "\n",
        "val_images = sorted(glob(os.path.join(DATA_DIR, \"val/*\")))\n",
        "val_masks = sorted(glob(os.path.join(DATA_DIR, \"valannot/*\")))\n",
        "indices_val = random.sample(range(len(val_images)), len(val_images))\n",
        "val_images = list(map(val_images.__getitem__, indices_val))\n",
        "val_masks = list(map(val_masks.__getitem__, indices_val))\n",
        "\n",
        "#test_images = sorted(glob(os.path.join(DATA_DIR, \"test/*\")))\n",
        "#test_masks = sorted(glob(os.path.join(DATA_DIR, \"testannot/*\")))\n",
        "\n",
        "\n",
        "def read_image(image_path, mask=False):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    print(\"image=\", image)\n",
        "    if mask:\n",
        "        image = tf.image.decode_png(image, channels=0, dtype=tf.uint8)\n",
        "        #print(\"mask 1st read\", image)\n",
        "        #print(\"Max value of mask as per tef.reduce are\", tf.reduce_max(image))\n",
        "        image.set_shape([None, None, 1])\n",
        "        #image = tf.cast(image, dtype=tf.float32)\n",
        "        #print(\"mask 2nd read\", image)\n",
        "        image = tf.image.resize(images=image, method= 'nearest', size=[IMAGE_SIZE_WIDTH, IMAGE_SIZE_HEIGHT])\n",
        "        #print(\"mask 3rd read after resize\", image)\n",
        "        #print('final image' ,image)\n",
        "        #print('unique values of tensor' , tf.unique(image))\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, channels=3, dtype=tf.uint8)\n",
        "        #print(\"1st step in read image\", image)\n",
        "        image.set_shape([None, None, 3])\n",
        "        image = tf.cast(image, dtype=tf.float32)\n",
        "        #print(\"2nd step in read image\", image)\n",
        "        image = tf.image.resize(images=image, method= 'bilinear', size=[IMAGE_SIZE_WIDTH, IMAGE_SIZE_HEIGHT])\n",
        "        image = image / 127.5 - 1\n",
        "        #print(\"image looks like\", image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_data(image_list, mask_list):\n",
        "    image = read_image(image_list)\n",
        "    mask = read_image(mask_list, mask=True)\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def data_generator(image_list, mask_list):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset = data_generator(train_images, train_masks)\n",
        "val_dataset = data_generator(val_images, val_masks)\n",
        "\n",
        "read_image(train_masks[20], mask=True)\n",
        "\n",
        "#print ((np.unique(cv2.imread(train_masks[1],cv2.IMREAD_UNCHANGED))))\n",
        "\n",
        "\n",
        "#print(\"Train Dataset:\", train_dataset)\n",
        "#print(\"Val Dataset:\", val_dataset)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of train_images =  <class 'list'>\n",
            "image= Tensor(\"ReadFile:0\", shape=(), dtype=string)\n",
            "image= Tensor(\"ReadFile_1:0\", shape=(), dtype=string)\n",
            "image= Tensor(\"ReadFile:0\", shape=(), dtype=string)\n",
            "image= Tensor(\"ReadFile_1:0\", shape=(), dtype=string)\n",
            "image= tf.Tensor(b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02\\x80\\x00\\x00\\x02\\x00\\x08\\x00\\x00\\x00\\x007\\x16\\x82'\\x00\\x00\\x07\\xaeIDATx\\x9c\\xed\\xdd\\xdbz\\x9aZ\\x18\\x86Q\\xec#\\xbd\\xff\\xeb\\xadYy\\xd6\\x81\\x9b\\x80\\x82\\xb2\\x99\\xcc\\x0f\\xe3\\x18\\x07\\xab\\xb1\\x9a\\xd6\\x95\\xbc\\xf9\\xc1\\t\\xd8\\xc3\\xdffS\\xff\\x96~b\\xefy-\\xfeS\\xd8\\xbb\\xe3\\xa6\\x7f\\xbapxa\\xbb\\x00\\xff\\xfd]\\x95_\\xf7\\x937\\x9e\\xd2\\x04\\x1d6\\xfa\\xe6\\x9a}L\\xb2\\xcd\\x04\\x94\\x1f\\x13\\xfdI?\\x01>\\xdb\\x16\\x13\\xd0\\xfcc\\xb2\\xe2\\x01\\xaa\\x8f9\\x8a\\x06(>\\xe6*\\x18\\xa0\\xfc\\x98\\xafX\\x80\\xf2c\\x89B\\x01\\xca\\x8fe\\x8a\\x04(?\\x96Z\\x1b\\xa0\\xf6X\\xc5B4Q\\xeb&\\xa0\\xf9\\xc7Jk\\x02\\x94\\x1f\\xab\\xad\\xd8\\x04\\xeb\\x8f\\xf5\\x16O@\\xf9Q\\xc2\\x82\\x00\\xa5G9\\xb3\\x03\\x94\\x1f%\\xcd\\x0cP~\\x945+@\\xf9Q\\xda\\x8c\\x00\\xe5Gy\\x8e\\x84\\x105u\\x02\\x1a\\x7flbR\\x80\\xeac+\\x13\\x02\\x94\\x1f\\xdby\\x19\\xa0\\xfc\\xd8\\xd2\\x8b\\x00\\xe5\\xc7\\xb6\\x9e\\x04(>\\xb67\\x16\\xa0\\xfa\\xa8b0@\\xf5Q\\x8b\\x85h\\xa2\\x86\\x024\\x00\\xa9\\xe6q\\x13,?*\\xea\\x07(>*\\xeb\\x06(?\\xaa\\xfb\\tP~\\x04\\\\\\x02T\\x1f\\x19\\xc7\\xa6\\x91\\x1fA\\xfe\\t\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00V9\\x8c\\xdf\\xd56Ms\\xaa\\xf6D\\xf8L\\x7f\\xd2O\\x80\\xcf6\\x1e`[\\xf1Y\\xf0\\xb1L@\\xa2\\x04H\\xd4h\\x80\\xb6\\xc0\\xd4`\\x02\\x12%@\\xa2\\xc6\\x02\\xb4\\x05\\xf6\\x15\\xa8\\xc2\\x04\\x1c\\xd6\\xb6\\n\\xacB\\x80C\\xda\\xb6i\\xcc\\xc0*\\x8e\\xe9'\\xb0??\\xdd\\xb5\\x8eDn\\xce\\x04\\xbcg\\xeeU%@\\xa2\\x04x\\xa7\\x1d\\xbd\\xc1\\x16\\x04\\xd8'\\xb9\\xca\\x0e\\xe7\\xaf\\xf8eo\\xfb\\xe7\\xc6\\xf5\\x1b\\xd1\\xfd\\xf8\\xf7\\x9f\\x1dx\\x9f\\xdf\\xaf\\xff\\x1f\\xce;,\\xfd\\x91\\xcf}o\\xda\\xfe\\xdf]\\xf0\\xbc\\xd9\\x87\\xaf\\x85\\x007\\xb7x\\x19\\xa6\\xf7\\xcdz\\xf1\\x8djo\\x8fi\\x7f\\x1e\\xbft\\xae\\xb6\\x8f\\x7f=\\xef\\xab\\xcc:\\xe0u(\\xb5\\xfd\\xce\\xee\\x1e3\\xfc\\xf1\\xc0\\x1f\\xc3\\xe7X\\xbc\\t\\xde\\xd4\\xb3\\x0e\\xcf\\x9b\\xdc\\xbb\\xa7]&\\\\\\x9b\\xe0\\xfa\\xf6\\xf9*\\xf8\\xc9O\\xc5.\\x7f`X\\xec\\xd8|\\x9d\\x7f\\xd9\\x95\\xd3mc\\xde4M\\xe7\\xe3\\xd1\\xfal\\xbc\\xdf\\xd5>' \\x1f\\xe3\\xf0xapw\\x1a~\\xad\\x1c\\x8d\\xdf\\x97_kt\\xbe~\\x06\\xda\\x07\\xaco \\xc0Q\\x0bb\\xbc\\x06\\xf8\\x1e\\x05\\n\\xb0\\xbe9Q}-\\xfd\\xc4\\xa6i\\xbek\\x14\\xd8\\tH:ob\\xce\\x04\\x9c\\xa4\\x17\\xe6\\xf7\\xcf\\x87\\xc1\\x9d\\xcd\\xc91\\x9a\\x80\\xf5\\x15\\x7f\\xf9\\xdb\\x1d\\x93;ym=\\xf5\\xa0\\x8d\\x15\\x9e\\x80\\xe2\\x13p\\xc0\\x9f\\xa6\\xe9\\r\\xc3\\xdd\\x94ywDph\\x00Z\\xdf\\xd9X\\x8d\\x00\\x9f\\xdbI\\x8c\\xf3)\\xb3\\x84|\\x80c~A\\x98\\xadJ_\\xb2\\x10M\\xd4~'`\\xcf\\xdb\\x8e\\xc3\\x0e\\xc3p\\xc8\\x9b\\x04\\xd8u\\x1cy\\xa1\\xbd\\xf6\\xa8M\\xccG\\x97\\xf9\\x86\\x01NR1\\xc6\\xef\\xa6\\xe4\\x9e\\xcc\\xfdy\\xbbK\\xf4.\\xb3\\xd8\\xb9\\xdf\\x1a`\\xd7\\xb1\\xb38Y8\\xcc\\x8a\\xc7\\xba\\xa7\\xf7\\xf4V\\x07\\x84>!\\xc0Q\\xc7\\xa6\\xb9\\xad\\x9c/)\\xb3\\xbb\\xb6Y\\xf3\\xd5\\xdc\\xf3\\xac\\xba\\xcb\\x99\\x8f\\x8f\\xdc\\xdb\\xbb=|t\\x80\\xa3\\xae5~=\\r\\xf3\\xfb\\xfe7\\x12K\\n\\x03\\x89=\\xb9s\\xe5\\xb6}\\x03\\xef\\xba\\xe3\\xbe\\xad\\xaf\\xc1\\x0fw\\xf9\\xd5\\x9asq\\xd8\\x0e\\xfb3\\x01Wx\\x1cx\\xbb]U\\xbd]3\\xd6\\xb9\\xb5\\x0f\\x02,\\xe2R\\xdew\\xd3\\x9c\\xc7\\xe4mn\\xeeif\\x9e\\xf6\\xf8\\x0e\\x03\\xbb\\xfd\\x99\\xe53\\x98\\x80\\xd5\\xdc\\x86\\xe1\\xf3\\x976U\\xe5G\\xa1\\x00\\xf3vRc\\xef-\\x05\\xba\\x97!nZ\\xa9\\x00\\xf7e\\xec8\\xe3^\\x14\\x8fQ\\x80\\xef\\xe1\\xd2\\xe2W\\xf7F\\xda\\x94\\x18\\x9f\\xac{\\x9f\\xef\\x12\\xe0\\x9b\\xeb\\x9e\\x8d\\x11)\\xf3\\xbc\\x1c\\xff\\xdf\\xe5\\xd6u\\x9b\\xfd\\xea\\xf2\\x9a\\xdb\\xe1j\\x01\\xfeV\\x95b\\x9cu4\\xbc\\xfbf\\x17\\x97\\xdf\\x10\\xe0'\\xe8.M\\x96\\x0es\\xf6\\xf9\\x18\\xfd\\xf9(\\xc0O\\xd6]\\x19ZZ\\xe6\\xca\\x13\\x82,D\\x13e\\x022`\\xce4\\xbc\\x8c\\xc0\\x85\\xa3L\\x80\\xbc\\xd2]\\x9b\\x1c(s\\xdd6X\\x80\\x94\\xf0\\xe7'\\xc0y\\xabA\\x02dS\\x9ds{\\x07\\xc3\\x14 \\x11\\xd7\\x18\\x05H\\x94e\\x18\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90(\\x01\\x12%@\\xa2\\x04H\\x94\\x00\\x89\\x12 Q\\x02$J\\x80D\\t\\x90\\xcd\\xb5O\\xee;\\x1c\\xaa=\\r>\\xd3%\\xbf\\xd3\\xf0\\xbd\\x02d[\\xfd\\xf1\\xf7\\x90\\xa1M05=l\\x8d\\x05\\xc8\\xa6\\xee\\x8b\\xbb\\xbf-@\\xa2\\x04H\\x94\\x00\\x89:\\xa6\\x9f\\x00\\xbf\\xd9\\xb3\\x15\\xc03\\x13\\x90(\\x01\\x12%@\\xa2\\x04H]w\\xbb\\x85\\x02$J\\x80\\xd4uw4X\\x80D\\t\\x90(\\x01\\xb2\\x9d\\xd7\\xeb\\xd0\\x02\\xa4\\xae\\xfb\\x13\\x02\\x05H\\x94\\x00\\xd9\\xcc\\x84-\\xb0\\x00\\xa9\\xca)\\xf9\\xec\\x8b\\x00\\xa9\\xe8\\xf1\\xd28\\xe7\\x03\\xb2\\xc4m\\xf7n\\xe4j\\xcb\\xc9L@\\xa2\\\\\\x17\\xcc\\x12C/pO\\xaf\\x1f\\xf2\\xf8(\\x9b`J\\xe9\\x157\\xbain{\\x0fjM@\\x96\\x98\\xb2\\xc47\\x89\\x00Yem\\x89\\x02\\xa4\\x94E-\\xda\\x07\\xa4\\xbe\\xeb\\x0eb\\xdb\\x9c\\x04H\\xce\\xc9: a\\x02$J\\x80D\\t\\x90(\\xcb0\\x943y!\\xe6\\xe78\\x89\\tH\\x94\\x00\\xa9\\xafs\\xa0X\\x80D\\t\\x90r&\\x9e\\x9d\\xda}\\x98\\x00\\x89\\x12 \\xb5\\xf5\\xe6\\xa4\\x00\\x89\\x12 \\xe5,8!K\\x80\\x943\\xe9EH\\xffA\\x02\\xa4.oP\\xc9\\x9e\\x08\\x90\\xaa\\xbc=\\x1b\\x1bz\\xb9\\x13\\xe8\\xcd\\x89\\xd8\\x17\\x01R\\xd1\\xe3\\x84\\x14 Q\\x02$J\\x80\\x94\\xf4\\xfcU\\xc8\\xc0\\xbd\\x02$J\\x80D\\t\\x90\\x92\\x9e\\x9e\\x8e0\\xb4}vU\\x1cE=?!\\xe61A\\x01R\\xd0\\xcb\\xf3\\xb1\\x1e\\n\\x14 %L?\\x13\\xf0.A\\x01\\xb2\\xda\\xdc\\xf3P]\\x94\\xc4n\\x98\\x80\\xac\\xb4\\xe8\\x8dQoCP\\x80,\\xd56\\xcdi\\xd5[D\\x9f\\x1ao\\xd1\\xcbR\\xed\\xed?\\xab\\x98\\x80,P\\xec_i8\\t\\x90Y\\xda\\x95\\x9b\\xdd{\\x02d\\xaa\\xa2\\xe1]\\xd9\\x07d\\x9aM\\xf2\\x13 \\xaf\\x95\\xde\\xea\\xf6X\\x88&\\xca> Om7\\xfb\\xcel\\x82\\x19\\xb5u|Mc\\x13\\xcc\\xb0\\xb6i\\xda\\x1a\\xfd\\xd9\\x04\\xf3\\xa8Jy\\x17\\xff\\x03v\\xbd\\xc1\\xc2\\xbe\\xf0\\xe4n\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82\", shape=(), dtype=string)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(640, 512, 1), dtype=uint8, numpy=\n",
              "array([[[6],\n",
              "        [6],\n",
              "        [6],\n",
              "        ...,\n",
              "        [0],\n",
              "        [0],\n",
              "        [0]],\n",
              "\n",
              "       [[6],\n",
              "        [6],\n",
              "        [6],\n",
              "        ...,\n",
              "        [0],\n",
              "        [0],\n",
              "        [0]],\n",
              "\n",
              "       [[6],\n",
              "        [6],\n",
              "        [6],\n",
              "        ...,\n",
              "        [0],\n",
              "        [0],\n",
              "        [0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[1],\n",
              "        [1],\n",
              "        [1],\n",
              "        ...,\n",
              "        [6],\n",
              "        [6],\n",
              "        [6]],\n",
              "\n",
              "       [[1],\n",
              "        [1],\n",
              "        [1],\n",
              "        ...,\n",
              "        [6],\n",
              "        [6],\n",
              "        [6]],\n",
              "\n",
              "       [[1],\n",
              "        [1],\n",
              "        [1],\n",
              "        ...,\n",
              "        [6],\n",
              "        [6],\n",
              "        [6]]], dtype=uint8)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1bYkHXoCv4i"
      },
      "source": [
        "#print(train_masks)\n",
        "print(train_images)\n",
        "print(train_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-2OJmow2VhG"
      },
      "source": [
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp4GSu2r2cIi"
      },
      "source": [
        "def DeeplabV3Plus(image_size_width, image_size_height, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size_width, image_size_height, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        #weights=\"imagenet\", include_top=False, input_tensor=model_input - removed imagenet weights\n",
        "        weights=None, include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    print(\"x shape 1 value\", x.shape[1] )\n",
        "    print(\"x shape 2 value\", x.shape[2] )\n",
        "\n",
        "    print(\"size is\", image_size_width // 4 // x.shape[1], image_size_height // 4 // x.shape[2])\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size_width // 4 // x.shape[1], image_size_height // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    \n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size_width // x.shape[1], image_size_height // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "\n",
        "model = DeeplabV3Plus(image_size_width=IMAGE_SIZE_WIDTH, \n",
        "                      image_size_height=IMAGE_SIZE_HEIGHT,\n",
        "                      num_classes=NUM_CLASSES)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jv41GSo2fm6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f56fcb63-8759-4c36-9027-48abf655abbf"
      },
      "source": [
        "#### This is the training cell\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "my_callbacks = [\n",
        "    #tf.keras.callbacks.TensorBoard(log_dir='/content/drive/MyDrive/Logs/Unet-7classes-finalpapersubmission'),\n",
        "    #keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/Models/deeplab/deeplab-finalpapersubmission-20201only\", save_freq = 'epoch', save_best_only=True)\n",
        "    #keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/Models/deeplab/deeplab-revisedcode-2020only\", save_freq = 'epoch', save_best_only=True)\n",
        "    keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/Models/deeplab/deeplab-7classes-finalpapersubmission_2019&2020_shuffled\", save_freq = 'epoch', save_best_only=True)\n",
        "]\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, callbacks = my_callbacks, epochs=25)\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.title(\"Training Loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "#plt.show()\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "#plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"Validation Loss\")\n",
        "plt.ylabel(\"val_loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "#plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.ylabel(\"val_accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "#plt.show()\n",
        "\n",
        "#model.save(\"/content/drive/MyDrive/Models/deeplab/deeplab-finalpapersubmission-2020only\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "3954/3954 [==============================] - ETA: 0s - loss: 0.1603 - accuracy: 0.9466INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models/deeplab/deeplab-7classes-finalpapersubmission_2019&2020_shuffled/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3954/3954 [==============================] - 2057s 515ms/step - loss: 0.1603 - accuracy: 0.9466 - val_loss: 0.1384 - val_accuracy: 0.9535\n",
            "Epoch 2/25\n",
            "3954/3954 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9687INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models/deeplab/deeplab-7classes-finalpapersubmission_2019&2020_shuffled/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3954/3954 [==============================] - 2034s 514ms/step - loss: 0.0887 - accuracy: 0.9687 - val_loss: 0.0958 - val_accuracy: 0.9674\n",
            "Epoch 3/25\n",
            "3954/3954 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9766INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models/deeplab/deeplab-7classes-finalpapersubmission_2019&2020_shuffled/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3954/3954 [==============================] - 2046s 518ms/step - loss: 0.0645 - accuracy: 0.9766 - val_loss: 0.0939 - val_accuracy: 0.9702\n",
            "Epoch 4/25\n",
            "3954/3954 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9807INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models/deeplab/deeplab-7classes-finalpapersubmission_2019&2020_shuffled/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3954/3954 [==============================] - 2047s 518ms/step - loss: 0.0523 - accuracy: 0.9807 - val_loss: 0.0856 - val_accuracy: 0.9744\n",
            "Epoch 5/25\n",
            "3954/3954 [==============================] - 2024s 512ms/step - loss: 0.0440 - accuracy: 0.9833 - val_loss: 0.1546 - val_accuracy: 0.9622\n",
            "Epoch 6/25\n",
            "3954/3954 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9849INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models/deeplab/deeplab-7classes-finalpapersubmission_2019&2020_shuffled/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  return generic_utils.serialize_keras_object(obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3954/3954 [==============================] - 2036s 515ms/step - loss: 0.0394 - accuracy: 0.9849 - val_loss: 0.0850 - val_accuracy: 0.9777\n",
            "Epoch 7/25\n",
            "3954/3954 [==============================] - 2024s 512ms/step - loss: 0.0362 - accuracy: 0.9859 - val_loss: 0.0897 - val_accuracy: 0.9769\n",
            "Epoch 8/25\n",
            "3954/3954 [==============================] - 2013s 509ms/step - loss: 0.0332 - accuracy: 0.9870 - val_loss: 0.0872 - val_accuracy: 0.9789\n",
            "Epoch 9/25\n",
            "3954/3954 [==============================] - 2026s 512ms/step - loss: 0.0311 - accuracy: 0.9877 - val_loss: 0.0925 - val_accuracy: 0.9788\n",
            "Epoch 10/25\n",
            "3954/3954 [==============================] - 2026s 512ms/step - loss: 0.0293 - accuracy: 0.9883 - val_loss: 0.0935 - val_accuracy: 0.9787\n",
            "Epoch 11/25\n",
            "3954/3954 [==============================] - 2025s 512ms/step - loss: 0.0278 - accuracy: 0.9888 - val_loss: 0.0922 - val_accuracy: 0.9792\n",
            "Epoch 12/25\n",
            "3954/3954 [==============================] - 2026s 512ms/step - loss: 0.0255 - accuracy: 0.9896 - val_loss: 0.0947 - val_accuracy: 0.9767\n",
            "Epoch 13/25\n",
            "3954/3954 [==============================] - 2024s 512ms/step - loss: 0.0253 - accuracy: 0.9897 - val_loss: 0.0993 - val_accuracy: 0.9795\n",
            "Epoch 14/25\n",
            "3954/3954 [==============================] - 2025s 512ms/step - loss: 0.0229 - accuracy: 0.9906 - val_loss: 0.1117 - val_accuracy: 0.9756\n",
            "Epoch 15/25\n",
            "3954/3954 [==============================] - 2023s 512ms/step - loss: 0.0214 - accuracy: 0.9911 - val_loss: 0.1069 - val_accuracy: 0.9795\n",
            "Epoch 16/25\n",
            "3954/3954 [==============================] - 2015s 510ms/step - loss: 0.0203 - accuracy: 0.9915 - val_loss: 0.1128 - val_accuracy: 0.9792\n",
            "Epoch 17/25\n",
            "3954/3954 [==============================] - 2016s 510ms/step - loss: 0.0226 - accuracy: 0.9908 - val_loss: 0.1077 - val_accuracy: 0.9801\n",
            "Epoch 18/25\n",
            "3954/3954 [==============================] - 2024s 512ms/step - loss: 0.0202 - accuracy: 0.9917 - val_loss: 0.1139 - val_accuracy: 0.9797\n",
            "Epoch 19/25\n",
            "3954/3954 [==============================] - 2024s 512ms/step - loss: 0.0184 - accuracy: 0.9923 - val_loss: 0.1219 - val_accuracy: 0.9794\n",
            "Epoch 20/25\n",
            "3954/3954 [==============================] - 2026s 512ms/step - loss: 0.0184 - accuracy: 0.9923 - val_loss: 0.1238 - val_accuracy: 0.9792\n",
            "Epoch 21/25\n",
            "3954/3954 [==============================] - 2016s 510ms/step - loss: 0.0175 - accuracy: 0.9926 - val_loss: 0.1228 - val_accuracy: 0.9798\n",
            "Epoch 22/25\n",
            "3954/3954 [==============================] - 2024s 512ms/step - loss: 0.0171 - accuracy: 0.9928 - val_loss: 0.1273 - val_accuracy: 0.9796\n",
            "Epoch 23/25\n",
            "3954/3954 [==============================] - 2014s 509ms/step - loss: 0.0177 - accuracy: 0.9926 - val_loss: 0.1086 - val_accuracy: 0.9790\n",
            "Epoch 24/25\n",
            "3954/3954 [==============================] - 2025s 512ms/step - loss: 0.0176 - accuracy: 0.9928 - val_loss: 0.1224 - val_accuracy: 0.9801\n",
            "Epoch 25/25\n",
            "3954/3954 [==============================] - 2025s 512ms/step - loss: 0.0157 - accuracy: 0.9934 - val_loss: 0.1314 - val_accuracy: 0.9798\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c22OwPuNCad7"
      },
      "source": [
        "## this is to retrain the saved model - RUN#2\n",
        "reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Models/deeplab/deeplab-7classes-finalpapersubmission_2019&2020_shuffled\")\n",
        "print(\"model loaded\")\n",
        "\n",
        "my_callbacks = [\n",
        "    #tf.keras.callbacks.TensorBoard(log_dir='/content/drive/MyDrive/Logs/Unet-7classes-finalpapersubmission'),\n",
        "    keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/Models/deeplab/deeplab-7classes-finalpapersubmission_2019&2020_shuffled\", save_freq = 'epoch', save_best_only=True)\n",
        "\n",
        "               ]\n",
        "\n",
        "history = reconstructed_model.fit(train_dataset, validation_data=val_dataset, callbacks = my_callbacks, epochs=50)\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.title(\"Training Loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"Validation Loss\")\n",
        "plt.ylabel(\"val_loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.ylabel(\"val_accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()\n",
        "\n",
        "#commented below as checkpoint already has save best. else it will create a duplicate\n",
        "#print(\"model to be saved\")\n",
        "#reconstructed_model.save(\"/content/drive/MyDrive/Models/deeplab/deeplab-finalpapersubmission-2020only\")\n",
        "#print(\"model saved\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}