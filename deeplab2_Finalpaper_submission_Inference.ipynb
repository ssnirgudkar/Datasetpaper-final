{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplab2-Finalpaper_submission-Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNZHxllDnazkkdNRGDxonYW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/Datasetpaper-final/blob/main/deeplab2_Finalpaper_submission_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW3lSbsBxFzp"
      },
      "source": [
        "## Use this version as the most final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoAs-oCSzGI6"
      },
      "source": [
        "## we have to keep batchsize = 2 or more. It does not run with batch size 1. Image will be resized to 512*512\n",
        "##changed mask datatype to uint16 from uint8. segments.ai masks are uint16. else it reads masks as 0\n",
        "##the resize was changed to nearest. but we can check if thats needed. bilinear is default. So we can try to keep it as bilinera and that shld also work. With this run, objects were not coming up well. objects and background was getting mixed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaETn9Z21S9y"
      },
      "source": [
        "# https://keras.io/examples/vision/deeplabv3_plus/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl9XF-Fty5RZ",
        "outputId": "56cd8f5a-49a0-41a7-dca8-f323f66eac11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "print(K.image_data_format()) # print current format\n",
        "#K.set_image_data_format('channels_last') # set format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBKEReZCZHYt",
        "outputId": "276c79fa-9e8f-4aa1-fb3e-b67477756f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "channels_last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove files \n",
        "#!ls /content/drive/MyDrive/InferenceLarge20192020shuffled/deeplab/program/*runrevised*.png \n",
        "#!ls /content/drive/MyDrive/InferenceLarge20192020shuffled/deeplab/picture/*runrevised*.png \n",
        "\n",
        "#!rm /content/drive/MyDrive/InferenceLarge20192020shuffled/deeplab/program/*runreviseddata*.png \n",
        "#!rm /content/drive/MyDrive/InferenceLarge20192020shuffled/deeplab/picture/*--runreviseddata-*.png \n",
        "\n",
        "#!rm /content/drive/MyDrive/InferenceLarge20192020shuffled/deeplab/program/*--test-*.png \n",
        "#!rm /content/drive/MyDrive/InferenceLarge20192020shuffled/deeplab/picture/*--test-*.png "
      ],
      "metadata": {
        "id": "pN5zfOV9S2sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/test.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/testannot.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "'''\n",
        "# Zip file with handpicked test images\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/select_test_images.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Zip file with masks of handpicked test images\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/select_test_masks.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "# copy the select_test_images and masks in test and testannot respectively \n",
        "!cp /content/drive/MyDrive/test_10_images/*.* /content/IRDatasetFinal/test\n",
        "!cp /content/IRDatasetFinal/select_test_masks/*.png /content/IRDatasetFinal/testannot\n",
        "'''"
      ],
      "metadata": {
        "id": "ClQ6pwk4y6Cv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "f3695fec-fd42-4168-ba34-082d44dadadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Zip file with handpicked test images\\nzip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/select_test_images.zip\", \"r\")\\nzip_ref.extractall(\"/content/IRDatasetFinal\")\\nzip_ref.close()\\n\\n# Zip file with masks of handpicked test images\\nzip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/select_test_masks.zip\", \"r\")\\nzip_ref.extractall(\"/content/IRDatasetFinal\")\\nzip_ref.close()\\n\\n# copy the select_test_images and masks in test and testannot respectively \\n!cp /content/drive/MyDrive/test_10_images/*.* /content/IRDatasetFinal/test\\n!cp /content/IRDatasetFinal/select_test_masks/*.png /content/IRDatasetFinal/testannot\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to use only for manual test images\n",
        "!cp /content/drive/MyDrive/test_10_images/*.* /content/IRDatasetFinal/test"
      ],
      "metadata": {
        "id": "H_H6t9x428yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "APP_FOLDER = '/content/IRDatasetFinal/test'\n",
        "totalFiles = 0\n",
        "totalDir = 0\n",
        "\n",
        "for base, dirs, files in os.walk(APP_FOLDER):\n",
        "    print('Searching in : ',base)\n",
        "    for directories in dirs:\n",
        "        totalDir += 1\n",
        "    for Files in files:\n",
        "        totalFiles += 1\n",
        "   \n",
        "\n",
        "print('Total number of files',totalFiles)\n",
        "print('Total Number of directories',totalDir)\n",
        "print('Total:',(totalDir + totalFiles))"
      ],
      "metadata": {
        "id": "vJvUv-g7l1Qy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efad6833-4787-4115-c3b6-07eaa198bbce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching in :  /content/IRDatasetFinal/test\n",
            "Total number of files 4508\n",
            "Total Number of directories 0\n",
            "Total: 4508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Now remove images that are not pure (rotated, mirrored etc from the test and testannot folders) \n",
        "# as we do not want to use them for testing \n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "test_dir = \"/content/IRDatasetFinal/test\"\n",
        "test_mask_dir = \"/content/IRDatasetFinal/testannot\"\n",
        "# Move rotated & mirrored files into tmp directory\n",
        "# The format of these files is aXXX_XXX_XX.png and aXXX_XXX_XX_XX.png\n",
        "# Number of underscores is 2 or 3\n",
        "dest_test_dir = \"/content/IRDatasetFinal/test_tmp\"\n",
        "dest_mask_dir = \"/content/IRDatasetFinal/testannot_tmp\"\n",
        "\n",
        "# Move non-augmented files from test directory into a temporary directory\n",
        "if (False == os.path.exists(dest_test_dir)):\n",
        "  os.mkdir(dest_test_dir)\n",
        "if (False == os.path.exists(dest_mask_dir)):\n",
        "  os.mkdir(dest_mask_dir)\n",
        "\n",
        "\n",
        "for eachFile in os.listdir(test_dir):\n",
        "  numUnderscores = eachFile.count('_')\n",
        "  if (numUnderscores > 1):\n",
        "    # File names in image & mask directories are identical so move both files in one shot!\n",
        "    shutil.move(os.path.join(test_dir, eachFile), os.path.join(dest_test_dir))\n",
        "    shutil.move(os.path.join(test_mask_dir, eachFile), os.path.join(dest_mask_dir))\n",
        "    \n",
        "'''\n",
        "    for eachFile in os.listdir(test_mask_dir):\n",
        "      numUnderscores = eachFile.count('_')\n",
        "      if (numUnderscores > 1):\n",
        "        # File names in image & mask directories are identical so move both files in one shot!\n",
        "        shutil.move(os.path.join(test_mask_dir, eachFile), os.path.join(dest_mask_dir))\n",
        "'''\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "C4o-MCy5l2FL",
        "outputId": "9ad15c37-06d4-4eda-d778-3f4d863825b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n    for eachFile in os.listdir(test_mask_dir):\\n      numUnderscores = eachFile.count('_')\\n      if (numUnderscores > 1):\\n        # File names in image & mask directories are identical so move both files in one shot!\\n        shutil.move(os.path.join(test_mask_dir, eachFile), os.path.join(dest_mask_dir))\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# check if test images are present in train / validation folder \n",
        "\n",
        "# Zip file with masks of handpicked test images\n",
        "import glob \n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/train.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/val.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "import os\n",
        "from os.path import exists\n",
        "\n",
        "file_present = 0\n",
        "for eachFile in os.listdir('/content/IRDatasetFinal/test'):\n",
        "    filename = eachFile.split(\".\")[0]  \n",
        "    #print(\"name of file\", filename)\n",
        "    \n",
        "    count_train = 0 \n",
        "    count_val = 0 \n",
        "     \n",
        "    for file in os.listdir(\"/content/IRDatasetFinal/train/\"):\n",
        "      if file.startswith(filename):\n",
        "         #print(\"File exists in train directory\", filename, os.path.join(\"/content/IRDatasetFinal/train/\", file))\n",
        "         count_train = 1 \n",
        "      if count_train == 1:\n",
        "        break \n",
        "\n",
        "    for file in os.listdir(\"/content/IRDatasetFinal/val/\"):\n",
        "      if file.startswith(filename):\n",
        "        #print(\"File exists in val directory\", filename, os.path.join(\"/content/IRDatasetFinal/val/\", file))\n",
        "        count_val = 1 \n",
        "      if count_val ==1:\n",
        "        break\n",
        "    if count_train ==1 or count_val == 1:\n",
        "      file_present = file_present + 1 \n",
        "    else:\n",
        "      print(\"files that are not present in train or val\", filename)\n",
        "\n",
        "print(\"total files from 325 that are either in val or train dir are\", file_present)\n",
        "\n",
        "'''\n",
        "    if filename in glob.glob('/content/IRDatasetFinal/train/*[0-9].*') == True:\n",
        "       print(\"test file exists in train directory\", filename)\n",
        "    \n",
        "    if filename in glob.glob('/content/IRDatasetFinal/val/*[0-9].*'):\n",
        "       print(\"test file exists in val directory\", filename)\n",
        "    \n",
        "\n",
        "    #print(\"file train path\", file_train_path)\n",
        "    #print(\"file val path\", file_val_path)\n",
        "\n",
        "    if (os.path.isfile(file_train_path) is True): \n",
        "      print(\"test file exists in train directory\", eachFile)\n",
        "    else:\n",
        "      print(\"No file found in train\", eachFile)  \n",
        "\n",
        "    if (os.path.isfile(file_val_path) is True):\n",
        "      print(\"test file exists in val directory\", eachFile)\n",
        "    else:\n",
        "      print(\"No file found in val\", eachFile)  \n",
        "\n",
        "    #print(os.path.isfile(\"/content/IRDatasetFinal/test/a1570555951_468803.png\"))\n",
        "\n",
        "\n",
        "'''   \n",
        "'''  "
      ],
      "metadata": {
        "id": "Bnh46Rbl755I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls /content/IRDatasetFinal/test/a1603391930_493382.png"
      ],
      "metadata": {
        "id": "0a0qkLrZlPRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "these are the files that were not in train or val.total files from 325 that are either in val or train dir are 286\n",
        "a1603391930_493382\n",
        "a1602782592_279944\n",
        "a1635456565_019968\n",
        "a1635456089_002472\n",
        "a1635457245_031288\n",
        "a1600888241_188905\n",
        "a1635461259_005672\n",
        "a1622943208_834271\n",
        "a1635458544_023433\n",
        "a1622942749_109772\n",
        "a1622943039_769827\n",
        "a1635457210_999884\n",
        "a1635457743_9989\n",
        "a1635456786_011603\n",
        "a1604682965_453746\n",
        "a1602782574_324635\n",
        "a1622943519_360246\n",
        "a1603387335_016333\n",
        "a1635461209_007017\n",
        "a1635461219_006688\n",
        "a1635455953_006892\n",
        "a1635452510_014722\n",
        "a1635458124_004488\n",
        "a1635458814_014660\n",
        "a1635459559_024613\n",
        "a1602782555_457181\n",
        "a1635461448_999508\n",
        "a1603392040_224361\n",
        "a1603388439_014541\n",
        "a1635456820_011965\n",
        "a1571166212_314216\n",
        "a1635459169_003060\n",
        "a1635457894_011637\n",
        "a1602782694_288267\n",
        "a1602783133_252807\n",
        "a1635453716_009826\n",
        "a1635453590_013916\n",
        "a1603210859_015065\n",
        "a1622943419_195525\n",
        "'''"
      ],
      "metadata": {
        "id": "xG-rtU9JUDv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Count the number of images in test and testannot again\n",
        "import os \n",
        "APP_FOLDER = '/content/IRDatasetFinal/test'\n",
        "totalFiles = 0\n",
        "totalDir = 0\n",
        "\n",
        "for base, dirs, files in os.walk(APP_FOLDER):\n",
        "    print('Searching in : ',base)\n",
        "    for directories in dirs:\n",
        "        totalDir += 1\n",
        "    for Files in files:\n",
        "        totalFiles += 1\n",
        "   \n",
        "\n",
        "print('Total number of files',totalFiles)\n",
        "print('Total Number of directories',totalDir)\n",
        "print('Total:',(totalDir + totalFiles))\n",
        "'''"
      ],
      "metadata": {
        "id": "81S3JowFl8Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "675_gYSZ1X9M"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D60VHKzY2QWn"
      },
      "source": [
        "IMAGE_SIZE_WIDTH = 640\n",
        "IMAGE_SIZE_HEIGHT = 512\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "NUM_CLASSES = 7\n",
        "#DATA_DIR = '/content/drive/MyDrive/TheIRDatasetMini_backup'\n",
        "DATA_DIR = '/content/IRDatasetFinal'\n",
        "#DATA_DIR = '/content/drive/MyDrive/TheIRDataset'\n",
        "#DATA_DIR = '/content/drive/MyDrive/IR -test'\n",
        "\n",
        "#VAL_DATA_DIR = \"./instance-level_human_parsing/instance-level_human_parsing/Training\"\n",
        "#NUM_TRAIN_IMAGES = 80\n",
        "#NUM_VAL_IMAGES = 10\n",
        "\n",
        "\n",
        "test_images = sorted(glob(os.path.join(DATA_DIR, \"test/*\")))\n",
        "test_masks = sorted(glob(os.path.join(DATA_DIR, \"testannot/*\")))\n",
        "\n",
        "\n",
        "def read_image(image_path, mask=False):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    #print(\"image=\", image)\n",
        "    if mask:\n",
        "        image = tf.image.decode_png(image, channels=0, dtype=tf.uint8)\n",
        "        #print(\"mask 1st read\", image)\n",
        "        #print(\"Max value of mask as per tef.reduce are\", tf.reduce_max(image))\n",
        "        image.set_shape([None, None, 1])\n",
        "        #print(\"mask 2nd read\", image)\n",
        "        image = tf.image.resize(images=image, method= 'nearest', size=[IMAGE_SIZE_HEIGHT, IMAGE_SIZE_WIDTH])\n",
        "        #print(\"mask 3rd read after resize\", image)\n",
        "        #print('final image' ,image)\n",
        "        #print('unique values of tensor' , tf.unique(image))\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, channels=3, dtype=tf.uint8)\n",
        "        #print(\"1st step in read image\", image)\n",
        "        image.set_shape([None, None, 3])\n",
        "        #print(\"2nd step in read image\", image)\n",
        "        image = tf.image.resize(images=image, method= 'bilinear', size=[IMAGE_SIZE_HEIGHT, IMAGE_SIZE_WIDTH])\n",
        "        image = image / 127.5 - 1\n",
        "        #print(\"image looks like\", image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_data(image_list, mask_list):\n",
        "    image = read_image(image_list)\n",
        "    mask = read_image(mask_list, mask=True)\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def data_generator(image_list, mask_list):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-2OJmow2VhG"
      },
      "source": [
        "'''\n",
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp4GSu2r2cIi"
      },
      "source": [
        "'''\n",
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        #weights=\"imagenet\", include_top=False, input_tensor=model_input - removed imagenet weights\n",
        "        weights=None, include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "\n",
        "model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
        "model.summary()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jv41GSo2fm6"
      },
      "source": [
        "'''\n",
        "#### This is the training cell\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "my_callbacks = [\n",
        "    #tf.keras.callbacks.TensorBoard(log_dir='/content/drive/MyDrive/Logs/Unet-7classes-finalpapersubmission'),\n",
        "    keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/Models/deeplab-finalpapersubmission\", save_freq = 'epoch')\n",
        "\n",
        "]\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, callbacks = my_callbacks, epochs=25)\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.title(\"Training Loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "#plt.show()\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "#plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"Validation Loss\")\n",
        "plt.ylabel(\"val_loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "#plt.show()\n",
        "\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.ylabel(\"val_accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "#plt.show()\n",
        "\n",
        "model.save(\"/content/drive/MyDrive/Models/deeplab-finalpapersubmission\")\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib1YVia5QVAu"
      },
      "source": [
        "## Inference \n",
        "## this is to use the saved model for predictions\n",
        "#reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Models/deeplab-finalpapersubmission\")\n",
        "#reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Models/deeplab/deeplab-finalpapersubmission-2020only\")\n",
        "#reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Models/deeplab/deeplab-revisedcode-2020only\")\n",
        "reconstructed_model = keras.models.load_model(\"/content/drive/MyDrive/Models/deeplab/deeplab-7classes-finalpapersubmission_2019&2020_shuffled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnz-wuZq2jV-"
      },
      "source": [
        "import PIL\n",
        "#matplotlib.pyplot.ioff()\n",
        "\n",
        "# Loading the Colormap. Defaulting since i have changed the colors hardcoded in the code\n",
        "'''\n",
        "colormap = loadmat(\n",
        "   \"/content/drive/MyDrive/deeplab/human_colormap.mat\")[\"colormap\"]\n",
        "colormap = colormap * 100\n",
        "colormap = colormap.astype(np.uint8)\n",
        "print(\"shape of colormap\", np.shape(colormap)) - (20,3)\n",
        "print(\"type of colormap\", type(colormap)) - ndarray \n",
        "'''\n",
        "\n",
        "colormap = np.ndarray(shape=(7,3), dtype=int);\n",
        "#colormap = np.zeros(7,3) \n",
        "#0 - sky - dark blue\n",
        "colormap[0,0]=0\n",
        "colormap[0,1]=113\n",
        "colormap[0,2]=188\n",
        "\n",
        "#1 - water - orange\n",
        "colormap[1,0]= 216\n",
        "colormap[1,1]= 82\n",
        "colormap[1,2]= 24\n",
        "\n",
        "#2 - bridge - yellow\n",
        "colormap[2,0]= 236\n",
        "colormap[2,1]= 176\n",
        "colormap[2,2]= 31\n",
        "\n",
        "#3 - obstacle - purple\n",
        "colormap[3,0]= 125\n",
        "colormap[3,1]= 46\n",
        "colormap[3,2]= 141\n",
        "\n",
        "#4 - living obstacle - green\n",
        "colormap[4,0]= 118\n",
        "colormap[4,1]= 171\n",
        "colormap[4,2]= 47\n",
        "\n",
        "#5 - backgnd - brown\n",
        "colormap[5,0]= 161\n",
        "colormap[5,1]= 19\n",
        "colormap[5,2]= 46\n",
        "\n",
        "#6 - self - red\n",
        "colormap[6,0]= 255\n",
        "colormap[6,1]= 0\n",
        "colormap[6,2]= 0\n",
        "\n",
        "def infer(model, image_tensor):\n",
        "    #print(\"image tensor looks like\", image_tensor)\n",
        "    predictions = reconstructed_model.predict(np.expand_dims((image_tensor), axis=0))\n",
        "    #print(\"predictions 1st step\", predictions)\n",
        "    predictions = np.squeeze(predictions)\n",
        "    #print(\"predictions 2nd step after squeeze\", predictions)\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    #print(\"predictions 3rd step after argmax\", predictions)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def decode_segmentation_masks(mask, colormap, n_classes):\n",
        "    r = np.zeros_like(mask).astype(np.uint8)\n",
        "    g = np.zeros_like(mask).astype(np.uint8)\n",
        "    b = np.zeros_like(mask).astype(np.uint8)\n",
        "    \n",
        "    for l in range(0, n_classes):\n",
        "        idx = mask == l\n",
        "        r[idx] = colormap[l, 0]\n",
        "        g[idx] = colormap[l, 1]\n",
        "        b[idx] = colormap[l, 2]\n",
        "    rgb = np.stack([r, g, b], axis=2)\n",
        "    #print(rgb)\n",
        "    return rgb\n",
        "\n",
        "\n",
        "def get_overlay(image, colored_mask):\n",
        "    image = tf.keras.preprocessing.image.array_to_img(image)\n",
        "    image = np.array(image).astype(np.uint8)\n",
        "    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n",
        "    return overlay\n",
        "\n",
        "\n",
        "#def plot_samples_matplotlib(display_list, figsize=(5, 3)):\n",
        "def plot_samples_matplotlib(display_list, filename, index, start, figsize=(5, 3)):\n",
        "    #_, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n",
        "    \n",
        "    \n",
        "    for i in range(len(display_list)):\n",
        "        if display_list[i].shape[-1] == 3:\n",
        "            #print(\"shape of display list\", np.shape(tf.keras.preprocessing.image.array_to_img(display_list[i])))\n",
        "            axes[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "        else:\n",
        "            #print(\"shape of display list 2\", np.shape(display_list[i]))\n",
        "            axes[i].imshow(display_list[i])\n",
        "    \n",
        "    index_mod = index+start \n",
        "    #Saving the inferred segmented image as a picture \n",
        "    plt.savefig('/content/drive/MyDrive/InferenceLarge20192020shuffled-batch2/deeplab/picture/' + str(index_mod) +'--runreviseddata-3--' +  filename)\n",
        "    \n",
        "    #plt.cla()\n",
        "    #plt.close(fig)\n",
        "    \n",
        "    #Displays the image on screen         \n",
        "    #print (index)\n",
        "    #plt.show()\n",
        "\n",
        "#the below function is used only if you want to display a single image and not a subplot of image, mask, gtmask \n",
        "def imageormask(display_list, filename, index, start, figsize=(5, 3)):\n",
        "    #_, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n",
        "    #fig, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n",
        "    \n",
        "    \n",
        "    for i in range(len(display_list)):\n",
        "        if display_list[i].shape[-1] == 3:\n",
        "            #print(\"shape of display list\", np.shape(tf.keras.preprocessing.image.array_to_img(display_list[i])))\n",
        "            plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "        else:\n",
        "            #print(\"shape of display list 2\", np.shape(display_list[i]))\n",
        "            plt.imshow(display_list[i])\n",
        "    \n",
        "    index_mod = index+start \n",
        "    #Saving the inferred segmented image as a picture \n",
        "    plt.savefig('/content/drive/MyDrive/InferenceReports/'+ 'mask'+ filename)\n",
        "    \n",
        "    #plt.cla()\n",
        "    #plt.close(fig)\n",
        "    \n",
        "    #Displays the image on screen         \n",
        "    #print (index)\n",
        "    #plt.show()\n",
        "\n",
        "\n",
        "def plot_predictions(images_list, masks_list, start, colormap, model):\n",
        "    for (index, mask_file) in enumerate(masks_list):\n",
        "      gt_mask_array = ((np.unique(cv2.imread(mask_file,cv2.IMREAD_UNCHANGED))))\n",
        "      gt_mask = (cv2.imread(mask_file,cv2.IMREAD_UNCHANGED))\n",
        "      #print (\"mask file is\", mask_file)\n",
        "      filename = mask_file.split('/')[-1]\n",
        "      #print(filename)\n",
        "      object_living_array = [1]\n",
        "      if((set(object_living_array) & set(gt_mask_array))== set(object_living_array)):\n",
        "            image_tensor = read_image(images_list[index])\n",
        "            #print(\"shape of image tensor\", np.shape(image_tensor))\n",
        "            prediction_mask = infer(image_tensor=image_tensor, model=reconstructed_model)\n",
        "            #print(prediction_mask)\n",
        "\n",
        "            output_Im = PIL.Image.fromarray(prediction_mask.astype(np.uint8))\n",
        "\n",
        "            index_mod = index+start \n",
        "\n",
        "            ##Saving the category ids in an image for programatic IoU check \n",
        "            output_Im.save(('/content/drive/MyDrive/InferenceLarge20192020shuffled-batch2/deeplab/program/' + str(index_mod) +'--runreviseddata-3--' +  filename))\n",
        "\n",
        "            prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 7)\n",
        "            overlay = get_overlay(image_tensor, prediction_colormap)\n",
        "            #print(\"shape of prediction mask is\", np.shape(prediction_mask))\n",
        "            #print(\"prediction colormap success\")\n",
        "\n",
        "            #want to get the gt mask displayed as well\n",
        "            #print(\"shape of gt mask is\", np.shape(gt_mask))\n",
        "            gt_colormap = decode_segmentation_masks(gt_mask, colormap, 7)\n",
        "            #print(\"gt colormap successful\")\n",
        "                              \n",
        "            '''\n",
        "            plot_samples_matplotlib(\n",
        "              [image_tensor, overlay, prediction_colormap], filename, index, start, figsize=(18, 14) \n",
        "              )\n",
        "            '''  \n",
        "                       \n",
        "            #adding gt mask to the plot                       \n",
        "            plot_samples_matplotlib(\n",
        "              [image_tensor, overlay, prediction_colormap,gt_colormap], filename, index, start, figsize=(18, 14) \n",
        "            )        \n",
        "            \n",
        "            '''\n",
        "            #the below code is if you want to print an individual image and not a comb image, mask, gt\n",
        "            imageormask(\n",
        "              [gt_colormap], filename, index, start, figsize=(18, 14) \n",
        "            )\n",
        "            '''"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = 2282\n",
        "end = 2296\n",
        "plot_predictions(test_images[start:end], test_masks[start:end], start, colormap, model=reconstructed_model)"
      ],
      "metadata": {
        "id": "JoIsnSixncf_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "592af63f-18f2-44b2-efc4-c55eb7f47027"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAD8CAYAAADjcbh8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3Qc93nw++8zW7G76EQHq9hFypJMNUvulossW3Zex5bjJLKjHJ2TnpvcJE7y3vtev6/fkx7HN8WOYidxcpPYjmzHslxlFUuyLEokRYoSeydA9Lq9zPzuHzMAQRJlQWCxC/D5nIOD3ZnZnQdlnv395tfEGINSSq1EVrkDUEqpUtEEp5RasTTBKaVWLE1wSqkVSxOcUmrF0gSnlFqxSpLgROTdInJURE6IyCdLcQ6llJqLLHY/OBHxAceAu4Eu4CXgo8aYQ4t6IqWUmkMpSnC3AieMMaeMMTngy8B9JTiPUkrNyl+C9+wAzk953gXcNtsLpKrWUNNaglCUWpmCdp76bJzG9DhVdq7c4ZTVXhg0xjRNt68UCa4oIvIQ8BAA1c3wsb8rVyhKLTs5oM8Yaka7+cF//jYtqRH8xil3WGUhcHamfaWoonYDq6c87/S2XcIY87AxZpcxZhdVdSUIQ6kVToTjdR3ccf9f88e3fJS0L1juiCpOKUpwLwGbRGQ9bmK7H/iZEpxHKSXC+Zpm/vdtP8NzHTt4x7l9/OzhH9KaGil3ZBVh0ROcMaYgIr8KfB/wAf9ojHltsc+jlLoo4w/x/XW38IO1uzjUsIa/fupviBSySLkDK7OS3IMzxnwH+E4p3lspNTMjwr9uv5vDDWv57X3/yQdP/BjfNXpvDnQkg1IrTsHy80L7dh541+/xK2/7NQ41rOFanfVRE5xSK1QqEObvd97LL7/t18lZZeswUVaa4JRayUTY3baN++77X3z2xg9ypqaFnOUrd1RLZtGHal1VEC1bjPaDU6q0xDjUZlN84fE/5/0nf0LA2OUOaVEI7DXG7Jpun5bglLpGGLEYDUX5hXf+Di+2bqUgK//yX/k/oVLqIhHGQ1He94FP80tv/02+tvEu7BXcmUQTnFLXoJFwNV/YeQ8PvOv3+Nzr3s9oMFrukEpCE5xS17BksIpfe+uv8qe7PkJPpGHFdSfRBKfUtU6EP7r1o9z5kb/iT3d9ZEWNadUEp5QCEU7XtfMHdz3IB97/KR5fczP2CmiEWP4/gVJq0Thi8YO1u/joPX/IVza/edm3tC7v6JVSi0+EoapaPvGu3+XPdn2EUzWtLNfRrJrglFLTyvkC/Pc3fJxdH/scf3DXLy7LKuvyi1gptWQcy8dIuJq/ed19fOKd/ycpf6jcIc1L5Y3ANYZN/afpqW0hEYpc3C4rtzOiUpUuGaziq5vfQtAu8N9OPMuN/SdoTY1UfBfhihuL2hwf5A+/9zf0Vq8i53ebq19YfxM/3rBLk5xSFcDn2KwZ7+Mvn/k89518vuxJbraxqBVXggvnM1Rn4tSlxye3tY73U51J4Fg+ztW3c66+nVSwShOeUmVgWz63S8mdD7JurJcdQ2cqdsGbikhwlrFxjJkxYTUnhvno3m8BkPUFeK19M599yy8sZYhqJZuoxegH5rwcbljDW3/6L/jwsR/xa/u/wfVDZ8temrtcRTQyrEoMY3n/ZJZjZv0lhew8m/pP0xwfWprg1IrndwwbhuPlDmP5EWE0XM3DO9/LXR/+LJ+6/efLHdEVKqIEB0JtehxHLN548kVkjvuCWX+InD8w6zGhfJY3nNpDwfJzsGMrE2kz5w+QDlYtWuQA4VyGUCE3+d5+O080m8a2LBKhqJYMKlzAdqjJ5MsdxvIlwlg4xtc23cWtvYe5tfcojZnxiijNVUQjw/pAxPzuqp0gQrCQI+DMPhHfaFUNBzq2kQxW8djOd5Cc2trquen8q/z6U/+EEcj4w0z8tgej9Rxq28RjO95BIrzwGRQsx+FnX/w6t595mf7qRs7Wd1CXHmfTwBlSgSoOtW3EIDy78VZONK8v/o212rQ0jOHmC24NYk/nqnJHs+yFCjnaE0M8dPAxHjj8OK3J4ZInuspvZDAQzWeKPrwuPc6bT+zGANXZJF94w/1XJAKfY2NhvPdOT26P5tKsGbnA8eYN7F2zc8GhR3IpXn/uINFcmvVDXawf6rrkXG8+8SIAJ5vWcaJpXdEJa+PAGdYNd/HDLXdpkishAbb1j/LY1tVzHqvmlvUHOV3Xxh/c9Yt8c+OdfPfrv09tLlm20lxF3IO7WgLccvYAH9n7Lbb3HMNvF1fNEGD94LmLpSTcKm1NOn7JtjkZw65zB6nOJuY8tGV8oOi39Tk2N51/jXcefpaqeSR+NX9NiQwt8fTcB6p5MSLsadnMe37qj3hl1fqyjWmtjBLcAoQLOe459DTvOPoch1o38/i2N5IMVtESH5z1dTt6jvLoDXe7fe2M4V2Hf8QtZw/wz7f/NHmfn666NhyxZi09Wcbw5uMv4CsiKdanx6aPP5+hbaz/km03dh3i3Yee5kDndvK+2e81qoWpzebxO+W/TbMSFSw/L7Ru480f/gw/d/iH/PmPPk/QKSxpaW7ZJ7gJQbvA67oPcUP3IUCQOabu6xzpoTk+SFd9O36nwO2nX6Z9rI///t2/xrZ8nGto57W2zXztpntmfR/fHPcLJ7SN9SMYprYRizdq47ee+MIlx4p31IbBc9x0/lX2rLkBYy3rwnZlMobtfSMYwOhdgNIQYSwU4ws77mFf00Y+9+Rn2TF4esmqjismwYFb9XT/T+f+RO6ua2Ug1gjA3Ueeo2V80Hu9wXIKXDd4jnA+S8HyEQ/FONC5jVSw6pIW2I7RXmrTxXUvyARCk9FNaBvr4+MvPDKZ0C5Xnx7ngd1fYzhax8mmdUWdRxWvPp1jw1CCs/Ux4kEtKZdSxh/k+fbrue/9/5P7jz7Nh44/w839x0temltRCW4+Okd7uanrNU40reUdR57DP80Sah1jffzUge/jIHxkX4DTjav5xuvejW1ZDEXred/BH1KbmTvBJYIR/n3XfZek3frkKJ/4yVdpTM4+ni/jDzEYbbi4YWp1eIkbH8Q4tI/2MRKppX2sHzCsHunhfH3b5DHjVdX0xxqXRcPI1oExovkCeZ+FsSo/3mVPhDO1bfzxrR/lP7a8lUe+/T+5ue/4jB/wi+GaTXA+x+ah5/6dvppVNCZHZj3WwhAu5Njad5Lf/8HfAjAcqSNcyBb1h8n5A25pceKiN4Ybug+zaeDMnK//0abbGKuqBkAchxu7D/Hm47v5hzvvJxmMLGki8dsFfuWZf0GMoXW8f9rY+6tX8al7foNkqMIXMTGGgO0guPdSmWUkjVp8Z2tauPe+T/Pmrlf4wuN/QXW+NA091+yNHQF8xqF9bPoLdabXTHw1pkaJ5or7o2T8IcyUi0eAO07vK+q84+HY5IW3aeA0Dz7/FXZcOMqHXv5OkVEvroBdoH28H4tLfx8TX7Fssuj7kkvOGGoyOaozOaoKNtv7RgHoGEsRzRXKHNw1RoS+aANf33gX7/ngH3GqprUkp7lmS3BL6eXV15P1T13IwxAq5Ob9Pned3EMsmyTvC+BzHDb3n6Iql+GO0/uwjMNopJYvv/59OJZv8YKfIppLEyiyK04l8hnDRw6cpi9WRV0mx6pUFoDu2gjJoF4K5VDw+flx+/W87wOf5tde/i/uP/oUdbnkor2//lVLzBahu671kupPsJDHuorZF3yOu0Rv0M7zphO7edOJ3ZP7BNi3+vpFiHgGxrC17+Qls7xMZyDWUNFdW/yOIe+zWD16sfOpI6LV03IS4VDjOn7p7b/Bqdo2PnDyx9zRc2hR7stds1XUpZIKRjjcuvGSbR/Z+y06R3rmfO1IVQ3nGjoA2NZznHXDF0dJXF41BDjSct3ilN6MoTExTMv4AHWpi/337ji1d85/utONqxd9rO9iaUhlieYKdI6Vr2e9moUIf3bLR/jg+z/FN697w6IsX6gluBJzxOLndn+dV9u3cKG2BYC28f6i5s862bSWqnyGrb0n2NZ7gs7R3hmPPdi2mQu1LWzrOc5IpJbe2mYAGhIjOJbFaKS26Jh9xuHXn/4n2kf7iIdjHGvZwPPrb6YhNX1n5WXBGNaMJrEtoSmZuSTBRfIFfLaD7dPP+0rQH6nnw+/9v/i37/4RP338mQW9lya4EqvNxLm56zVu6npt3q99/bmDvP7cQWzLx6nG2cdKDsYa2NFzjHcd+hGHWzeye92NAOy4cIwjrdcVPaY1kktz25mXqU3HCToFGlOj3H56H7ef3jfv+CvN1oExztVF2TAcx1e4+AFTnc0TdBzSmuAqRt4X4M92fZjW5DC39h4h5FxdI9CcCU5E/hG4F+g3xuzwtjUAXwHWAWeADxtjRkREgM8C9wAp4OPGmDmvjGDIpqYuRy5rYYyQzUy00a0cV/PTTLzG79hsHjgz67EbB85iWz4E2N57gu29Jyb31aXHeWLznZe05M6kNj3Ox178xiUzuhQbu4NwfD4zpiwhn9ctZPVo8oqhWX2xKtIB/ayvNC+1buU9H/wj/uS5f+AXXv0eVfb8G+aK+cj6Z+Ddl237JPCEMWYT8IT3HOA9wCbv6yHgc8UEEQg6bNk5ys5dw1x/8zBbdoyxcdsYdY1ZqqLafF+M1aM9l9yjm6omEyeaS5U8BiNwvr695Oe5GvWpHG3jaQJzTKiqKksyWMVvvvmXufcDn+aVVfP/8JwzwRljngGGL9t8H/Al7/GXgA9M2f4vxvUCUCcibRTBstyvQMBQ15ijoSnL5uvHuG7LOC3tKVraUzS3pbGsypz7vZI1x4d4z6GnCeWzWI6NNUs/tY7R3snZlVeSgOPMOT5ZVaaCz8+Ta27mD9/wC7zauG5ef8WrLZe3GGMmmgF7gRbvcQdwfspxXd62K5oMReQh3FIe7VVX5tmJ2lS0ukC02p2OyHEgVp3HdtyddkEYGgiTSfmmjGDSz+fLCe7kAm3j/e6wNLvAwY6tnGlcTTJYNdkAESzkuPvws/iucgERwV0VzbZ8GKC/upFCBXQZCRRs3n6iB2uaK8MA5+oqfNSFAuCx6+7gaMNqHnnsU+wcPF3Ulb7gGw/GGCMi8/5oNMY8DDwMsKM+UNTrLQua2i7Oj2YMtK1OkYwHsG333t3okLswrQHio0EcRxMewHWD5/jk9/9ucnjZbWf244hwuHUTj2+7C4BwPsvq0bm7r8xEjOGXn/lX9xNWhB9tvI0DndswCEdbriMbKM+iwQHH0JJIz3hB1Gbmf29Hlcfxug7ed9+nuaPnEL/+8je4o+fQrMdfbYLrE5E2Y0yPVwWdmNCsG5ja3NfpbSuJif6Z1bUXe9e3tLsJ0HGg+2yUwb4wEwUSxwh2YWrPsWtHdfbS3uEWBssYdvYcZWfP0UU5x8TwNwAMvP3Y87z92PPYYvHd69/C/s7r6a5rdZd8rBAC1Op6DMuHCOdqWjhX08LLTRv5i2c+D6d3z3j41baLPwo84D1+APjmlO0/L67bgbEpVdklZVnQuS7JztcPc8Mt7tf1N41QU58nEnW/xDK4ZT29N1NKPuNw76tP8juP/z0fe/Eb85s1+Wp5A+hDBZu7zvQRsPXe7UpzrGE1H7r3f8x6TDHdRP4DeAuwSkS6gP8B/DHwVRF5EDgLfNg7/Du4XURO4HYT+cTVBr8YRMA/pfbr89ts3ekOsDYGUgk/uayPwf4QjiOMjwQxOvNhyYTsHDd0H+ZNJ17kmU23le5ExvCOo8+xtfckJ1puYteFmcvsBjjaVHwnaFVZLh3jfaU5E5wx5qMz7Hr7NMca4FeKiqwMpnYDE4FYTQEo0NCUxbahpyuCYwuOIwwPhMjnVl5/vHILF7JUZxJgDOuHzhPLJjlf3048HMW2FqcvmmUMbzq+m7UjF9jWP0pvw1tm7OScCPrprrlyVTa1MmjvRo/PB51r3b5ixkBbZ4rx0eBkbSqT9tHbFbmsdqXJb75ea9vCk1vu5N2Hnuadh5+lJpMg5/PzasdWHrnxPfRXe0v3LWDwe8v4AI3JURwsxqJbZn2v8XCAsXD5W3pVaWiCm4YIhMIOTa2Xttg2rMpOtsoO9oUZ6g97t3o00RVr48AZfueHn2fdUNdkg0TAKXDrmf1sGDzHYLSef731p66YgWU+GlKjRHMpcv46MqGWWY89otXTFU0TXJEuVmldkWiB9jVJMik/o8NBRoYudoFwbKFQ0HGN06nOJq9o0QW3LNyUGGZVYpjffOof+eHWu3h86xtxrmKxnTtO78PgY7jmdZhZ2tGyPouz9bEFlRZVZdMEd5X8AYM/YAhX5ahtyLF6/cWLNjHu5/yZ2OTzfM4il9X7ecUQoDkxxAcOfJ+jLRs4M8ckAwCxTIKf3vdtAt6A7K29Jyn4IqRDbbMmr5GqIP2x8GKFriqQJrhFIAI+/8WbczX1ea6vv7jOQy5ncfZEjLERr8XHCI4DmvBmFs57jRFFCBby3Hr2AJEpi2SPRTbhyOwtbCcaa8jrcowrmia4Eri80BAKOazfHMex3R35vMXIYIiRwRC2LVqlnUYmEGI8XD3tvtaxfvxOgUzALX1FcmnGqqonE5yDj0yoee6qp6DV0xVOE9wSCQQMeH3yQmGHaMy9hweQzfhIJf1gYKg/TDLh97qoTHVtXYiRfIa69Bhn6bx0hzFc33OMj+z91uT0T2IMwSlrRYzU3EC86rpZ37+7popjq2LEMgkSoagmuhVKE1yZiLhdUwAiUZtI1MYYaGzOkstakwkuk/Zx8khNGSOtPLecPUBohsVvHCyy/vo5E9Z4KMDWngO88/AzPHzXz3Bq1dqi5stTy4smuAoycX2Fwg6hsOM9tlm9Pjk5mCyb9jE8GMK+Bqq0dek4PruA7Sv+33Qstp10eI4ZuozN+sFT3Dn0HPXpMX796X/mxbWvI+sPYls+Xlp7Azl/EAdhOFp/VS25qjJogqtwgaChfc3FySqNgdaOFJm0+6eLjwcYHwm4VdxJK6Mk8nO7v84tZw5woa6Fr9x8L7bl463HnqdjhrUp8r4oiap1GJn93zqYH2PDwPewcOfFq0uP884jzwLu0K17XnsSg3Ck9To+f9fPkgrpSIflShPcMiMCkZhNJOZenPWrsji2kEm79d3zp6Mk4gEcW5Z9B+SAU2Bnz1G29J9k48AZ/vZNP8+64W5qpulHZ4BUqINsoGHO942lzyBMP+mnuyxjgbzl56s331tRM5+o+dMEt8xNdFGJVrt9wDZuG8dxhGTcTzrl96qzgjGQy/qWZdIL2gVaxwfwOzbhfHbaY4z4GI9unPPemzh5ItmeOcu4Zxo76a1p4vbT+1g7cmFy+1C0joPtW9xzIgzFGrBLtNC2WjhNcCuMO3uKIRjKUWdytHZeHF/b1x252BcPSKd8y2ZCAVssfm7319jad2ra/dlAA9nAqjnfJ5rpIpQfmvO4guUjWMhz/95vUT9lsWsHmUxojgiPb3sTe1fv4Ezjar1XV4E0wa1gly/Y3rY6Rdvqi/fzxoaDXgut24ThOOA4lXmR1mST7Ow5Nu0+A8QjG6fddzm/nSpqbYa8L0Akn7li+nYLgzVlCbv3vvoEdx9+hqc33c4jN7+X3BzT96ilpQnuGnJ57a2mPscNt1wszWTSfnq7qyYbLPI5a1m01ub8dSSq1s5ZPQ3mR4hmzs35fgXLx4823cYv/vg/iGWuvN83lQAhO8/bjv2EVckRvviGj5AM6RoPlUIT3DXMssAKXizNBIJ5YjX5i1NEpXz0XoiQy7hJLpkIUMhXVsIzQKJq7ZzDsgD8hQRVuf45jxuO1OFzbNYNd2EVOdtzwCmw48JR1oxc4HDrpqJeo0pPE5y6xNRqbSRms35TfHLfYF+Yc6fcSQSMA7Zd/mRnxEeyas3cIxGMwTLFrbFrgDtP7qFqhgaNmcTDMU41rpnXa1RpaYJTs5qaNxqbM9Q1uhf9+EiQ7nNuVczOC7kyNVY4EsS2ipsRpDo9fQPF5c40rqY6W9xA/6mSwQjOTInWGCzj0Do+QNYfZCha727X0RMlpQlOFc1dnNutsjU0Zalf5Sa7fM5ioDdMMh4gm/WRTi7Nv5UtAUZj27CtufuqhXN9hHMDRb1vJhDixq7X5h3Py6uvJz/DOrA1mQQP7H6End1HSAfCdNW3cah1E6+1bebMqrmnhFJXRxOcuipTq7KhsEPnOrd1Nj7uZ3ggTHw0QDrtwylhNTbvr2E0tqOoUlAsfRafM3eVM2/5sIyDz5m+I/BMcj4/6UBo2lgsx2Zr7wlef+7gZKNEXU+cHT3HOH96Hz/cehfPXXcLhbmGpBlDLJtk48BZQoUcR1qvIxUIk9eW2xlpglOLqrqmQHVNAtsW0kl3lhRjhO4zEfKXNFAsYdXMOEUlN3Cj2tR/Gr+Z3zKDBzq288SWu67YHizk+JmX/ovbzuyf9ifuGO1l7XAXz153y5znsIzD//HUF1k71IUYQzYQ4kxDJ9/b/hYOtW2aO0Feg/Q3okrC5zPEagrEagoYA9FYfnI9i/hYgK6zUVjAqAoHH2PRrUUdazk5qrIX5j4Q8Ds2rfHBecdjWz7y/iurpzsuHOWNJ1/CP0OJ8GjLBv5j131FTShQk0lQnxwj4L2XP5dme+9xNvef5m/e/AD7V18/77hXOk1wquSuWM8iVsAYwbaF0aEguaxvMvkVy7FCJIpsPa1On8Iy86tyzocBMtNVE42hKTE8Y3IDd42Kbb0nOLVqDfE55qXrHO2hITV6yTYBxDhU5TPucBVttLiEJji15Px+Q+e6JMZA51ohEfeTSfsY6g/j2EI65Z9zSvd0qHnOWUMAgoVRGsYPFN1F5Gpk/UGe3nzHFdstY7jt9MuzvrZztJdfe/qfGK2q4UjrRgzwWvsWTqxay2Cs4ZKEVZOevmXXbxx+av93eXn19ZOzHCuXJjhVNhMTBdTW56mpy9Pc5k45noj7GeipwrYF4wijw8ErJgnI+6pB5m7AqMr2YplcSeKfcKG2hTONnVdsXz3STcfY9FM7TRVwbJqSIzSdfAmAN558ifFwNV9+/ft4/rpdk8fdenb6+3jgVq2L7JN8TdEEpyrC1JqV21DhdjAu5IXDB+ooFCyMcbukOOLHsYprOYxkL5S8OaMuNc69B59g75qdJEIRdy0JEepTY4QK80+ugrvORHLKPHSWYxMsTD+LsZqZJjhV0Xx+w/abRsGA461ZcWpoM6NV2+Z8bSx1ilBuuOQxNqTH+G/7v8u9rz5BIhTlWPN6QNg4cOaq3/NY83oOt16cQKApMTzj+xUsH4/uvHvaRo5rnSY4VdHctSvcupcPaGlPk6tNc7p37jnYQvkh/E5qzuMWgwDhQo5wIceq0yNzHj+X4WjdJTOTWMbBmqHriiMWr7Zv1nnpplH+wYRKzdNgZo41FwAxBUL5hScatbxpglPLgjHuVzxfT29q7VxH47fytNX1grgTgF78qnw9NU08ueUNl2yrTccRc2X8BnQ1sFloFVUtC8lCDbv738VItpmsM9fYU8NtzY/TERkg2+FOeFTIWwz1Tyy2Xdmf64lQ9IpZSW7seu2KyTcn/Nuu+xiJ1C1FaMuOJji1JLIBQQwEC/MvRWXsKo6M7qI3vY6QL0XYlyRjx2Y8XoDq4Ah+n4O/+mJSqK3P0Rcr0N9TheOAMUIuW3lTtqeCl/ZlC+Wz+Bxn2ihHq2p4tX2LDtOagf5WVMmlQxZP3VlLOOuw7nyG02vcC3j9uQwbzs2wiIwBg8VAuoO9g28jbUfpiJzkxsZnGM2t4sd975vxfLXBIWL+sSu2i0BLR5rm9rTbKusIXWejk7OfVMoaFefqO4jk0mQCIaK5NA8+/xXWDHdPe+x4uFpLb7OYM8GJyGrgX4AW3Cr/w8aYz4pIA/AVYB1wBviwMWZERAT4LHAPkAI+bozZV5rw1XIw0OintzmAz4au9iAFvwXGsOPI9C2ctvFxfOx1nBrfwVi+kcZQL29seoKmcBcA55KbZz1fyJcmYE2fOKfOgmL5DGuvuzg6IBn3M9BbhV0QhgZClCvR3fvqE9xy9gA5f5BQIUtzfKjCypjLRzEluALw28aYfSJSDewVkceBjwNPGGP+WEQ+CXwS+D3gPcAm7+s24HPed3UNMkAuaIEIth/mShqpQoz+dCf7h96EYNhW9xJrYsdoCLlTjRsDfamZ508LWSnaIqeLjm/q/Xl3coA4hYIQrckzPBCikLfIZa0lXYxHoOgB/8eb1808yaaaO8EZY3qAHu9xXEQOAx3AfcBbvMO+BDyNm+DuA/7FGGOAF0SkTkTavPdR1xADxKM+9rxu+vtlx66rIpxxqEnYGGNxIbWePQNvI2NHaYucYVvdSzSFu4scP27YUP0q2+tfpCYwsqAx536/obUjTUt7GoBkwk8u46PrbJSst8D2xQbN8iaXguWjOpucc6B+yXgTCjTHBzEiHG9aV1Hz083rHpyIrANuAnYDLVOSVi9uFRbc5Hd+ysu6vG2a4K4hBkhELZ66q5ZU1TSlHxGOrw9zvj3EG18Yo3DsOn7S9x58UmBd9WFubfoBlhTfIFEf7OeWph/itxZnUP3Uqmx1TQFTXaC6Nk+h4G5Mp/ycOFSzKOdaiHcdfoYbuo/wd2/6eXpqmyjMMKPwYhNjiORSbO4/zcd/8p/UZuLkfQH+4P2/w0D13OvTLpWiE5yIxICvAb9pjBmXKZ8WxhgjMo//Rvf9HgIeAmif7gJQy8LlXbMydpTBTBvR6gGevcvHUL1/+pKFMYTifsLdNRw/8zriQ+tZV32ErXV7qA6MzJjcbOPDNgHcFOq+r2CzJnYUn5RuxhARCIYcgiH3eSDgsHaje/8un7MY7At7LbKwlKU6AdrG+/nkD/6O483reLV9C09tfkPJRzWsSgzzOz/8e6ozCSJ5d5IEn2Nz0/nXeG7jraSCc08jvxSKSnAiEsBNbv9mjPm6t7lvouopIm3AxHps3cDUmySd3rZLGGMeBh4G2FEfWB49MNUljIFDo7eRtcOM5txP7XShmrFcI4Yh8kOvYA2B0zoMVd5NfzHgc5NT+Fu3kj59HeHAADc0Psfq6PE5S2AZO0ZTuJva4BCn4juwpMAbmr/D6tjxJa2hBSmqfbwAABm2SURBVIJuNRbc30NzW5p0yk8+ZzE8ECKT9pHJ+Ly+xaUNTIBYLsVNXYfY2X2EhuQYR1o2MBqppbuuFVusRa++3tB9mOb44CU/mc84fHTPo9zQfYRHbr6Hs/UdGMtL+sYgLH2n5GJaUQX4InDYGPOXU3Y9CjwA/LH3/ZtTtv+qiHwZt3FhTO+/rUxpO8aR0deTsadZ6Li3icCX347BQE0SE3QTl1nTR+EtByCYJ5WrxTIWm2oP0FZ1loITRDD4rJkniIwFxnh901O8Ouy2WwmGutAAlsxvivHFJOKuSxEKuzOHNLVmyOeEsZEgZ09WgzFe1bb0F7ffOLz3tSd572tPkgqEOd24mi/ceT8Fy08mELpkfOtCiJewLmdh2NlzlI0/OMN/3vRent/wegJ2gbrUGA2pMY60XjftnHUBO0/emqG0vwDFlODuBH4OOCgi+71tf4Cb2L4qIg8CZ4EPe/u+g9tF5ARuN5FPLGrEqiIYA/3pTrL27BMsCgLjscmLwQxXEzy0DtMxiIxUA7Bv8C3sH3ojAE3hbiL+ia4bhrWxowR9bhVIxKEmMIxMGXJlGz9dyU3UBHZX1GS2gaChoSlLXUMO2xb6e6oYHXKTi21bZDOlHxgfyWfY1nuC//WtPweEcw3tHGrdxP7O6xmJ1JAIeR9M8/zFieOwpX/mJRgNcLxpPYlQlN99/PM0x4ewjIMYQ09tM2caOznQsZ1X2zdTsPy0xAf54P7vkfcHKHhV6/2d2znQsf3K2CbuiRQZs5hpxrcttR31AfO1t1XOjUk1t750J7v730U831DCs5hLkpklNo2hXkQckvlaEgW3g+uq0AXe3vFl/LOU/Mpt6mWWjPu54K0pm4j73cV4FrA+xbziwJ19pK96FaORGs42dHKseT2nVq0hEYpQKKIU9aF93+buI88SnmGuOwP873f9Ktt7j/PBA9+ftqRni8UzG2/j1Ko1fOjlb1OTSVxyXCoQ5vkNr+eRm+4hHQiDCOFchg/ve4yTTWvZve7Giw0qn3nHXmPMrmlOoyMZ1NVpCPWzOnqcQ6O3UrqqlzC1ImQbi/7MlX3g0nYUgwVUboK7vL/d5h1jGAO5nMXAhSr6e8MU8tYVMxcvehy498rax/tpH+9ne+8J3n3oaUYitQxG6/mHO3+GRChCaspkm5dbPdIzY3KbYERmrMbixfDW4z/hrcd/Mu3+SD7D247+mHVDXXz+jT/LcLSON558kbce+wm3n3mZg+1bGK+au8VYE5y6KgErx/b6FxnPN9CV3Eg5+4M5xkfeCRGwlteMtyIQCjm0r03S2plifCxAIW8xNhJkdCjkrUsxeXTp4gAaUmPUpcb51Lf/kqFoPSdXrWGsqoYfbbqNoWj9vN4v6w+SXYR7fRawYfAc/8+3/5I/vfuX2N+5nfcd/CGRXJqP7nmUZzfeyqHWTbO+hyY4ddVCvgw3Nj7DQKad7HQNDUskbUfpT3eyrvpI2WJYCMsCyzI0rLrYSJFK+LFtN6n1nI8wPuomjLkW41lQHBgi+QyR0R5Wj/ZggDefeIHe6iYAuutaOdfQwYbBs7O+z/7O66nOJukY61uUmIJ2HjAMxBp5duOt3PPqU7zh9D5u6D7CT9bfzP83y+s1wakFqQkMc2PDc7w4cLdXTSyHCmpdWAQiEK2+2F0mGhsnnxeMEU4criGVWKLOvEBdOk5d2l0fY+ssDQtTHWzfwk/t/x6bFjBl+1RfvfleuuvaQIRHd97NTedfo2Osj1guxd1Hn9MEp0pHBNZVH2Is18hApgMHi7HsKpwrkl3pklDISlEf6p/7wGXK5zf4/AZj4Lot4yQTAUa89WSzGR+FfGV1lH/z8RdouYrFs6cTD0UvmQ4q5w8wHKktunSoCU4tmN8qcPOqpwBwsBjJNmOMe9FlnSrOxrdONhYMZNrJ2W4vdwPeqISFJT+/lSfijy/oPZYDEYjEbCIxm1UtbteZgZ4w46NBMmlfUevJLoXNi1RyA/d+3vCU6aAMwp61N7Cz51hRr9cEpxbFRCuhD4dV4UvXAu2MngTcrhIZO4Jt3H872/j5Sd97JjsKO8YibceY7wXqt/KXdCe5Fkz8vpvbMzS3ZygUBLsgnDxSQ3yscga7l0I4X/xSjJrg1JIRgSr/xTngjIF3dHxlsnRnOwH60qsnnycLNV4LrWsk2zyZHCffE4dNNQdKOg51OfD7DT6fYcOWuNu3LmsxNBAmlfB7ffCW533Kyz+2LONw25mXi369JjhVNiLgn5KYAlaetdVHJ58bA9vrXpw4mtHcKmxzcQTAaK6J+uAAdaGBihrFUC4iEK6yCVfZGAOtHWni4wEG+8Jk0j6SCT+OI0vWqXihbLH4wbY3kZuy3uttZ/bTMdo7y6supQlOVaxLk5ahPjRwyf7Lq8LqIvGGvtbU5ampy2Pb7sI7Z0/GGBks32zF85H3+dm7ZidGLjai7Dr3CiG7+P6OmuCUugb4fODzOazbmKCxOUshLwz1h0nGvVJdBSY8yzhc33OM7T3H2btmJ83xIa4bmL0P3uV0LKpS16CJyz6Z8DPYG8a2heHBEE4FLak4XWaaLg0/MPSKjkVVSl00Uf2PVReIVScwjrsIz9iI27/OOOUv0S1GBJrglFKIBes2uokuEQ9QKAi9XRHiYwvvp1hOmuCUUoC3DoXPbZgAiMYKJOIBxkfcJDc8EKJQqJwqbDE0wSmlpuXOUpylsSmLMZDPC6lEgGym/ItjF0sTnFKqKBu3jePYQmI8gO0I3WeiZNK+iu5IrAlOKTWniWUULctQ1+gOlaquybvdTQbcjsTDA5XXv04TnFLqqkwsoxiJJcnnhGDIYWw4SC5nYS/RIjtz0QSnlFqwQNCwZkMCsx4yKR+9FyIkxgJlr8JqglNKLYqJamwkZrNhc5x8XijkLYb6Q/R0RcrSiVgTnFKqJAIBQyBg074mRbjKZnggTDZjkcv5KOSXaJ3Ykp9BKXVNsyxY1ZKlsTkLuMsm9pyPkMm4k3RiKNlqYprglFJLYnJ4WE2BTdePUyi4VdjxkQCnj9eU5Jya4JRSZeH3G/x+GxHD+s3jxEcDpFN+cjmLfM439xsUc45FeRellLpKobBDc1uGplZ3nYmRwRBDAyFyWYtkIoBZwDoTmuCUUhVhogrb0JSloSmLbUMq6efYwToKBU1wSqkVxOdzp3PadP0YuayFXbDo66kinfRRbIlOE5xSqmKJXJzdxBhobMkwPBBifNRdOWxsZPYVxDTBKaWWBRG3YaK5LUNzWwZjoO9CFcyyNMfymtxJKaU8ItDSlp71GE1wSqllS+bIYJrglFIrliY4pdSKNWeCE5GwiLwoIgdE5DUR+ZS3fb2I7BaREyLyFREJettD3vMT3v51pf0RlFJqesWU4LLA24wxrwNuBN4tIrcDfwJ8xhizERgBHvSOfxAY8bZ/xjtOKaWW3JwJzrgS3tOA92WAtwGPeNu/BHzAe3yf9xxv/9tFpPxTeyqlrjlF3YMTEZ+I7Af6gceBk8CoMabgHdIFdHiPO4DzAN7+MaBxmvd8SET2iMiekayzsJ9CKaWmUVSCM8bYxpgbgU7gVmDrQk9sjHnYGLPLGLOrPqRtHUqpxTevzGKMGQWeAu4A6kRkYiREJ9DtPe4GVgN4+2uBoUWJViml5qGYVtQmEanzHlcBdwOHcRPdh7zDHgC+6T1+1HuOt/9JY9xlJ5RSaikVMxa1DfiSiPhwE+JXjTGPicgh4Msi8mngZeCL3vFfBP5VRE4Aw8D9JYhbKaXmNGeCM8a8Atw0zfZTuPfjLt+eAX56UaJTSqkF0Lv7SqkVSxOcUmrF0gSnlFqxNMEppVYsTXBKqRVLE5xSasXSBKeUWrEqZ9GZ6QY76CQkSqkFqIgEF4zbtO8exSoYrLyb6DJ1fjINF5cEy9QHyFdZGL9o4lNKFaUiEpxlG6ovZC/ZVjWSh9MXV8yxA4IdsOi/sYZka2ipQ1RKLUMVkeAAjMDAzmoKYfe2YHikQHg4N7k/mLAJpGxCY3lNcEqpolRMggPAQLwtDD4h3nHpPblAysbKGQpV2i6ilCpOxWQLMdBwLIlle4lN5JKvfNRPtj6AHfaVN1Cl1LJRMQnOAJm6AEbbD5RSi6RiqqjpVUEu3F6HqZiUq5Ra7iomwSVagxifFt+UUounYspLqSZtGVVKLa6KSHB2UMjWVUxhUim1QlREgstX+XR0glJq0VVEgkNzm1KqBCojwSmlVAloglNKrVia4JRSK5YmOKXUilU5CW66CS+VUmoBKiLBBZI2Ypc7CqXUSlMRCc6XN3Q+P0z98SSBRAGxtTSnlFq4ikhwAJHBPE0H46x7cohoT1arrEqpBauo8VECUDC0HBjHn4uRWhUkH7HcQfg60kEpNU8VleDATXL+rEPL/nHsoLsOw9C2GPH2EMZfMQVOpdQyUNEZw5czBJM2rXvHqD2TdqutWnVVShWp4kpw0xEDTYcSVHdniHdWkWwJko/qAH2l1OyWRYIDsAqGyFCeyFCebLWP4c0xUk1BdxEaTXRKqWkUXUUVEZ+IvCwij3nP14vIbhE5ISJfEZGgtz3kPT/h7V+32EGH4jat+8ZY/dwwodGCVluVulbNce3P5x7cbwCHpzz/E+AzxpiNwAjwoLf9QWDE2/4Z77hFJwYCCZvVz7n950IjeXA00Sl1zXAMqw4lZj2kqAQnIp3Ae4EveM8FeBvwiHfIl4APeI/v857j7X+7d/yiE9xOws2vJlj97DCte8eI9mRKcSqlVIVpOJak4Vhy1mOKLcH9FfC7gOM9bwRGjTEF73kX0OE97gDOA3j7x7zjLyEiD4nIHhHZM1BkELPxFQy15zPUnkvjT9luaU6rrkqtLMYgtiF2IUP9qRQyxyU+ZyODiNwL9Btj9orIWxYnSjDGPAw8DLBL5gqzeLHuLOGhIeyQRbwzTKI9TK562bSlKKVmEfC6jVUN593FlOdQzJV/J/B+EbkHCAM1wGeBOhHxe6W0TqDbO74bWA10iYgfqAWG5v2TXCUBAhmHQMYhPJYgMpCj76Ya8lFNckotZ76sQ/uLo4RGC0WvcjBnFdUY8/vGmE5jzDrgfuBJY8zHgKeAD3mHPQB803v8qPccb/+TxpSvrhjpz9H20hiNhxOEh3NabVVqGZKCQ8vLY/NKbrCwkQy/B/yWiJzAvcf2RW/7F4FGb/tvAZ9cwDkWTICq4TyNhxO0vThGtCeLlXU00Sm1HBiDL2vTeDRJrCc77/WppIyFq0m7RMyeJTqXY4Edtki0hBjeEqMQ8S3RmZVS81Xdlab5QBxfzpmxQUFgrzFm13T7KnosailYDgRSDnWn08QuZAjEC9p/TqlK5BiifTn82ZmT21yu2TvvArS8EqcQTJKt9ZPoCDO2tsqdmkkpVV7GUH0hQ8259ILe5ppNcBP8OQf/QI6qoRz+lM3g9pib/XR8q1JlVXs6fdUltwnXfIKbYDlQfzJFeCRPfHXYnWxTZyxRqiyC4wXCo/kFv48muCks2xAdyBEdyJGt9jG0JUayLYQTuOZuVSpVPsZQfzKFlV/4vXFNcDMIxW3a9o6RqQuQq/EztCVKPqa/LqVKyhiquzPUnE/Pu0vIdPSKnYUYqBrJEx7JY+UdhrbGyNb6tdqqVCkYQ2QgR9PBONYiLSOqCa4IAlRfyBIeyTO2topEW5h8zKdVV6UWWf2JFIG0M/eBRdIrdB4CaYfGI0nWPDNEw9GkrhGh1CIKpGz86cVdAV5LcPMkgNhQfyJJMFEg3hkm2RzCCejShkpdLbENLS+PExorzH3wPGiCu0qW41ZbYxey5Kp9jG6IMN5ZhRPSQrFS81U1mKNqMLcoDQtT6dW4QILb4tp8IO4uWJ22tdqq1HwYQ2isgLV4t94maQlukQhQ3ZWhaijH0NYYY+uqtMqq1FyMIZCwqV3gkKyZaIJbRILbENH8SpzQaJ5ka4hUc0jHtyo1A1/WoX33KKHxxb33NkGrqCVg2Yb602k6Xhil9kwKKWiVVakrGIM/6xCKlya5gSa4khIDzQfiNL8yrlMyKXUZX9ahZd94UWsrXC2topaYADVdGQpVPsY7w+4AfkurrOraJnmH5oNxwiP5RW85nUpLcEvAKhgaDydY+9QQsZ6slubUNa9qOE/1+UxJkxtoglsygpvo2l8cpfFoEsmXoE1cqWWi5nxpWk0vp1XUJSQABhqPJIj2ZhnZFCHeEdbuJOqa4svYVA2Vtmo6QRNcGUzMUuJ/JU4h5CO9KqBJTl0TxDZE+7IEUos75nQmWkUto0DGofP5EWrOZbQribomxC5kaN03vuCpyIulCa7MLNvQ8vIYHT8ZIdqTQWxNdGqFMoZYb3bJkhtogqsIlgPRgRwdL4wSGciWOxylSiI8nCc8tPB1FuZDE1wFEQMNR5LUnk5pSU6tKFWDOVpfHie4RPfeJmgjQ4WJDOepGs4T7cvSd2MNdsjSBgi1vBlDtDdbsvGms9ESXAUSIHYhS9ueMXw5nTVYLW9WwW05Lcu5y3JWNScBIgM5On88PLnojVLLjjHEujNlKb2BVlErmhgIjRZY/eww2Wo/AzfUkG7UPnNq+QgkbJoOJZa05XQqLcFVOAEsG6pGCzQcSyJakFPLhTHUn0rhy5Tvn1YT3DIS7c/S+WO3v5zel1OVLpiwF20B56ulVdRlRByIDObc2YJbQgxeHyMf0z+hqkDGUN2VwcqV94NYS3DLkK9gqO7OsOpQAl/W0dKcqizGUHcy5fbnLHMo+vG/TE0schMezTOyIcLo+gjo2g+qQtR0ZQiU8d7bhKJKcCJyRkQOish+EdnjbWsQkcdF5Lj3vd7bLiLy/4rICRF5RURuLuUPcC0T3PsczQfjNJxMaklOqcvMp4r6VmPMjcaYXd7zTwJPGGM2AU94zwHeA2zyvh4CPrdYwarpiYH6Y0lWvZbQ/nKq7ILxQtn6vV1uIffg7gO+5D3+EvCBKdv/xbheAOpEpG0B51FF8OcMDceTrH52mJqzaaycJjpVHpH+XMVM/1VsgjPAD0Rkr4g85G1rMcb0eI97gRbvcQdwfspru7xtlxCRh0Rkj4jsGbiKwNWVxEB4tEDrvjFa947p2g9q6RlDeHRpZustRrGNDHcZY7pFpBl4XESOTN1pjDEi8+urbIx5GHgYYNc8X6tmJwaifVlWHU4wcl0EO+wrd0jqGuHLOkQGcuUOY1JRJThjTLf3vR/4BnAr0DdR9fS+93uHdwOrp7y809umlpDlQMPRJO27R4l164zBammIAStfOf9rcyY4EYmKSPXEY+CdwKvAo8AD3mEPAN/0Hj8K/LzXmno7MDalKquWkACRoTztL46y6lBcW1lVaRlDpD+LVUFzGRZTRW0BviHuAG8/8O/GmO+JyEvAV0XkQeAs8GHv+O8A9wAngBTwiUWPWs2LGKg9myYf8TG6IaILT6uSEAfqT6bKNrB+OmIq4FNdROLA0XLHUaRVwGC5gyjCcokTlk+syyVOWD6xLkaca40xTdPtqJSRDEen9K+raCKyZznEulzihOUT63KJE5ZPrKWOU8eiKqVWLE1wSqkVq1IS3MPlDmAelkusyyVOWD6xLpc4YfnEWtI4K6KRQSmlSqFSSnBKKbXoyp7gROTdInLUm17pk3O/oqSx/KOI9IvIq1O2VeS0UCKyWkSeEpFDIvKaiPxGJcYrImEReVFEDnhxfsrbvl5EdnvxfEVEgt72kPf8hLd/3VLEOSVen4i8LCKPVXicy2IKMxGpE5FHROSIiBwWkTuWNE5jTNm+AB9wEtgABIEDwPYyxvMm4Gbg1Snb/hT4pPf4k8CfeI/vAb6LO2DgdmD3EsfaBtzsPa4GjgHbKy1e73wx73EA2O2d/6vA/d72zwO/5D3+ZeDz3uP7ga8s8e/1t4B/Bx7znldqnGeAVZdtq6i/vXfuLwG/6D0OAnVLGeeS/UFm+OHvAL4/5fnvA79f5pjWXZbgjgJt3uM23D57AH8PfHS648oU9zeBuys5XiAC7ANuw+3c6b/8/wD4PnCH99jvHSdLFF8n7tyGbwMe8y60iovTO+d0Ca6i/vZALXD68t/LUsZZ7ipqUVMrldmCpoVaCl716Cbc0lHFxetV+/bjTsjwOG6pfdQYMzEr4tRYJuP09o8BjUsRJ/BXwO8CE5PpNVZonFCCKcxKYD0wAPyTV+3/gjeefcniLHeCW1aM+7FSUc3OIhIDvgb8pjFmfOq+SonXGGMbY27ELSHdCmwtc0hXEJF7gX5jzN5yx1Kku4wxN+POoP0rIvKmqTsr5G/vx73l8zljzE1AkoszfwOlj7PcCW45TK1UsdNCiUgAN7n9mzHm697mio3XGDMKPIVb1asTkYmhglNjmYzT218LDC1BeHcC7xeRM8CXcaupn63AOIFlM4VZF9BljNntPX8EN+EtWZzlTnAvAZu8lqog7s3aR8sc0+UqclooERHgi8BhY8xfVmq8ItIkInXe4yrc+4SHcRPdh2aIcyL+DwFPep/yJWWM+X1jTKcxZh3u/+GTxpiPVVqcsHymMDPG9ALnRWSLt+ntwKEljXOpborOciPyHtwWwJPAH5Y5lv8AeoA87qfPg7j3VZ4AjgM/BBq8YwX4Wy/ug8CuJY71Ltyi/SvAfu/rnkqLF7gBeNmL81Xg//a2bwBexJ1W6z+BkLc97D0/4e3fUIb/g7dwsRW14uL0Yjrgfb02cd1U2t/eO/eNwB7v7/9fQP1SxqkjGZRSK1a5q6hKKVUymuCUUiuWJjil1IqlCU4ptWJpglNKrVia4JRSK5YmOKXUiqUJTim1Yv3/o+oimyG4Q+sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This was a a trial to run w/o test masks \n",
        "\n",
        "def plot_predictions(images_list, start, colormap, model):\n",
        "    for (index, image_file) in enumerate(images_list):\n",
        "      #gt_mask_array = ((np.unique(cv2.imread(mask_file,cv2.IMREAD_UNCHANGED))))\n",
        "      #gt_mask = (cv2.imread(mask_file,cv2.IMREAD_UNCHANGED))\n",
        "      #print (\"mask file is\", mask_file)\n",
        "      filename = image_file.split('/')[-1]\n",
        "      #print(filename)\n",
        "      object_living_array = [1]\n",
        "      #if((set(object_living_array) & set(gt_mask_array))== set(object_living_array)):\n",
        "      image_tensor = read_image(images_list[index])\n",
        "      prediction_mask = infer(image_tensor=image_tensor, model=reconstructed_model)\n",
        "      #print(prediction_mask)\n",
        "\n",
        "      output_Im = PIL.Image.fromarray(prediction_mask.astype(np.uint8))\n",
        "\n",
        "      index_mod = index+start \n",
        "\n",
        "      ##Saving the category ids in an image for programatic IoU check \n",
        "      #output_Im.save(('/content/drive/MyDrive/InferenceLarge20192020shuffled-batch2/deeplab/program/' + str(index_mod) +'--runall-1--' +  filename))\n",
        "      output_Im.save('/content/drive/MyDrive/InferenceReports' +  filename)\n",
        "      #prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 7)\n",
        "      #overlay = get_overlay(image_tensor, prediction_colormap)\n",
        "      #print(\"shape of prediction mask is\", np.shape(prediction_mask))\n",
        "      #print(\"prediction colormap success\")\n",
        "\n",
        "      #want to get the gt mask displayed as well\n",
        "      #print(\"shape of gt mask is\", np.shape(gt_mask))\n",
        "      #gt_colormap = decode_segmentation_masks(gt_mask, colormap, 7)\n",
        "      #print(\"gt colormap successful\")\n",
        "\n",
        "      \n",
        "      \n",
        "      #adding gt mask to the plot                       )\n",
        "      plot_samples_matplotlib(\n",
        "              [image_tensor, overlay, prediction_colormap], filename, index, start, figsize=(18, 14) \n",
        "                             )\n",
        "      "
      ],
      "metadata": {
        "id": "SsoaPjImlUd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start = 0\n",
        "end = 14\n",
        "plot_predictions(test_images[start:end],  start, colormap, model=reconstructed_model)\n"
      ],
      "metadata": {
        "id": "dVNLuxtMmXmS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}