{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IR_7classes_Unet_Finalpaper_submission_Inference",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/Datasetpaper-final/blob/main/IR_7classes_Unet_Finalpaper_submission_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nmjKJ8iztVY"
      },
      "source": [
        "Use this as the final code. - DOES NOT USE ANY PREEXISTING WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mr3hxfyDRis",
        "outputId": "c3dd1de5-eb31-4ea2-a6ff-a50e0792539c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "aRF78c43RdG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/test.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/testannot.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "'''\n",
        "# Zip file with handpicked test images\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/select_test_images.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Zip file with masks of handpicked test images\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/select_test_masks.zip\", \"r\")\n",
        "zip_ref.extractall(\"/content/IRDatasetFinal\")\n",
        "zip_ref.close()\n",
        "\n",
        "# copy the select_test_images and masks in test and testannot respectively \n",
        "!cp /content/IRDatasetFinal/select_test_images/*.png /content/IRDatasetFinal/test\n",
        "!cp /content/IRDatasetFinal/select_test_masks/*.png /content/IRDatasetFinal/testannot\n",
        "'''"
      ],
      "metadata": {
        "id": "KK-yzVFNDSjr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "ce21e906-47a0-4446-f47e-4da6f919678a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Zip file with handpicked test images\\nzip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/select_test_images.zip\", \"r\")\\nzip_ref.extractall(\"/content/IRDatasetFinal\")\\nzip_ref.close()\\n\\n# Zip file with masks of handpicked test images\\nzip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/IRDatasetConsolidated/select_test_masks.zip\", \"r\")\\nzip_ref.extractall(\"/content/IRDatasetFinal\")\\nzip_ref.close()\\n\\n# copy the select_test_images and masks in test and testannot respectively \\n!cp /content/IRDatasetFinal/select_test_images/*.png /content/IRDatasetFinal/test\\n!cp /content/IRDatasetFinal/select_test_masks/*.png /content/IRDatasetFinal/testannot\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "APP_FOLDER = '/content/IRDatasetFinal/test'\n",
        "totalFiles = 0\n",
        "totalDir = 0\n",
        "\n",
        "for base, dirs, files in os.walk(APP_FOLDER):\n",
        "    print('Searching in : ',base)\n",
        "    for directories in dirs:\n",
        "        totalDir += 1\n",
        "    for Files in files:\n",
        "        totalFiles += 1\n",
        "   \n",
        "\n",
        "print('Total number of files',totalFiles)\n",
        "print('Total Number of directories',totalDir)\n",
        "print('Total:',(totalDir + totalFiles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuKFa_k3DZa1",
        "outputId": "380d797a-8ff8-4e85-8667-027c62c7667e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching in :  /content/IRDatasetFinal/test\n",
            "Total number of files 4508\n",
            "Total Number of directories 0\n",
            "Total: 4508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Now remove images that are not pure (rotated, mirrored etc from the test and testannot folders) \n",
        "# as we do not want to use them for testing \n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "test_dir = \"/content/IRDatasetFinal/test\"\n",
        "test_mask_dir = \"/content/IRDatasetFinal/testannot\"\n",
        "# Move rotated & mirrored files into tmp directory\n",
        "# The format of these files is aXXX_XXX_XX.png and aXXX_XXX_XX_XX.png\n",
        "# Number of underscores is 2 or 3\n",
        "dest_test_dir = \"/content/IRDatasetFinal/test_tmp\"\n",
        "dest_mask_dir = \"/content/IRDatasetFinal/testannot_tmp\"\n",
        "\n",
        "# Move non-augmented files from test directory into a temporary directory\n",
        "if (False == os.path.exists(dest_test_dir)):\n",
        "  os.mkdir(dest_test_dir)\n",
        "if (False == os.path.exists(dest_mask_dir)):\n",
        "  os.mkdir(dest_mask_dir)\n",
        "\n",
        "\n",
        "for eachFile in os.listdir(test_dir):\n",
        "  numUnderscores = eachFile.count('_')\n",
        "  if (numUnderscores > 1):\n",
        "    # File names in image & mask directories are identical so move both files in one shot!\n",
        "    shutil.move(os.path.join(test_dir, eachFile), os.path.join(dest_test_dir))\n",
        "    shutil.move(os.path.join(test_mask_dir, eachFile), os.path.join(dest_mask_dir))\n",
        "    \n",
        "'''\n",
        "    for eachFile in os.listdir(test_mask_dir):\n",
        "      numUnderscores = eachFile.count('_')\n",
        "      if (numUnderscores > 1):\n",
        "        # File names in image & mask directories are identical so move both files in one shot!\n",
        "        shutil.move(os.path.join(test_mask_dir, eachFile), os.path.join(dest_mask_dir))\n",
        "'''\n",
        "'''"
      ],
      "metadata": {
        "id": "aCiolPJKNBAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Count the number of images in test and testannot again\n",
        "import os \n",
        "APP_FOLDER = '/content/IRDatasetFinal/test'\n",
        "totalFiles = 0\n",
        "totalDir = 0\n",
        "\n",
        "for base, dirs, files in os.walk(APP_FOLDER):\n",
        "    print('Searching in : ',base)\n",
        "    for directories in dirs:\n",
        "        totalDir += 1\n",
        "    for Files in files:\n",
        "        totalFiles += 1\n",
        "   \n",
        "\n",
        "print('Total number of files',totalFiles)\n",
        "print('Total Number of directories',totalDir)\n",
        "print('Total:',(totalDir + totalFiles))\n",
        "'''"
      ],
      "metadata": {
        "id": "WYrN6Yk8U8-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "754aS1YJgOVn"
      },
      "source": [
        "# Image segmentation with a U-Net-like architecture\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2019/03/20<br>\n",
        "**Last modified:** 2020/04/20<br>\n",
        "**Description:** Image segmentation model trained from scratch on the Oxford Pets dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jUdvZcqUxIb"
      },
      "source": [
        "##Inference corresponding to run#7 of training - No preloaded weights - actual size images + mirror + rotate . GPU (Standard RAM)\n",
        "train - 26081  val - 7452  test - 3726 \n",
        ", epochs = 100\n",
        "training time - 2 days, testing -  min  \n",
        "Model - /Models/Unet-7classes-finalpapersubmission2\n",
        "file names prefix - run11 \n",
        "Loss - 0.0341\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XufPVSlGtx0e",
        "outputId": "c41d9553-f3c6-4a4e-a91c-98b369af6318"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"keras version is\", tf.keras.__version__)\n",
        "print (\"tf version is\", tf.__version__) \n",
        "!python --version"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keras version is 2.7.0\n",
            "tf version is 2.7.0\n",
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQgZp6JWgOVp"
      },
      "source": [
        "## Prepare paths of input images and target segmentation masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qGXBC0LgOVp"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "input_dir_test = \"/content/IRDatasetFinal/test\" # directory containing 2570 IR input images\n",
        "target_dir_test = \"/content/IRDatasetFinal/testannot\" # directory containing 2570 segmented IR images\n",
        "\n",
        "img_size = (512, 640) #it's height and width. it's a 1/2 sized image from the original image from segment.ai\n",
        "#img_size = (512, 640) #it's height and width. This is the original image from segment.ai\n",
        "#img_size_width_ht = (320,256)\n",
        "img_size_width_ht = (640,512)\n",
        "\n",
        "num_classes = 7\n",
        "batch_size = 10\n",
        "test_batch_size = 1\n",
        " \n",
        "\n",
        "# check for distinct label values in all masked files \n",
        "def getFullyQualifiedImagePaths(image_dir):\n",
        "  return sorted([ os.path.join(image_dir, fname)\n",
        "                  for fname in os.listdir(image_dir)\n",
        "                  if fname.endswith(\".png\")\n",
        "                ])\n",
        " \n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5SOaM9JvIao",
        "outputId": "a30d3035-b01c-4b99-dc8c-d319586df189"
      },
      "source": [
        "# Inference \n",
        "#prints how many files are in the test batch\n",
        "input_test_paths = getFullyQualifiedImagePaths(input_dir_test)\n",
        "target_test_paths = getFullyQualifiedImagePaths(target_dir_test)\n",
        "\n",
        "#prints how many files are in the batch\n",
        "print(\"Number of test images:\", len(input_test_paths))\n",
        "print(\"Number of test masks:\", len(target_test_paths))\n",
        "\n",
        "#pattern of the test images and masks \n",
        "for input_path, target_path in zip(input_test_paths[:5], target_test_paths[:5]):\n",
        "    print(input_path, \"|\", target_path)\n",
        "\n",
        "#!nvidia-smi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test images: 4508\n",
            "Number of test masks: 4508\n",
            "/content/IRDatasetFinal/test/a1570555926_835540.png | /content/IRDatasetFinal/testannot/a1570555926_835540.png\n",
            "/content/IRDatasetFinal/test/a1570555926_835540_2.png | /content/IRDatasetFinal/testannot/a1570555926_835540_2.png\n",
            "/content/IRDatasetFinal/test/a1570555926_835540_412.png | /content/IRDatasetFinal/testannot/a1570555926_835540_412.png\n",
            "/content/IRDatasetFinal/test/a1570555926_835540_412_2.png | /content/IRDatasetFinal/testannot/a1570555926_835540_412_2.png\n",
            "/content/IRDatasetFinal/test/a1570555926_835540_415.png | /content/IRDatasetFinal/testannot/a1570555926_835540_415.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n26N4oURgOVq"
      },
      "source": [
        "## Prepare `Sequence` class to load & vectorize batches of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvauKhqOgOVr"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "\n",
        "class OxfordPets(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            x[j] = img\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
        "            #in case we want to see if masked image have the right values\n",
        "            #print(tensorflow.keras.preprocessing.image.img_to_array(img))\n",
        "            y[j] = np.expand_dims(img, 2)\n",
        "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2: \n",
        "            #if classes are 3, keras expects masked values to be 0,1,2 only. Cannot take 1,3,5. It checks numerically not just the number of masked values\n",
        "            #y[j] += 1 \n",
        "        return x, y\n",
        "#!nvidia-smi\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nJLhugZvqTP"
      },
      "source": [
        "# Inference \n",
        "import random\n",
        "# Instantiate data Sequences for each split\n",
        "\n",
        "test_gen = OxfordPets(test_batch_size, img_size, input_test_paths, target_test_paths)\n",
        "#!nvidia-smi"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach\n",
        "1. load a model \n",
        "2. in a for loop\n",
        "3. OxfordPets give 2 images at a time\n",
        "4. run prediction\n",
        "5. visualize and save"
      ],
      "metadata": {
        "id": "zjPPcQqYGV5U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9gYCGN4gOVt"
      },
      "source": [
        "## Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!nvidia-smi\n",
        "\n"
      ],
      "metadata": {
        "id": "UOcb3bzQdBnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model \n",
        "#reconstructed_model1 = keras.models.load_model(\"/content/drive/MyDrive/Models/Unet-7classes-finalpapersubmission2\") # model with 2019 and 2020 data for 100 epochs\n",
        "#reconstructed_model1 = keras.models.load_model(\"/content/drive/MyDrive/Models/unet/Unet-7classes-finalpapersubmission_2020only\") # model with 2020 only images for 50 epochs\n",
        "reconstructed_model1 = keras.models.load_model(\"/content/drive/MyDrive/Models/unet/Unet-7classes-finalpapersubmission_2019&2020_shuffled\") # model with all imaged and shuffled\n",
        "\n"
      ],
      "metadata": {
        "id": "-syDRT_NHCP1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfDabGp9atzh"
      },
      "source": [
        "#### Prediction\n",
        "\n",
        "from IPython.display import Image, display\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "\n",
        "def display_mask(i,file_num_dir,imagefilepathtokens):\n",
        "    \"\"\"Quick utility to display a model's prediction. we need color masked images, so displaying both color and black and white\"\"\"\n",
        "    \n",
        "    #0 - sky(dark.blue). \n",
        "    #1 - water(light.blue).   \n",
        "    #2 - bridge(yellow).   \n",
        "    #3 - obstacle(purple).  \n",
        "    #4- living ob(green).  \n",
        "    #5- backgnd (orange). \n",
        "    #6 - self(pink)\n",
        "    # Colors are same as segments.ai scheme\n",
        "    label_colours = [(0,113,188), (216,82,24), (236,176,31), (125, 46, 141), (118, 171, 47), (161, 19, 46), (255,0,0)]  \n",
        "    \n",
        "    \n",
        "    mask = np.argmax(test_preds1[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    #print(\"mask shape = \", np.shape(mask))\n",
        "    #print(\"mask length = \", len(mask[i, 0]))\n",
        "    #print(\"mask length = \", len(mask[i]))\n",
        "    #print(\"Unique pixel values = \", np.unique(mask))\n",
        "    #print(\"Type of mask = \", type(mask))\n",
        "    \n",
        "   \n",
        "    #img1 = PIL.Image.new('RGB', (640, 512))\n",
        "    img1 = PIL.Image.new('RGB', img_size_width_ht)\n",
        "    #print (\"image size\", img_size_width_ht)\n",
        "    pixels = img1.load()\n",
        "    #print(type(pixels))\n",
        "    #print(pixels[0,0])\n",
        "    for j_, j in enumerate(mask[:, :, 0]):\n",
        "        #print (j_, j)\n",
        "        for k_, k in enumerate(j):\n",
        "             #print(k_, k)\n",
        "             if k < num_classes:\n",
        "                pixels[k_,j_] = label_colours[k]\n",
        "    output = np.array(img1)\n",
        "\n",
        "    outputpicturePath = '/content/drive/MyDrive/InferenceLarge20192020shuffled/unet/picture' + '/' + str(file_num_dir) + '--runreviseddata-1--' + imagefilepathtokens[-1]     \n",
        "    \n",
        "    #print(\"this is the colored inferred image\")\n",
        "    #display(img1)\n",
        "\n",
        "    ##Saving the category ids in an image for programatic IoU check \n",
        "    img1.save(outputpicturePath)\n",
        "\n",
        "    '''\n",
        "    print(\"this is the gray inferred image\")\n",
        "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
        "    display(img)\n",
        "    '''\n",
        "    return mask\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "test_input_dir1 = \"/content/IRDatasetFinal/test\"  \n",
        "test_target_input_dir1 = \"/content/IRDatasetFinal/testannot\"  \n",
        "test_input_img_paths1 = getFullyQualifiedImagePaths(test_input_dir1)\n",
        "test_target_img_paths1 = getFullyQualifiedImagePaths(test_target_input_dir1)\n",
        "no_of_test_img = len(test_input_img_paths1)\n",
        "print(\"Num test images = {0}\".format(no_of_test_img))\n",
        "\n",
        "prev = 0\n",
        "for i in range(1, no_of_test_img+1, 2):\n",
        "#for i in range(11, 20+1, 2):\n",
        "    #print(\"prev={0},i={1}\".format(prev,i))\n",
        "    test_gen1 = OxfordPets(test_batch_size, img_size, test_input_img_paths1[prev:i+1], test_target_img_paths1[prev:i+1])\n",
        "    print(\"iteration running is\", i)\n",
        "    #print(\"image_path\", test_input_img_paths1[i])\n",
        "    #!nvidia-smi\n",
        "    tf.compat.v1.GPUOptions(allow_growth=True)\n",
        "    test_preds1 = reconstructed_model1.predict(test_gen1)\n",
        "    #print(\"Size of test_preds1 = {0}\".format(len(test_preds1)))\n",
        "    #!nvidia-smi\n",
        "    #free memory / restart runtime \n",
        "\n",
        "    # Inference \n",
        "    ## Now visualize predictions for a specific image. Change the value of i\n",
        "\n",
        "    for jj in range(0, 2):\n",
        "        # check if ground-truth target mask as any object - class id - 3\n",
        "        image_gt = tf.keras.preprocessing.image.load_img(test_target_img_paths1[jj])\n",
        "        input_arr = tf.keras.preprocessing.image.img_to_array(image_gt)\n",
        "        gt_mask_array = np.unique(input_arr) \n",
        "        object_living_array = [1] # we need all 300+ images. hence putting here 1 instead of 3,4\n",
        "        #print(gt_mask_array)\n",
        "        #intersect_array_object = np.intersect1d(gt_mask_array, object_array) \n",
        "        #intersect_array_living = np.intersect1d(gt_mask_array, living_array) \n",
        "        if((set(object_living_array) & set(gt_mask_array))== set(object_living_array)):\n",
        "            #print (\"intersect output 2\", gt_mask_array )\n",
        "            #print(gt_mask_array)\n",
        "            #display the raw input image \n",
        "\n",
        "            file_num_dir = prev+jj #this denotes the sequence of the image in the test dir\n",
        "            #print(\"this is the raw image\")\n",
        "            #display(Image(filename=test_input_img_paths1[file_num_dir]))\n",
        "          \n",
        "            #display the ground truth masked image \n",
        "            img = PIL.ImageOps.autocontrast(load_img(test_target_img_paths1[file_num_dir]))\n",
        "            #print(\"this is the ground truth mask\")\n",
        "            #display(img)\n",
        "          \n",
        "            #set file names for storing the prediction \n",
        "            imagefilepathtokens = test_input_img_paths1[file_num_dir].split('/')\n",
        "            #print(\"tokens = \", imagefilepathtokens)\n",
        "\n",
        "            outputFilePath = '/content/drive/MyDrive/InferenceLarge20192020shuffled/unet/program' + '/' + str(file_num_dir) + '--runreviseddata-1--' + imagefilepathtokens[-1] \n",
        "            #print(\"output file name = \", outputFilePath)\n",
        "\n",
        "            #invoke the prediction function \n",
        "            output_mask = display_mask(jj,file_num_dir,imagefilepathtokens)  # Note that the model only sees inputs at 150x150.\n",
        "            output_mask = np.squeeze(output_mask, axis=2)\n",
        "            #result = np.where(output_mask==0)\n",
        "            #print(\"indices where output_mask has 0 value = \", result)\n",
        "            output_Im = PIL.Image.fromarray(output_mask.astype(np.uint8))\n",
        "\n",
        "            ##Saving the category ids in an image for programatic IoU check \n",
        "            output_Im.save(outputFilePath)      \n",
        "    prev = i+1      \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}